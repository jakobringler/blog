{"/":{"title":"fx notes","content":"\nPress `Ctrl` + `K` to search for something \n\nYou can use the interactive graph or the content lists below to browse the site and discover connections between notes\n\n### Content Lists\n- [All Notes](/notes)\n- [[notes/Houdini |Houdini]]\n- [[notes/Machine Learning |Machine Learning]]\n- [[notes/Projects and RnD |Projects and R\u0026D]]\n- [[notes/Resources |Resources]]\n- [[notes/Other |Other]]\n\n### About \nfx notes serves as a public *second brain* for my visual effects, proceduralism and machine learning related notes and experiments.\n\nIf you want to contribute or fix mistakes you can click the `Edit Source` button at the top of every page and open a pull request on GitHub.\n\nI hope you find something useful!\n\nBest,\n\nJakob\n\n[[notes/About |more]]\n\n\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/.trash/Houdini":{"title":"","content":"","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/64-bit":{"title":"64-bit","content":"## Make Houdini process data in 64-bit\nThis is a write up of Jeff Lait's [masterclass](https://vimeo.com/381044402) about 64-bit Processing in Houdini.\n\n### The Problem\nUsually Houdini works with 32 bit data. To show the limitations you can transform some geo a couple million units away from the origin and then back. Values will get quantized and data will be lost, since those numbers go past floating point limitations. To fix this you can use the `Attribute Cast` node which specifies which attribute values should be calculated using 64 bit.\n\n### How to tell if something is in 64-bit \"mode\"\nIn the geometry spreadsheet as well as the mousewheel / inspection menu the attribute is displayed with a small 64 to indicate it's precision.\n![[notes/images/64bit.png]]\nAlso there is a intrinsic detail attribute called precision that defines the preferred precision to be used downstream.\n\n### VEX still 32-bit?\nTo make vex process in 64-bit you have to change the precision option under the bindings tab.\n\nThere is no 'double'-type in vex. It only depends on the precision chosen. If it's set to 64-bit, everything will be 64 bits long (int, float, vector etc.). This also ensures that the code won't need any changes when switching modes.\n\n### Preferred Precision\nSince many old nodes were built to work with 32-bits SideFX introduced a global precision override to avoid having to change all precision settings. You can set this global preferred precision by using an `Attribute Cast` node again.\n\nThe mode is indicated by a small `64` badge next to the node.\n\n### Creating new attributes while in 64-bit precision\nNew attributes will be created in the correct precision depending on the currently preferred one. (Works with wrangles and attribcreate nodes)\n\n### When to use 64-bit processing\n1. Geometry moves far away from the origin\n2. You have a very large amount of geometry (think fur or feather grooms) -\u003e anything above 2 billion points\n3. opencl types get set differently depending on precision mode used\n\n\n### Vellum Simulations\nTo make Vellum compute in 64-bit modes you just have to feed 64-bit geometry into the solver / constraint nodes.\nGPU preformance may suffer a lot doing this.\n\n---\n\nsources / further reading:\n- [64-bit Processing in Houdini 18.0 - Jeff Lait (Houdini)](https://vimeo.com/381044402)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/About":{"title":"About","content":"Hi there!\n\nIm currently working as an FX TD and created this page to document and share workflows, ideas and notes about Houdini and various other vfx topics as well as my recent machine learning adventures. \n\nThis is by no means a documentation or manual, but much more a digital personal notebook. That's why it's rather unstructured at times and also contains different summaries of already existing resources and tutorials. I try my best to link to any sources. The quality of the notes may vary greatly and there is no garantuee for correctness.\n\n[Hit me up](https://twitter.com/jakobrin) if you find any mistakes or know of better ways to do things.\n\nIf you are serious about contributing you can check out this page on [[notes/FX Notes Formatting|formatting]].\n\nI hope you find something useful!\n\nBest,\n\nJakob\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Abusing-Heightfields-as-Fast-Image-Canvas":{"title":"Abusing Heightfields as Fast Image Canvas","content":"\n### The Problem\n\nA big part of getting a machine learning model to run is to prepare and import the data in the right _shape_. Usually this shape will be some n-dimensional [[notes/Tensors |Tensor]]. \n\nWhen working with 2D image data the files are read from disk. There are tons of tools and functions in most of the DL Frameworks that convert all sorts of files to correctly shaped tensors. In Houdini however, the data at hand is most likely not in a format (.jpg .png etc.) that can be extracted easily by a premade data loading solution.\n\nTo extract the data somewhat _liveish_ without having to write anything to disk and read it back in another step, a function that reads directly from houdini geometry is more efficient.\n\nThis brings us to the main issue: Where to store the data?\n\nUnfortunately storing image data on 3D geometry isn't very efficient. Polygon grids also store connectivity information and other data which make them rather slow to work with, especially when dealing with higher resolutions. \n\nTested when trying to do [[notes/Digit Recognition in Houdini |Digit Recognition in Houdini]].\n\n### The Solution\n\n**Heightfields** or '2D-Volumes' (weird name). \n\nThey have a grid-like topology and only store a single value per voxel instead of unnecessary connectivity information or other data. \n\nHoudini also ships with a python function to extract voxel data quickly, which allows us to convert the 2D information into the necessary shape.\n\n```Python \n# this goes in a Python SOP\n\nimport hou\nimport numpy as np\n\nnode = hou.pwd()\ngeo = node.geometry()\n\nW = 128 # image width\nH = W   # image height\n\ncanvas = geo.prim(0) # reads the 0th primitive \n\ncanvasVoxels = canvas.allVoxels() # reads all voxel values as one big array [1, 2, ... , n]\n\ninput = np.asarray(canvasVoxels, dtype=np.float64)\ninput = input.reshape(W,H) # creates a tensor of shape [128,128,1]\n```\n\nSee [[notes/Tower Sketcher |\"Training a Neural Net to Understand my Drawings\"]] to find out more about the applications of this technique.\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Alembic":{"title":"Alembic","content":"## Work with the Alembic Hierarchy\n\n### Building Path Hierarchy\n\nYou can use the `[ ] Build Hierarchy From Attribute` option on the `Alembic ROP` to manage how geometry gets organized in an alembic file and rebuilt in e.g. Maya. the node expects a primitive attribute usually called `@path`.\n\n//primitive wrangle\n\n```C\ns@path = \"/Group/GeoNodeInMaya/ShapeNodeInMaya\"\n```\n\nThis is similar to how [[notes/USD |USD]] hierarchy is defined with `@name`.\n\n### Path Attribute to Groups\n\nA lot of the time you want to have access to the path as groups:\n\n//primitive wrangle\n\n```C\nstring name[] = split(s@path, \"/\");\n\nforeach (string s; name){\n    setprimgroup(0, s, @primnum, 1, \"set\");\n} \n```\n\nFor more snippets check out [[notes/VEX Snippets |VEX Snippets]].\n\nThis creates groups for every segment of the path which can be a lot. To deal with them you can use wildcards in pretty much all fields that take group names.\n\nSay you have a few paths that also include some material information:\n\n- /GEO/Exterior/Glass/GEO\n- /GEO/Interior/Glass/GEO\n- /GEO/Interior/Leather/GEO\n\nYou can use that to assign a glass material to the relevant parts by using `*Glass*` in the group field.\n## Hiccups\n\n### Getting the Time Range, Startframe and Endframe\n\nThere is no time range intrinsic available by default so we need to write 3 lines of python to read the range from the file itself.\n\n// python shell\n\n```Python\nimport _alembic_hom_extensions as abc \nabcPath = r\"C:/Path/To/Cache.abc\" \ntimeRange = abc.alembicTimeRange(abcPath)\n```\n\nYou can make it more procedural by first getting the filename automatically. I read it from the instrinsics first and stored it in a regular attribute, but I'm sure you could do that in python too.\n\n![[notes/images/getaabcstartendframe.png]]\n\n// python SOP\n\n```Python\nimport _alembic_hom_extensions as abc\n\nnode = hou.pwd()\ngeo = node.geometry()\nfps = hou.fps()\n\nabcPath = geo.primStringAttribValues('abcfilename')[0]\nabcTimeRange = abc.alembicTimeRange(abcPath)\n\nstartframe = int(abcTimeRange[0] * fps);\nendframe = int(abcTimeRange[1] * fps);\n\ngeo.prims()[0].setAttribValue(\"startframe\", startframe)\ngeo.prims()[0].setAttribValue(\"endframe\", endframe)\n```\n### Vertex Colors to Maya\n\nMake sure to:\n- not have any degenerate prims or loose points by using a `clean` node\n- have `Cd` as a vertex attribute not point or prim\n\n\"Sometimes\" this also helped:\n- Use Hierarchy from Path option (could be any hierarchy)\n- Create a 4 float `Cd` that also has an alpha channel\n- Qualifier type has to be color (Clr) see below\n\n![[notes/images/4fltColor.png]]\n\n---\n\nsources / further reading:\n- [Exporting vertex colors (and other data) from Houdini to Maya - Toadstorm](https://www.toadstorm.com/blog/?p=240)\n- [SOLVED: Alembic vertex colors export for Maya](https://www.sidefx.com/forum/topic/38939/?page=1#post-178401)\n- [Houdini to Maya alembic: can't pass Cd SOLVED](https://www.sidefx.com/forum/topic/84355/?page=1#post-364666)\n- [Exporting alembic with instances is bigger file size?!](https://www.sidefx.com/forum/topic/84766/)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Attribute-Adjust-Syntax-Tricks":{"title":"Attribute Adjust Syntax Tricks","content":"\n## Setting Random Non-continuous Values\n\nYou can use the following syntax to specify a range from where the values will be picked. Set the value type to `List of Values`.\n\n```\n1-3; 5-9:0.25 \n```\n\nThis tells the node to create values from 1 to 3 with a stepsize of 1 and values from 5 to 9 with a stepsize of 0.25.\n\n---\n\nsources / further reading:\n- [Sexy Explosions | Attila Torok | Houdini 18.5 HIVE](https://www.youtube.com/watch?v=Gxfq9DZTuRM)\n- [Houdini DOCs](https://www.sidefx.com/docs/houdini/nodes/sop/attribadjustfloat.html)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Bash-Basics":{"title":"Bash Basics","content":"\n## Navigation\n// change directory\n\n```bash\ncd /mnt/drive/\n```\n\n// show files in directory\n\n```bash\nls\nls -a\n```\n\n## Files \u0026 Folders\n// create new file in current directory\n\n```bash\ntouch main.py\n```\n\n// create folder in current dir\n\n```bash\nmkdir foldername\n```\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Bash-and-Git":{"title":"Bash and Git","content":"\n- [[notes/Bash Basics |Bash Basics]]\n- [[notes/Git Basics |Git Basics]]\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Bounding-Box-Orientation-on-Arbitrary-Point-Clouds-with-PCA":{"title":"Bounding Box Orientation on Arbitrary Point Clouds with PCA","content":"The most straight forward utility tool you can build using [[notes/Principal Component Analysis|PCA]] is an automatic bbox generator. Even though the bound SOP can do the exact same thing, it's still a nice exercise that helps you wrap your head around what principal components are and how you can use them.\n\nI recommend reading the article on [[notes/Principal Component Analysis|Principal Component Analysis]] first!\n\nThis works because the first PCA component gives you the direction, where the most variance occurs in your data. You could think of this as the longest distance through your point cloud. The components are ordered by importance and a perpendicular to each other. So the second one is the direction that has the second most variance and so on.\n\nIf you visualize those, you get something that already looks like a transform.\n\nThe first one (blue) is essentially our Z vector, the second one our X and we can just construct Y using the cross product.\n\n![[notes/images/pca_bboxorientation.gif]]\n\n1. The first thing we need to do is center the incoming points on the origin using a `matchsize SOP`. \n2. We stash the transform for later use and call it offset.\n3. Then we run PCA on it and compute 3 components (2 are enough but 3 already looks like the axis we will create). \n4. Make sure to set points per sample to 1.\n5. We can then split point 0 out (this is our origin) and construct the transform matrix on it\n\n// point wrangle  \"build_matrix\"\n\n```C\nvector z = normalize(point(1, \"P\", 0) - v@P); \nvector up = normalize(point(1, \"P\", 2) - v@P);\n4@xform = maketransform(z, up)*invert(4@offset);\n```\n\n6. we can then invert that matrix and lock the point cloud to the world space axis in the origin\n\n// point wrangle \"invert_tranform\"\n\n```C\n4@m = point(1, \"xform\", 0);\nmatrix m = invert(4@m);\nv@P *= m;\n```\n\n7. In the end we just need to fit a box around it and move everything back to the original position\n\n// point wrangle \"transform_back\"\n\n```C\nmatrix m = point(1, \"m\", 0);\nv@P *= m;\n```\n\n![[notes/images/pca-bbox-orient-walkthrough.gif]]\n\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/pca_orient_pc.hip)\n\nAn inherent problem with this approach is that the axis flip a lot. The same thing happens in the `bound  SOP` and other implementations. You could counter that by solving the orient forward through time and comparing the orientation to the frame before and flip it back when a big jump in orientation happens.\n\n---\n\nsources / further reading:\n- [Oriented Bounding Boxes in VEX - Andy Nicholas](https://www.andynicholas.com/post/oriented-bounding-boxes-in-vex)\n- [Principal Component Analysis - Houdini DOCs](https://www.sidefx.com/docs/houdini/nodes/sop/pca.html)\n\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Collisions":{"title":"Collisions","content":"Most of the following are my personal notes on Jeff Wagners awesome masterclass on [Collisions](https://www.sidefx.com/community/collision-geometry-in-dop-simulations/). It's a bit on the longer side of presentations (6ish hours in 3 parts), but I highly recommend watching it! It also comes with demo files and a PDF which I will quote multiple times below.\n\n## How Colliders are built in DOPs\n\nDOPs is set up in a way that most HDAs are built from an empty object. Then there is data added to it. \"Data\" can mean loads of things:\n- geometry\n- physical simulation parameters\n- display geometry\n- render parameters to define what is visualized in the viewport\n\n### Ground Plane\n\n![[notes/images/groundplane_deconstruct.png]]\n\n1. (yellow) creates volume collision data and sets render parms to make it invisible in the viewport\n2. (red) applies motion/position data (also from OBJ level)\n3. (green) applies physical parameters such as `bounce` and `friciton`\n4. (blue) can be used to turn object active\n5. (purple) fetches display geometry and sets render parms \n6. (pink) labels collider\n7. (peach) stores info where the DOP object comes from\n8. (orange) bullet collision setup\n9. (dark green) display geometry SOP network\n10. (grey blue) collision geometry SOP network\n\nThis is just a sparse overview to go back to. To really understand it watch part 1 of Jeff Wagner's presentation (00:12:00 - 00:25:00).\n### Static Object\n\nThe `static object` DOP is a little bit more involved then the ground plane, but it's based on the same concepts .. just with more boilerplate nodes and switch logic around it.\n\n![[notes/images/staticobject_deconstruct.png]]\n\nI matched the node colors to show the similarities.\n\n### Deforming Object\n\nThis just exists as a shelf tool which sets up the `static object` DOP correctly to account for transformations and deformations.\n\n## Different Solvers Expect Different Colliders\n\n### POP Solver\n\nSupported Collision data:\n1. SDF Volume geometry (default)\n\t- very fast, real 3D\n2. Height Fields (same as SDF Volume data) \n\t- very fast but no holes\n\t- only `unchanged`, `stop`, `die` response / hit behaviour work\n3. Polygonal geometry\n\t- slow ( has to calculate primitive, primuvs for each particle )\n\t- can be used for complex systems that depend on per hit data like `hitnormal` etc.\n\t- `slide`, `stick` response / hit behaviour only works with polygons\n\nParticle simulations tend to be either of the following two types:\n1. A LOT of dumb particles\n2. A FEW smart particles\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e Dumb particles have no real rules other than forces motivating them through space. Using the default SDF Volume type colliders is the preferred method. The test to see if a particle is in or out of an SDF is a continuous linear lookup. The solver can predict when a particle is about to go from +ve outside space to =ve inside space of the SDF Volume. Remember the SDF is built from surface geometry. It encodes the surface upon SDF creation so all the hard work has been done. \n\u003e Am I in or out? SDF look up is almost instantaneous. \n\u003e \n\u003e If you have smart particles with lots of rules especially around collisions, you can choose to use polygons. When you collide against polygons, you can inherit the bi-linearly interpolated attribute values from the polygon faces. This means that you need to record the polygon face number and the actual local u and v hit co-ordinates on that polygon face. \n\nTo make particles collide with polygons you can use  the `POP Collision Detect DOP` in the pop source stream of the simulation. Just give it a SOP path and it should work.\n\n![[notes/images/Pasted image 20231018130122.png]]\n\n##### Particle Orientations and Collisions\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e When particles collide against a surface, the velocity vector of the particle is mirrored along the hit normal or the tangent of the face. This works for both SDF Volume colliders and polygonal surface colliders. \n\u003e \n\u003e By default, any instanced or copied geometry to these points will use the `v` velocity attribute to orient the geometry, if there is no `vector4 orient` quaternion attribute on the geometry\n\n### Grain and Wire Solver\n\nGrains uses the pop solver and the wire solver behaves like POPs so the same rules apply to both.\n\nSupported Collision data: \n1. SDF Volume geometry (default) \n2. Height Fields (same as SDF Volume data) \n3. Polygonal geometry\n\n### FLIP Solver\n\nUnlike the POP Solver which can take different but separate collider types the [[notes/FLIP |FLIP]] solver needs polygonal geometry **AND** a surface vdb!\n\nSupported Collision data:\n1. SDF Volume data for surface collisions \n2. Height Fields (same as SDF Volume data) \n3. Volume Velocity Fields \n4. (Polygon surfaces through POP Collision Detect but at your own peril)\n\nOn the FLIP solver under `Volume Motion` \u003e `Collision` you can specify where to read the collision velocities from (Point or Volume).\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e The main issue is resolution. For test FLIP sims and detailed colliders, you will not have enough collision detail. You have to enable.\n\u003e \n\u003e Thin colliders like cups and vessels containing fluid can benefit from: • Thick boundary walls.\n\u003e • if moving fast, lots of substeps and blending between frames. (Default shelf setup). \n\u003e • Extra velocity on the collider adding in some surface normals to the velocity.\n\nTo avoid precision errors make sure to simulate in a moderate scene scale ( not too small or too big ). I wrote a little more about it [[notes/General Workflow Tips#Simulation Scale |here]].\n\n### PYRO Solver\n\nSupported Collision data:\n1. SDF Volume data for surface collisions \n2. Height Fields (same as SDF Volume data) \n3. Volume Velocity Fields\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e You can use particles as collider sources using the `Gas Particle To Field` DOP to take your points and convert them in to SDF collision fields for use by the Pyro Solver. \n\u003e \n\u003e You need to name the field “collision”. \n\u003e \n\u003e If you go this route, you also have to use a second `Gas Particle To Field` DOP to construct a companion “collisionvel” vector field to hold the velocity of your collider. \\[...]\n\u003e \n\u003e Same goes for FLIP colliders. How these merge in to other existing collision and collisionvel fields is determined by how you mix these in with the `Gas Linear Combination` DOP\n\n### VELLUM Solver\n\n[[notes/VELLUM |VELLUM]] is an XPBD (Extended **Position-Based Dynamics**) solver.\n\nSupported Collision data:\n- Polygonal geometry\n\nas the name \"position based\" suggests there is no way to use sdf volumes as colliders, because they are only represented as a single point in Houdini.\n\nThere isn't much to take care of except making sure that you have a consistent point count on the collider geometry.\n\nYou can use the `POP Collision Ignore` node in DOPs to specify which vellum geometry is affected by which collider. Read more [here](https://www.sidefx.com/docs/houdini/vellum/collisions.html).\n\n### CLOTH Solver\n\nCloth supported Collision data: \n1. Cloth colliders from the Cloth shelf \n2. Polygonal geometry (supports swept collisions for sub-frame interpolation) \n3. SDF Volume data for surface collisions \n4. Height Fields (same as SDF Volume data) \n5. Volume Velocity Fields\n\nCloth solver (subset of the Solid FEM solver) is a special solver that prefers to collide against other cloth objects.\n\n### SOLID (FEM) Solver \n\nSolid supported Collision data: \n1. Solid colliders from the Solid shelf \n2. Polygonal geometry (supports swept collisions for sub-frame interpolation) \n3. SDF Volume data for surface collisions \n4. Height Fields (same as SDF Volume data) \n5. Volume Velocity Fields\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e The Solid solver uses FEM or Finite Element Methods to stimulate different materials from soft fleshy objects to concrete and everything in between.\n\u003e \n\u003e Collisions with like tet objects that are very rigid is a robust way of managing collisions. But many times we wish to integrate the Solid solver with other collision data such as polygon data. You have two choices: Use the Collision Shelf Static or Deforming tools to introduce polygon colliders, or use the Solid shelf and create a static solid collider comprised of tets.\n\n### Rigid Body (BULLET) Solver\n\nSupported Collision data:\n- Bullet Collision data (solver specific)\n- Height Fields\n\nDifferent Collider Types:\n1. Geometry Based\n\t- Convex Hull\n\t- Concave (not recommended)\n2. Primitive Types\n\t- Box\n\t- Capsule\n\t- Cylinder\n\t- Compound (important!)\n\t- Sphere\n\t- Plane\n3. SDF Based\n\t- Height Field\n\n##### Solver Substeps:\n\nGlobal Substeps x Bullet Substeps = Total Substeps\n\nImportant to resolve collisions / interpenetration / tunneling =\u003e especially when lots of different objects are close to each other.\n\nBullet substeps work like this:\n\nN = number of bullet substeps \n\n1. Take 1 global substep and divide it in N bullet substeps\n2. Take the velocity and divide it through N as well\n3. Move geometry forward N times by velocity/N and resolve collisions after each step \n\nrecommended minimum: 10\n\n##### Height Fields as Bullet Colliders:\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e Height fields make excellent bullet colliders. You can represent complex surfaces with height fields for efficient bullet simulations.\n\u003e \n\u003e Your only limitation is with height fields themselves. They do not support overhangs. The height fields by their nature are both concave and convex. It is good to know that there is a very fast solution to describing terrain collisions in Bullet without resorting to creating manifold terrain colliders. \n\u003e \n\u003e As with infinite plane colliders, you can use several height fields to define collision planes in your Bullet simulations. \n\u003e \n\u003e Just like infinite planes, these height field colliders have an inside and an outside. You can think of these as SDF colliders defined by a single voxel high volume. \n\u003e \n\u003e Unlike infinite planes, height fields are not infinite. The size of the height field is what the bullet solver uses. Any collisions beyond the height field will will not be by that height field.\n\nYou can use the `Terrain Object` shelf tool under the `Collisions` to set this up correctly.\n\n##### Concave Colliders with Convex Hulls\n\nDefault bullet colliders are convex:\n\n![[notes/images/Pasted image 20231018110552.png]]\n\nTo work around this without sacrificing performance too much there are two similar ways. Both split up the geometry into smaller pieces which will be \"convex hulled\" separately. Make sure to enable `Create Convex Hull per Set of Connected Primitives`!\n\n1. **Voronoi Fracture**\n\n![[notes/images/Pasted image 20231018110815.png]]\n\n2. **Convex Decomposition**\n\n![[notes/images/Pasted image 20231018111022.png]]\n\nYou can also use boolean operations or develop your own algorithm that splits up the geometry into mostly convex pieces.\n\n##### Concave Colliders with Spheres\n\nIf you need fast but pretty accurate collisions in RBD sims you can generate loads of little spheres inside the geometry and use those as a compound to calculate collisions. In fact this is the fastest way to calculate collisions, because you only need to know 1 point and the radius to calculate where collisions would occur.\n\n![[notes/images/sphere_bullet_collider.png]]\n\nMake sure to use `bake ODE` and set the `Geometry Representation` on the RBD Object to `Compound`. \n\nYou can then apply the transformation back to the original geometry by using `extract transform` for example.\n\n### BULLET Collision Objects and other Solvers\n\nList of possible combinations with bullet colliders: \n- bullet and particles \n- bullet and grains \n- bullet and wires \n- bullet and flip fluids \n- bullet and pyro \n- bullet and cloth \n- bullet and solids (FEM)\n\n##### BULLET and Particles or Grains\n\nParticles collide quite well against bullet simulation objects.\n\n##### BULLET and Wires\n\nWires collide against bullet objects, but there won't be any mutual affection. It's hard to get this working without extensive sop solver trickery and extremely light bullet pieces. Use [[notes/VELLUM |VELLUM]] instead!\n\n##### BULLET and FLIP\n\nFLIP uses the Bullet collision geometry to build it's colliders. When using packed RBD objects the geometry you see in the viewport only exists on the GPU, which is why FLIP can't access it. Same thing applies to pyro.\n\n##### BULLET and Pyro\n\nBULLET objects collide the same way as in FLIP. No access to packed geometry.\n\n##### BULLET and Solids (FEM) and Cloth\n\nPossible. Needs higher number of substeps on DOP network to get stable results.\nSubdivided geometry can help improving interaction, because the solid solver uses polygons to calculate collisions (too few points = inaccurate collisions).\n\n##### BULLET and VELLUM\n\nMutual VELLUM - BULLET interaction is unfortunately not possible. You can somewhat fake it but it will always be a hacky workaround.\n\nBy now you can use `Shape Match Constraints` and `Vellum Transform Pieces` in VELLUM to achieve a similar look, though. Check out [John Lynch's Talk](https://www.youtube.com/watch?v=5s8I2fs8kMs) about the topic!\n\n## Colliders and Substeps\n\n### Substepping\n\nThere are two areas where you can find substepping: \n1. The global substep rate used to march all solvers forward.\n2. Local solver substepping\n\nBoth of these have ramifications when it comes to collision geometry that is either animated or deforming. Static geometry is static so no issues there.\n\n### Global vs Solver Substeps\n\nGlobal Substeps = Set on the DOP network / affects all microsolvers etc. end to end\n\nSolver Substeps = Set on each micro solver or DOP object / affects only sub objects (closed system)\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e Global substeps are easy to understand. The global substep rate is set at the top level DOP Network folder node. The node that contains the entire DOP network.\n\u003e \n\u003e This rate forces all the solvers to sync up at these points in time.\n\u003e \n\u003e Colliders are fetched from their SOP sources at these points. All the geometry for all the colliders is refreshed at these global substeps. Yes you can inject any geometry you like. Remove colliders, introduce new colliders. Your imagination is your only limit. In general colliders should move **gracefully** through your simulation. Solvers like that.\n\u003e \n\u003e Solver substeps are solver specific substeps that divide up the time between global substeps in to smaller steps. Solver substeps are used for stability and accuracy within the solver. \n\u003e \n\u003e FLIP solvers can be run at 2 or more substeps to prevent instability between global substeps. Actually in some cases will run 100 or more internal substeps. \n\u003e \n\u003e Bullet Solver makes good use of substeps to prevent instability. \n\u003e \n\u003e Solver substeps can be imagined as running in their own little world only communicating with the rest of the world at the global substeps. They will digest all their constraints including collision geometry and manage them in their own way.\n\n### Viewport Debugging (Playback Rate)\n\nTo display substepped collision geometry you have to change the playback rate in the playbar options. You can set the rate to match the sub-frame global rate you set in the dop settings.\n\n### Collider Substep Interpolation\n\nBy default colliders are interpolated linearly, which works fine in most cases where objects move somewhat straight. If you need finer control it's best to cache out the colliders with substeps. More in the next section.\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e If you are using the default shelf setup for deforming colliders, you are getting linear blending between frames. For most fast moving objects, this works because fast moving objects tend to travel relatively straight. But we are in VFX and directors want snappy animation and powerful movement. \n\u003e \n\u003e In this case where you have snappy colliders moving very fast, that linear interpolation between frames can spell trouble. What you have is a trajectory that looks like a polygon curve with points at the frames and linear sequences. Those abrupt changes at the frames can do a lot of damage. Especially if an object is animated to stop almost instantaneously and then reflected upward or back. \n\u003e \n\u003e That instantaneous change at the frame creates an almost infinite amount of energy that may cause issues\n\n### Smoothing Out Fast Moving Colliders\n\nTo smooth out the motion of your colliders you have a few approaches: \n1. Cache out your colliders at subframes to minimize the abrupt changes. \n\t- remember to change file cache `$F` variable to `$FF` to ensure correct file naming\n2. Rework your colliders to use animation curves for the transforms.\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e If your geometry is deforming rapidly in size, scaling up or down, then your only resort is the first option. \n\u003e \n\u003e Actually the first option works well in most cases. Even providing one extra subframe of data works well. \n\n## Deforming Colliders\n\nIf you need volume data for colliders always cache them to disk to improve performance! Also make sure to construct and provide a `collisionVel` velocity volume.\n\nEach Solver deals with deforming collision geometry differently but there is a choice as to how we bring deforming geometry in to the Simulation environment: \n- Deforming geometry cached to disk\n- Animated geometry at the object level\n- Crowd Agents\n\n\u003e // from the SideFX pdf:\n\u003e \n\u003e Many of the solvers have to deal with collision data as volume SDF data either as VDB caches or our own volume SDF caches. This is one area where VDB shines and is the recommended workflow both for less memory both on disk and in memory and the speed of VDB cache creation and loading from disk.\n\nSolvers that require VDB SDF volumetric data as colliders:\n- Particle POP Solver and Grains PBD Solver \n- FLIP Solver \n- Pyro and Smoke Solvers\n\n## General Collider Workflows\n\n### Collision VDBs from Bad Topology\n\nWhen dealing with bad or damaged / 'non watertight' topology you can use the `VDB Topology to SDF` node to approximate a surface from the bad vdb result you get from the `VDB from Polygons` node.\n\n![[notes/images/badtopofix.png]]\n\n### Performance\n\nTo avoid regenerating collision geometry / vdbs of a moving but nondeforming object it makes sense to avoid SOP level animation and only animate on OBJ level and use the transform to move the collider in the DOP network. To do so you have to point the `OBJ Path` Parameter in the static object to the geo node containing your collider and enable `Use Object Transform`. This speeds up collider calculations massively. However you will lose per point collider velocities, because the geometry isn't moving in SOPs. \n\n![[notes/images/collision base setup.png]]\n\n---\n\nsources / further reading:\n- [Collision Geometry in DOP Simulations - SideFX](https://www.sidefx.com/community/collision-geometry-in-dop-simulations/) includes demo files and a pdf summary\n\t- [Collisions - Pt 1 | Jeff Wagner | Houdini Illume Webinar](https://vimeo.com/252645795)\n\t- [Collisions - Pt 2 (Colliders \u0026 Bullet Simulations) | Jeff Wagner | Houdini Illume Webinar](https://vimeo.com/254343083)\n\t- [Collisions - Pt 3 | Jeff Wagner | Houdini Illume Webinar](https://vimeo.com/255979341)","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Common-Attributes":{"title":"Common Attributes","content":"\nThis is my own extended \u0026 modified list of attributes based on SideFX DOCs, John Kunz' [VEX Attribute Glossary](https://wiki.johnkunz.com/index.php?title=VEX_Attribute_Glossary), Matt Estella's [VexCheatSheet](https://www.tokeru.com/cgwiki/VexCheatSheet) and others.\n\n## Every Day Use\n\n### Global Attributes\nAvailable in most wrangles\n\n```C\n// Available in all SOP wrangles\ni@Frame // current frame number =\u003e $F\nf@Time // current time in seconds =\u003e $T\nf@TimeInc // timestep currently being used for simulation or playback\n\n// only in DOPs\ni@SimFrame // integer simulation timestep number =\u003e $SF\nf@SimTime // simulation time in seconds =\u003e $ST\n\n// Available in Attribute Wrangle (point, vertex, primitive and detail)\nv@P // position of the current element\n\ni@ptnum // point number attached to the currently processed element\ni@numpt // total number of points in the geometry\ni@vtxnum // linear number of the currently processed vertex\ni@numvtx // number of vertices in the primitive of the currently processed element\ni@primnum // primitive number attached to the currently processed element\ni@numprim // total number of primitives in the geometry\n\n// useful: elemnum and numelem morph into whatever wrangle you are in\ni@elemnum // index number of the currently processed element\ni@numelem // total number of elements being processed\n\n// Available in Volume Wrangle\nv@P // position of the current voxel\nf@density // density value at the current voxel location\ni@resx, i@resy, i@resz // resolution\nv@center // center of the volume bbox\nv@dPdx, v@dPdy, v@dPdz // change in position that occurs in the x, y, and z voxel indices\ni@ix, i@iy, i@iz // voxel indices (only dense volumes / non-VDBs ) range from 0 to resolution-1\n\n// Available in Heightfield (Volume) Wrangle\nf@height // height at xy coordinate\nf@mask // mask at xy coordinate\n```\n\n### Geometry Attributes\nstuff that Houdini knows, expects and casts automatically to the correct type\n\n```C\n// Int\n@id // unique number \u003e temporal coherent\n@piece // like id but used more in rbd context\n\n// Float\n@pscale // particle radius size / uniform scale / set display particles as 'Discs' to visualize.\n@width // thickness of curves.  Enable 'Shade Open Curves In Viewport' on the object node to visualize\n@Alpha // alpha transparency override. Used by viewport to set the alpha of OpenGL geometry\n@Pw // spline weight\n\n// Vector3\n@P // point position\n@Cd // diffuse color override\n@N // surface or curve normal. Houdini will compute the normal if this attribute does not exist\n@rest // Used by procedural patterns and textures to stick on deforming and animated surfaces. Stores the P of a rest position\n@uv // UV texture coordinates (point/vertex)\n@v // point velocity. can be used for motionblur calculation\n@w // angular velocity used for rbd generally\n@force // used in simulation for many solvers that don't like you to edit v directly, but v will be calculated after updating force\n\n// Vector4\n@orient // local orientation of a point (represented as a quaternion)\n\n// String\n@name // unique name identifying which primitives belong to which piece \n@instance // path of an object node to be instanced at render time\n@shop_materialpath // material assignment pre primitive\n@path // used to rebuild alembic hierarchy\n```\n\n### Instancing / Copy Attributes\n\nall the things the copy to points SOP understands:\n\n```C\n// Float\nf@pscale // size of a copy\n\n// Vector3\nv@P // position of a copy\nv@Cd // color\nv@N // used to orient Z axis of the copy towards\nv@up // up direction for local space, typically {0, 1, 0}\nv@trans //translation of a copy\nv@scale // scale of a copy\nv@pivot // local pivot of a copy\nv@v // velocity for motionblur, used as +Z if no orient or N is present\n\n// Vector4\np@orient // local orientation of the point (quaternion)\np@rot // additional rotation. applied after orient, N, and up\n\n// String\ns@shop_materialpath // shader assignment\ns@material_override // dict mapping parameter names to values to override material properties\ns@instance\ns@instancefile // file path to geometry to instance.\ns@instancepath // geometry to instance. file on disk or an op: path\n\n// Matrix 3x3 or 4x4\n3@transform // transformation matrix (rotation, scale, and shear) overriding everything except translations from P, pivot, and trans\n4@transform // transformation matrix (translation, rotation, scale, and shear\n```\n\n## Context Specific\n\n### POP Attributes\n\n```C\n```\n\n### Grain Attributes\n\n```C\n```\n\n### Packed RBD Attributes\n\n```C\n```\n\n### RBD Constraint Attributes\n\n```C\n```\n\n### FLIP Attributes\n\n```C\n```\n\n### Vellum Point Attributes\n\n```C\n```\n\n### Vellum Constraint Attributes\n\n```C\n```\n\n### KineFX\n\n```C\ns@name // joint name attribute\n3@transform // world space 3×3 transform of the point (rotation, scale, and shear)\n4@localtransform // transform of the point relative to its parent\ni@scaleinheritance // determines how a point inherits the local scale from its parent\n```\n\n## Niche Stuff\n\n### GL Viewport Attributes\n// detail wrangle\n\n```C\ni@gl_wireframe // if set to 1: geometry will always appear as wireframe in viewport \n// if set to -1: geometry will always appear shaded in viewport -\u003e allows for guide geometry to be drawn shaded\ni@gl_lit // if set to 0: geometry will always appear without lighting \ni@gl_showallpoints // if set to 1: renders all points as sprites even if they are connected to geometry\ni@gl_spherepoints // if set to 1: render points as spheres\ni@gl_xray // if set to 1: draw geo in xray mode \nf@vm_cuspangle // controls cusp angle to generate normals when the geometry doesn't have any\ns@gl_spritetex // custom sprite texture for points\ni@gl_spriteblend // if set to 0: no depth sorting or blending\nf@gl_spritecutoff // discards pixels with alpha value below this \n\nv@Cd             \nf@Alpha \nv@N \nf@width // curve width \u003e `Shade Open Curves In Viewport` has to be turned on on the geo node\nf@pscale // if no pscale exists the viewport defaults to 1.0, while Mantra defaults to 0.1\n\ni@group__3d_hidden_primitives  // add primitives to this group to hide them from the 3D viewport\n\nf@intrinsic:volumevisualdensity // primitive intrinsic attribute controlling the opacity of volumes\nf@volvis_shadowscale // detail attribute controlling the shadow strength for volumes\n```\n\n### Intrinsic Attributes\n\n// primitive wrangle\n\n```C\n// Primitive Intrinsics\nfloat primbounds[] = prim(0, \"intrinsic:bounds\", @primnum);\nfloat primarea = prim(0, \"intrinsic:measuredarea\", @primnum);\nfloat memusage = prim(0, \"intrinsic:memoryusage\", @primnum);\n```\n\n// any wrangle\n\n```C\n// Detail Intrinsics\nint ptcount = detail(0, \"intrinsic:pointcount\", 0);\nfloat bounds[] = detail(0, \"intrinsic:bounds\", 0);\n\n```\n\n\u003e [!check] **Bounds:**\n\u003e \n\u003e Bounding box intrinsic attributes, like `intrinsic:bounds` or `intrinsic:packedbounds` are returned in (xmin, xmax, ymin, ymax, zmin, zmax) order.\n\nYou can set some intrinsic attributes in wrangles:\n\n// primitive wrangle\n\n```C\nsetprimintrinsic(0, \"pointinstancetransform\", i@primnum, 1, \"set\");\n```\n\nThis enables the use of the standard instancing attributes to transform any packed pieces (e.g. pscale, orient, ... ) further down stream **after setting it up!**\n\n![[notes/images/pointinstancetransform_intrinsic.png]]\n\n---\n\nsources / further reading:\n- [Copying and instancing point attributes - Houdini DOCs](https://www.sidefx.com/docs/houdini/copy/instanceattrs.html)\n- [Viewport display attributes - Houdini DOCs](https://www.sidefx.com/docs/houdini/model/attributes.html)\n- [VexCheatSheet - cgwiki](https://tokeru.com/cgwiki/VexCheatSheet)\n- [VEX Attribute Glossary - John Kunz](https://wiki.johnkunz.com/index.php?title=VEX_Attribute_Glossary)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Crowds":{"title":"Crowds","content":"The following notes only cover crowds with rigged agents, which are evaluated in real time during the simulation. Simple setups with instanced geometry caches are not covered here but may just as well be a viable solution.\n\n## Agents\n\nEvery crowd instance is an agent.\n\n### Agent Definition\n\nAn agent definition typically consists of a rig, one or more agent layers, which can hold different geometry variations or extra geometry and one or more agent clips, which store different animations for the rig.\n\nTo start creating an agent definition you need the `agent` SOP.\n\nIn the agent SOP you have multiple options to bring in your rig. The most straightforward way is to use a character rig and bring in a `Mocap Biped 3` character that ships with Houdini. This one is especially useful because it comes with many different animation clips which you can use.\n\nI usually start by bringing in a t-posed character rig and name the clip \"rest\". \n\n### Geometry Variations \u003e Agent Layers\n\nAgent layers are used to store different geometry variations in the same base agent. You can for example attach different faces, hairstyles, clothing or other accessories to the agent definition, which can be selected or randomized with attributes later on. To attach something to you agent you can use the `agent layer` SOP.\n\n##### Attaching Rigid Geometry (Accessories, Weapons etc.)\n\nI like the old `Agent Layer` HDA more then the new one when it comes to attaching rigid geometry, which is already placed correctly (hat, sword etc.), because it has the option `Keep Position When Attaching`.\n\n![[notes/images/agentlayerbindings.png]]\n\nAnything you want to attach, has to have a name attribute. You can pin it to the rig by choosing a bone which is used to transform the geometry correctly. See example below:\n\n![[notes/images/agent_layer_assign.png]]\n\nIf you want to add something to your base geometry, make sure to activate `Source Layer` under `Use Existing Shapes` and set it to whatever you chose to name your default layer.\n\n##### Attaching Deforming Geometry (Clothes etc.)\n\nTo attach clothes or other geometry that has to be deformed correctly you can leave the `Transform Name` empty and select a shape deformer. \n\n![[notes/images/agentlayerbindingsshapedeform.png]]\n\nFor this to work the geometry has to be bound to the rig. When using the `Mocap Biped 3` I just crack the HDA open and bind the cloth geo to the rig using the shelf tools.\n\n![[notes/images/agentlayercapture.png]]\n\nDepending on your geo you may also get away with attribute transferring the `boneCapture` attribute from you t-posed character after `agent unpack`-ing them.\n\n### More Animations \u003e Agent Clips\n\nTo give your agent more animations you can use the `Agent Clips` SOP. \nYou can provide the node with different datatypes:\n- Character Rig\n- FBX\n- File\n- CHOP\n- USD\n- Motionclip\n\nWhen working with the default `Mocap Biped 3` I tend to create many different ones on OBJ level with different clips applied. Then I point the agent clips node to those OBJ objects. \n\n![[notes/images/agentclips_mocapbiped3.png]]\n\nMake sure to set the start / end frame correctly to avoid unnecessary handles. This also comes in handy later when setting up custom looping and blending.\n\nTo prepare you agents for locomotion in a simulation you should disable `Inplace Animation` on the mocap biped 3 and enable `Convert to In-Place Animation` on the agent clips SOP. More on locomotion further down.\n\n### Debugging your Agent\n\nTo visualize a specific agent (correct layers \u0026 clip) you can use the `agent edit` SOP. \n\nTo preview a crowd and randomize different aspects you can use the `crowd source` SOP in combination with `crowd assign layers`.\n\n## Animation\n\nThere are different ways to set up and use animation clips to create a crowd. Depending on your usecase you might get away with simple in-place animation or constant forward speed.\n\n### In Place Animation\n\nWIP\n\n### Locomotion\n\nWIP\n\n### Foot Locking \u003e Agent Prep\n\nWIP\n\n### Clip Transitions\n\nThe `Agent Clip Transition Graph` SOP lets you define, which and how these clips can blend between each other. There is a automatic function, which does a fairly good job a lot of the time. You can as well filter some of those automatic connections out by hand or create new ones that it didn't pick up by itself. I found the output of the automatic mode too confusing to work with when it comes to working with lots of clips: \u003e15. I often end up just it up manually to be safe.\n\n##### Dealing with Mirrored Clips Automatically\n\nIn this example I only set up 3 transitions by hand and the wrangle connects each mirrored version accordingly. It just assumes that all the timings and blend durations etc. are identical for the base and the mirrored clip.\n\n![[notes/images/transitionmirror.png]]\n\n// point wrangle \"clipname_to_name\"\n\n```C\ns@name = s@clipname;\n```\n\n// primitive wrangle \"create_mirror_connections\"\n\n```C\nstring mirrorsuffix = \"_m\";\nstring newclipname_a = s@clip_a + mirrorsuffix;\nstring newclipname_b = s@clip_b + mirrorsuffix;\n\nint clip_a = nametopoint(0, s@clip_a);\nint clip_a_m = nametopoint(0, newclipname_a);\nint clip_b = nametopoint(0, s@clip_b);\nint clip_b_m = nametopoint(0, newclipname_b);\n\nif( clip_a_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a_m, clip_b);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, newclipname_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, s@clip_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n\nif( clip_b_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a, clip_b_m);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, s@clip_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, newclipname_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n\nif( clip_a_m != -1 \u0026\u0026 clip_b_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a_m, clip_b_m);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, newclipname_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, newclipname_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n```\n\n### Looping \u0026 Blending \u003e Clip Properties \n\nYou can use the `Agent Clip Properties` SOP to specify which clip can loop and how long/where to blend to avoid jumps. This creates a point per clip, which gets fed into the DOP crowd object. \n\n##### Dealing with Mirrored Clips Automatically 2\n\nI use the following snippet combined with the one above to manage mirrores clips somewhat automatically.\n\n![[notes/images/mirror properties.png]]\n\nIt's important that it has access to the tranistion graph output via the second wrangle input.\n\nAs before we also need to create a `name` attribute based on the clipname.\n\n// point wrangle \"create_mirror_properties\"\n\n```C\nstring mirrorsuffix = \"_m\";\nstring clipname = s@clipname;\nstring newclipname = clipname + mirrorsuffix;\n\nint clip_m = nametopoint(0, newclipname);\nint clip_exists = nametopoint(1, newclipname);\n\nif( clip_m == -1 \u0026\u0026 clip_exists != -1)\n{\n    int pt = addpoint(0, set(0,0,0));\n    int pA_clipname = setpointattrib(0, \"clipname\", pt, newclipname);\n    int pA_agent = setpointattrib(0, \"agentname\", pt, s@agentname);\n    int pA_clipnamesrc = setpointattrib(0, \"clipname_source\", pt, newclipname);\n    int pA_blend_a = setpointattrib(0, \"blend_duration_after\", pt, @blend_duration_after);\n    int pA_blend_b = setpointattrib(0, \"blend_duration_before\", pt, @blend_duration_before);\n    int pA_loop = setpointattrib(0, \"enable_looping\", pt, @enable_looping);\n    int pA_gait = setpointattrib(0, \"gait_speed\", pt, @gait_speed);\n    int pA_loopr = setpointattrib(0, \"loop_range\", pt, u@loop_range);\n    int pA_sampler = setpointattrib(0, \"sample_rate\", pt, @sample_rate);\n    int pA_start_time = setpointattrib(0, \"start_time\", pt, @start_time);\n}\n```\n\n### Blend Clips Together \u003e Transform Groups\n\nWIP\n\n## Adjusting Clips using KineFX\n\n### Mirroring\n\nThe `Rig Mirror Pose` SOP allows you to flip your clips and essentially double the amount of animations you have practically for free.\n\nIt doesn't work all the time because you need to provide it some kind of rest post / somewhat symmetrical centered frame of your clip.\n\n![[notes/images/rigmirrorpose_crowd.png]]\n\nYou can feed the updated `Motion Clip` back in an `Agent Clip` SOP.\n\n### On the Fly IK Chains\n\nThis allows you to easily manipulate and extend your animation library!\n\n- Drop down a `Agent Animation Unpack` SOP\n- time freeze and blast out any bones you want to add an animation to\n- animate with a `Rig Pose` SOP\n- Use the `IK Chains` SOP to blend in the animation\n\t- make sure to enable `Match By Name`\n- Drop down a `Motion Clip` and feed it back to an `Agent Clip` node to create a new clip\n\n![[notes/images/kinefx_ikchains_oncrowd.gif]]\n## Simulation\n\nWIP\n\n### DOP Setup\n\nWIP\n\n### Triggers\n\nWIP\n\n### Forces\n\nWIP\n\n### Ragdolls\n\nWIP\n\n### Partial Ragdolls\n\nWIP\n\n### Vellum\n\nWIP\n\n## Post Sim Tweaks\n\n### Agent Look At\n\nYou can make agents look at stuff after having already simulated everything! That is amazing and very simple to set up. You just need a `Agent Look At` SOP. Pipe in your crowd cache and you are good to go. You can use it in Simulation or in Live mode. If you don't simulate any turns you just directly modify the skeletons rotation to make each agent look at a target. Might have some snappy unexpected behavior but is usually good enough.\n\n## Rendering\n\n### Solaris, Karma \u0026 Hydra Delegates\n\n##### Texture Variations in Solaris\n\n1. **Karma CPU (VEX Shaders)**\n\nFor VEX Shaders you can modify many shader graph parameters using the `Material Variation` LOP. This way you can randomize or write custom logic in a vex snippet to define e.g. which path your diffuse texture is supposed to come from.\n\n![[notes/images/assign_different_textures_crowd_karmavex.png]]\n\n- the **index** variable  is a unique id per element \n- the **value** variable  is used to write back to the parameter specified under \"Name\"\n\n2. **Karma XPU (MTLX Shaders)**\n\nAs of now (Houdini 19.5) it's not possible to get access to the filepath parameters in the `MTLX Image` node. The only node that can be accessed is the `MTLX Geometry Property Value`. Unfortunately you can't feed the `MTLX Image` node any string inputs. That's why you have to load in all your textures in separate nodes and vary the assignment with some switch logic based on your geompropvalue input. \n\nI usually assign different point attributes to the packed agent crowd in SOPs and read them in the shader and don't even bother using the `Material Variation` node.\n\n![[notes/images/geompropvalue_shadersetup_crowd_matvariation.png]]\n\n##### Targetting Sub-Geometry in Solaris\n\nLet's say you have different agents with different layers. Some of those layers include extra geometry like a hat that requires a different shader than the main agent body. In OBJ / Mantra world you would have assigned the correct shader using material style sheets.\n\nIn USD you can't easily access sub-geometry when you are using `Instanced SkelRoots` (which is the default and recommended setting when working with crowds / SOP Crowd Import). If you wanted to assign specific shaders to different parts of your agent you would have to import the crowd as `SkelRoots` (not instanced) which you should probably only ever do for certain hero agents to not run out of RAM.\n\nWhat now? \n\nYou can do this via collections! (a USD thing) The setup isn't very straight forward though and I wouldn't have been able to get it to work without [this](https://www.sidefx.com/forum/topic/81706/?page=1#post-362705) forum post explaining how to do it.\n\nThe setup:\n\nI want to assign the \"hat\" geometry a different shader then the rest of the agents. As explained above normal assignment using e.g. wildcards doesn't work:\n\n![[notes/images/Pasted image 20231106143329.png]]\n\n1. Drop down a `SOP Crowd Import` LOP and point it to your crowd\n2. Create a `Collection` LOP\n\t1. give it a name\n\t2. check the `Allow Instance Proxies in Collection` toggle under options\n\t3. use the `%reference` expression to point to your geometry\n\n```C\n%reference:/crowd/agentdefinitions/agent/shapelibrary/hat\n```\n\n![[notes/images/Pasted image 20231106142401.png]]\n\n3. Create a material library node and build some shaders\n4. Use the `Assign Material` LOP to assign the material based on the collection name\n\t1. make sure to use `%yourCollectionName` \n\t2. set the Method to `Collection Based`\n\t3. point the `Path` to your crowd object\n\n![[notes/images/Pasted image 20231106142535.png]]\n\nThis should work in all hydra delegates (tested in Karma \u0026 Redshift).\n\n![[notes/images/Pasted image 20231106143438.png]]\n\n### Mantra \u0026 Third Party\n##### Material Style Sheets\n\nI didn't bother writing anything down because Mantra and Style Sheets are pretty much gone with USD and Solaris being more and more adopted. The [Zombies for Everyone](https://www.sidefx.com/tutorials/zombies-for-everyone-quick-intro-to-crowds/) course chapters 20 - 22 give you a good idea of the basics.\n\n---\n\nsources / further reading:\n- [Zombies for Everyone | Quick Intro to Crowds - SideFX](https://www.sidefx.com/tutorials/zombies-for-everyone-quick-intro-to-crowds/)\n- [Intro to Crowds - SideFX](https://www.sidefx.com/tutorials/intro-to-crowds/)\n- [🚶🏻‍♂️ Crowds in Houdini Tutorials - Juanjo Martínez](https://www.youtube.com/playlist?list=PLyzn6-dYuCbFby6Ynlk5ziOM-YsTRw68K)\n- [Crowds - cgwiki](https://www.tokeru.com/cgwiki/HoudiniCrowd.html)\n- [Targetting sub-geometry in Solaris in a crowd sim - SideFX Forum](https://www.sidefx.com/forum/topic/81706/?page=1#post-362705)\n- [Be in Control - Crowds and KineFX | Mikael Pettersen | Houdini 18.5 HIVE - SideFX](https://www.youtube.com/watch?v=1cLPYWj4Lqk)","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/DaVinci-Resolve":{"title":"DaVinci Resolve","content":"\n- [[notes/Python Scripting in Resolve |Python Scripting in Resolve]]\n- [[notes/Lua Scripting in Resolve |Lua Scripting in Resolve]]\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Deformation-Wrangle-vs-Attribute-Wrangle":{"title":"Deformation Wrangle vs Attribute Wrangle","content":"## Transforming Geometry\n\nMatrix transformations can be applied to \"every\" attribute in one go using the deformation wrangle.\n\n![[notes/images/Pasted image 20230914131210.png]]\n\n- left: base\n- middle: attribute wrangle transform\n- right: deformation wrangle transform\n\n### Attribute Wrangle\n- can apply the transformation only to one attribute at a time. In this example only `@P`\n\n```C\nmatrix m = ident();\nrotate(m, chf(\"angle\"), 4);\n\nv@P *= m;\n```\n### Deformation Wrangle\n- takes care of vectors and quaternions (such as `N` or `orient`) and transforms/deforms/rotates them properly\n- can only update point or vertex attributes (no primitives)\n\n```C\npos = pos;\nxform = xform;\n\nmatrix m = ident();\nrotate(m, chf(\"angle\"), 4);\n\npos *= m;\n```\n\nCheck out Roc Andic's video I linked below to find out more!\n\n---\n\nsources / further reading:\n- [Houdini VEX: What's so special about the deformation wrangle? - Roc Andic](https://www.youtube.com/watch?v=DRTk5nBr-aA)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Digit-Recognition-in-Houdini":{"title":"Digit Recognition in Houdini","content":"\n![[notes/images/digitRec_Banner.gif]]\n\nthe files can be found here: [GitHub - Houdini + PyTorch Digit Recognition](https://github.com/jakobringler/houdini_pytorch_digitrecognition)\n\n### sources / further reading:\n- [PyTorch Tutorial 13 - Feed-Forward Neural Network](https://www.youtube.com/watch?v=oPhxf2fXHkQ)\n- [4 pixel cam AI - Machine Learning in Houdini Tutorial](https://www.youtube.com/watch?v=WNEEokEq-Fg)","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Exporting-Image-Sequences":{"title":"Exporting Image Sequences","content":"\n- set your timeline in and out options to the desired length\n- drop down a `Movie File Out` TOP and choose image sequence\n- put the following snippet as an expression in the file path \n``` Python\n'myImgSeq' + me.fileSuffix.replace( str(tdu.digits( me.fileSuffix )), f\"{tdu.digits( me.fileSuffix ):04}\")\n```\n- to also export audio reference the audio CHOP in the related parameter\n- set TouchDesigner to non-Realtime mode by toggling off the Realtime Button at the top of the screen (this avoids dropping any frames)\n- set the loop option to `once`\n- go to the beginning of your timeline\n- turn on `Record` on the Movie File Out TOP and hit play\n\n---\n\nsources / further reading:\n- [TouchDesigner Forum - Derivative](https://forum.derivative.ca/t/resolved-export-movie-image-sequence-possible/5185)\n- [TouchDesigner Forum - Derivative](https://forum.derivative.ca/t/number-padding-for-moviefileout-me-filesuffix/249397)\n- [Exporting – TouchDesigner Tips, Tricks and FAQs 1 - bileam tschepe (elekktronaut)](https://www.youtube.com/watch?v=_1r8KLZWZ20)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/FLIP":{"title":"FLIP","content":"\n## General\n\n### Particle Velocity vs Volume Velocity\n\nIt's hard to have exact art directable control over the FLIP particles when working with volume velocities. It makes sense to modify the particle velocity directly in cases where detailed particle control is required.\n\n### Volume Loss\n\nTo avoid losing volume in the simulation activate `Apply Particle Separation` under the `Particle Motion \u003e Separation` tab on the flip solver.\n\n**Important Parameters**\n- Grid Scale\n- Particle Radius Scale\n\n\u003e [!Quote] Quote from SideFX Docs\n\u003e \n\u003eIf the **Particle Radius Scale** / **Grid Scale** \u003e= sqrt(3) / 2, then particles will never be under-resolved.\n\n### Collisions\n\nIn general FLIP needs polygonal data as well as a surface vdb to calculate proper collisions. You can provide collision velocities through a volume or point velocities on the geometry. To read more on the topic you can check out these notes about [[notes/Collisions |Collisions]]!\n\n## Small Scale\n\n### Droplets, Sheets \u0026 Tendrils\n\nKey Parameters:\n- Velocity Transfer (APIC Swirly)\n- Surface Tension\n- Substeps\n- Timescale\n\nWhen increasing the surface tension by itself the fluid will collapse in on itself. To prevent this substeps have to be increased. If possible this can also be countered by reducing the timescale (e.g. instead of increasing substeps from 4 to 8 the timescale could be set to 0.5).\n\n### Surface Adhesion\n\nYou could build a particle based setup in which you use the normal of the collider to build some kind of force to pull the fluid towards the surface. Unfortunately this causes issues with velocities when particles hit the collision from above and the adhesion force constantly fights gravity causing the simulation to slow down.\n\nLuckily there is a better way with less shortcomings:\n\nThe `Gas Stick On Collision` node is perfect for the job and can be piped in the `Volume Velocity` input of the flip solver directly. \n\nIt has parameters to control the scale and distance of the effect as well as separate controls for the scale along the normal and tangent. The bias parameter remaps the linear falloff along the distance. For realistic results use a higher normal scale and a lower tangent scale as well as a lower bias. This avoids slow moving fluid while insuring the adhesion effect. \n\nYou can combine this node with the `Use Friction and Bounce` option in the flip solver to get some bouncy motion back when the fluid is fast enough.\n## Large Scale\n\n### Transparency\n\nSometimes a collider has too much of a displacement force on the particles. When simulating a shark or dolphin in water there might be too big of an expansion in the direct surroundings of the collider. To counter this you can use the transparency parameter under `Volume Motion` \u003e `Collisions`. The value describes a percentage amount of particles that can pass through the collider, which helps art directing and decreasing the displacement effect.\n\n### Air Field Simulation\n\n---\n\nsources / further reading:\n- [FLIP Solver DOCs](https://www.sidefx.com/docs/houdini/nodes/dop/flipsolver.html)\n- [HOUDINI FLIP | LIQUID MORPH TUTORIAL -- Part 2 (+ projects) -  Sadjad Rabiee](https://www.youtube.com/watch?v=5sD9uTsewVI)\n- [Efficient techniques for realistic small scale Tendrils, Droplets and Sheets in Houdini - Jacktone Okore](https://www.youtube.com/watch?v=rxxR3hFYqLg)\n- [Volume Loss in FLIP SIM? Grid Scale and Particle Radius Scale - Janks](https://www.youtube.com/watch?v=JTyPcg5x6b8)\n- [Houdini | Adhesive FLIP Simulations | Masterclass - CG Forge](https://www.youtube.com/watch?v=gOgtdIk0QiI)\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/FX-Notes-Formatting":{"title":"FX Notes Formatting","content":"## Contributing\n\n### GitHub Pull Requests\n\nIf you just want to fix already existing notes you can open a pull request on GitHub in your browser by clicking the `Edit Source` button on every page.\n\nFor more involved contributions and completely new pages you need to fork the repository and add your new files there before opening a pull request.\n\nTo edit the notes in obsidian follow the instructions in the next chapter.\n\n### Obsidian Setup\n\nYou have to open the `contents` folder in obsidian. not the root directory of the repo.\n\n#### Templates\n\nthere is a barebones template for new notes. After creating a new note you should be able to press `Ctrl+T` to fill in the template and start from there.\n\n## Headline\n\ntext should start here\n### Subheadline\n\ntext should start here\n#### Subsubheadline if really necessary\n## Images\n\nAll image files have to live in `../content/notes/images`\n\n![[notes/images/imagetensor.png]]\n\nYou can scale Images by adding max pixel width value to the end of the path:\n\n`![[notes/images/imagetensor.png|500]]`\n\n![[notes/images/imagetensor.png|500]]\n\n## Code Blocks\n\nuse triple backticks to start a multiline code block and single backticks for inline code.\n\nuse C style colors for VEX by specifying the language after the opening backticks (C, Python etc.)\n\nspecify what type of wrangle this goes in and the name it has (e.g. in the screenshot above)\n\n// point wrangle \"wrangle_name\"\n\n```C\nvex\n```\n\nPython for Python and so on\n\n```Python\nprint(\"Hello Houdini\")\n```\n\nAttributes and other important stuff can be in single backticks:\n\n```\n`attributes`\n```\n\nwill be rendered like this:\n\n`attributes` \n## Links\n\n`[External Link](https://fxnotes.xyz)`\n\n[External Link](https://www.youtube.com/watch?v=dQw4w9WgXcQ)\n## Callouts\n\nyou can have fancy callout boxes using this format:\n\n![[notes/images/md_formatting_callout.png]]\n\nI had to take a screenshot because the website doesn't render anything in square brackets at all. just insert it down here instead of TAG:\n\n```\n\u003e TAG **Tips**\n\u003e \n\u003e Hot Tip!\n```\n\nbeside `tip` you can also put:\n- info\n- todo\n- success\n- quesiton\n- warning\n- failure\n- danger\n- bug\n- example\n- quote\n\n\u003e [!tip] **Tips**\n\u003e \n\u003e Hot Tip!\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e Important Source!\n\n## Downloads\n\nshared files have to live in `../content/notes/sharedfiles`\n\n```\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/filename.hip)\n```\n\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/filename.hip)\n## Page End\n\nevery page ends with this line `---` and the sources block\n\n---\n\nsources / further reading:\n- links and shit (format: `[linkname](link)`)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Feedback-Loops":{"title":"Feedback Loops","content":"\nLike a [[notes/SOP Solver |SOP Solver]] in Houdini!\n\nThe setup is a little different but the same logic applies. A TOP feedback loop is regularly used to blur, fade or transform pixels like you usually do with attributes in Houdini.\n\nThe most common setup is built using a feedback and a comp node like this:\n\n![[notes/images/touchdesigner_basic_feedbackloop.gif]]\n\nIn the feedback node I pointed to the comp9 node which overlays the feedback with the current frame. Everything in between the `comp9` node and the feedback node can be seen as the inside of a SOP solver.\n\nIn this example I used a level node to multiply the brightness with 0.95 every frame and blur it a little which leads to this fading effect.\n\n---\n\nsources / further reading:\n- [The Secret of Feedback Loops in TouchDesigner - Tutorial](https://www.youtube.com/watch?v=JkKHKsY31no)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/General-Workflow-Tips":{"title":"General Workflow Tips","content":"\n### Temporary Attributes\n\nWhen creating HDAs or working on bigger setups it might make sense to  prefix any temporary attributes with a special character such as `__` to make removing it later on easier e.g. by using `__*` in an attribute delete SOP. \n\nBy doing so you also ensure that you don't run into issues with incoming attributes that have the same name.\n\n### Cache Node\n\nThe [Cache Node](https://www.sidefx.com/docs/houdini/nodes/sop/cache.html) can be used to optimize playback and store data until anything upstream changes. Pretty simple and obvious, but somehow I learned pretty late about this small but awesome node and I still see it rarely in tutorials or other peoples shared setups.\n\n\u003e [!Quote] Quote from SideFX Docs\n\u003e \n\u003e lets you scrub otherwise sluggish animations in real time, play pop networks backwards, etc. because the animation is precomputed and stored in memory\n\n### Attribute Adjust Nodes\n\n- Introduced in 18.5\n- Very handy for all sorts of attribute manipulation, remapping or distorting\n- Can do unit conversion from e.g. seconds to frames\n- Check out [[notes/Attribute Adjust Syntax Tricks |Attribute Adjust Syntax Tricks]] for more\n\n### Simulation Scale\n\nWhat scale should you sim in?\nVex is only 32bit by default so you should scale your simulation to a size where you avoid huge and tiny values. Otherwise you will run into stability issues (e.g. particles escaping colliders etc.).\nIn FLIP for example, a good rule of thumb is to have your particle separation between 0.5 - 10 cm.\n\nValues should stay between 16.000 and 1e-4 (0,0001) to avoid running into precission errors\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Gifs-for-Obsidian":{"title":"Gifs for Obsidian","content":"\n## ffmpeg from mp4 to pngs\n// bash\n```bash\nffmpeg -i video.mp4 -vf scale=1280:720,fps=15 pngs/video.%04d.png\n```\n\n## gifski from pngs to good quality gif\n// bash\n```bash\ngifski -o video.gif -W 1280 -H 720 pngs/video.*.png\n```\n\n\n---\n\nsources / further reading:\n- [FFmpeg: high quality animated GIF?](https://stackoverflow.com/questions/42980663/ffmpeg-high-quality-animated-gif)\n- [Gifski](https://gif.ski/)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Git-Basics":{"title":"Git Basics","content":"\n## Concept\n...\n\n## Workflow\n1. pull latest\n2. create new branch\n3. do work\n4. rebase against main/master\n\t1. resolve conflicts\n5. push to remote\n6. raise pull request\n7. discuss\n8. merge\n\n## Setup\n### Initializing Git Repo:\n// create git repo in current dir:\n\n```bash\ngit init . \n```\n\n// get version\n\n```bash\ngit --version\n```\n\n### Staging Files\n// add files to be tracked by git\n\n```bash\ngit add filename\n```\n\nThis marks the file and tells git to pay attention to it when commiting to the commit history\n\n// add all files from this directory and downwards\n\n```bash\ngit add .\n```\n\n// stage all files in repo (also upwards)\n\n```bash\ngit add -A\n```\n\n// unstage files\n\n```bash\ngit rm -r --cached filename\n\ngit rm -r --cached .\n```\n\n### Status\n// get insight into which files are staged, commited or untracked\n\n```bash\ngit status\n```\n\n### Configuration\n// configure user info etc.\n\n```bash\ngit config --global user.name \"[name]\"\n\ngit config --global user.email \"[email address]\"\n\ngit config --global color.ui auto\n```\n\n## Commit, Push \u0026 Pull\n## Commits\nLike a save point for all staged files\n\n```bash\ngit commit -m \"message to describe what is being commited\"\n```\n\n// get overview of commits \n\n```bash \ngit log\n```\n\n// compact layout\n\n```bash\ngit log --oneline\n```\n\n// show specifics of a commit\n\ncopy COMMITHASH from log and dod\n\n```bash\ngit show COMMITHASH\n```\n\n// show changes since last commit\n\n```bash\ngit diff\n```\n\n// add messages after commit\n\n```bash\ngit commit --amend -m \"message\"\n```\n\n### Branches\n\n\u003e Branches are an important part of working with Git. Any commits you make will be made on the branch you’re currently “checked out” to. Use `git status` to see which branch that is.\n\n// get current branch\n\n```bash\ngit branch\n```\n\n// show all branches\n\n```bash\ngit branch -a\n```\n\n// show remote branches\n\n```bash\ngit branch -r\n```\n\n// rename branch to main\n\n```bash\ngit branch -M main\n```\n\n// create branch\n\n```bash\ngit branch branchname\n```\n\n// switch branch\n\n```bash\ngit checkout branchname\n```\n\n// switch and create branch\n\n```bash\ngit checkout -b branchname\n```\n\n// switch to main\n\n```bash\ngit checkout -\n```\n\n// delete branch\n\n```bash\ngit branch -d branchname\n```\n\n### Push\n// push the repository to the remote github repo\n\n```bash\ngit remote add origin git@github.com:...\n```\n\n```bash\ngit push -u origin main\n```\n\n// push new branch to remote\n\n```bash\ngit push --set-upstream origin branchname\n```\n\nor\n\n```bash\ngit push -u origin branchname \n```\n\n## GitHub\n### Configure SSH Keys\nIf you can't push to your remote repo you might have to configure an ssh access\n\n-\u003e [GitHub SSH Docs](https://docs.github.com/en/authentication/connecting-to-github-with-ssh)\n\n### Clone from Remote\n// download / mirror remote repo to local machine\n\n```bash\ngit clone https://github.com/username/reponame.git\n```\n\n## Pull\n// download changes from remote server \n\n```\ngit pull\n```\n\n// get from specific branch \n\n```bash\ngit pull origin main\n```\n\n## Pull Requests\nask the repository owner on e.g. github to merge your changes to the main/master branch\n\n## Merging\n### Merge\n// merge branch to current branch (main)\n\n```bash\ngit merge branchname\n```\n\n### Rebase\n\nAssume the following history exists and the current branch is \"topic\":\n\n```\n          A---B---C topic\n         /\n    D---E---F---G master\n```\n\nFrom this point, the result of either of the following commands:\n\n//update current branch with changes that happend on master in the mean time\n```bash\ngit rebase master\ngit rebase master topic\n```\n\nwould be:\n\n```\n                  A'--B'--C' topic\n                 /\n    D---E---F---G master\n```\n\n**NOTE:** The latter form is just a short-hand of `git checkout topic` followed by `git rebase master`. When rebase exits `topic` will remain the checked-out branch.\n\n## Misc\n### .gitignore file\n\n\u003e Sometimes it may be a good idea to exclude files from being tracked with Git. This is typically done in a special file named `.gitignore`. You can find helpful templates for `.gitignore` files at [github.com/github/gitignore](https://github.com/github/gitignore).\n\n### Submodules\nto be able to use git repos inside other git repos use submodules:\n\n// install a git repo as a submodule into another repo\n\n```bash\ngit submodule add https://github.com/username/reponame\n```\n\nwhen cloning repos with submodules you get the folders but not the files\n\n// to get the files of the submodules\n\n```bash\ngit submodule init\ngit submodule update\n```\n\n// do it all in one step\n\n```bash\n\ngit clone --recurse-submodules https://github.com/username/reponame\n```\n\n[Submodules - Git Book](https://git-scm.com/book/en/v2/Git-Tools-Submodules)\n\n---\n\nsources / further reading:\n- [Git Cheat Sheet](https://training.github.com/downloads/github-git-cheat-sheet/)\n- [Git Book](https://git-scm.com/book/en/v2/)\n- [Git and GitHub Tutorial For Beginners | Full Course [2021] [NEW]](https://www.youtube.com/watch?v=3fUbBnN_H2c)\n- [Git MERGE vs REBASE](https://www.youtube.com/watch?v=CRlGDDprdOQ)\n- [Git Rebase DOCs](https://git-scm.com/docs/git-rebase)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/HDA-Parameters":{"title":"HDA Parameters","content":"\n### Hiding or Disabling Parameters\neach parameter has a 'Disable When' \u0026 'Hide When' option where specific rules can be specified.\n\nSyntax:\n\n{ parametername == VALUE }\n\nalso accepts != \u003e \u003c \n\n### Executing Python Scripts\nTo write a HDA specific script you can define it in the PythonModule on the scripts page in the type properties. To run it you have to call the following command on any parameter of that HDA in the 'Callback Script' field.\n\n```Python\nhou.pwd.FUNCTION()\n```\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Houdini":{"title":"Houdini","content":"### VEX\n- [[notes/VEX 101 |VEX 101]]\n- [[notes/VEX Snippets |VEX Snippets]]\n- [[Common Attributes |Common Attributes]]\n- [[notes/UsefulMathNumbers |Useful Math \u0026 Numbers]]\n- [[notes/WaveExpressions |Wave Expressions]]\n- [[notes/vexStrings |Dealing with Strings]]\n- [[notes/Vector Shenanigans |Vector Shenanigans]]\n- [[notes/Quaternion-Euler-Rotations |Quaternion \u0026 Euler Rotations]]\n- [[notes/Matrix Operations |Matrix Operations]]\n- [[notes/primuv and xyzdist |primuv \u0026 xyzdist]]\n- [[notes/Deformation Wrangle vs Attribute Wrangle |Deformation Wrangle vs Attribute Wrangle]]\n### SOPs\n- [[notes/SOP Solver |SOP Solver]]\n### FX\n- [[notes/General Workflow Tips |General Workflow Tips]]\n- [[notes/Vocabulary |Vocabulary]]\n- [[notes/Collisions |Collisions]]\n- [[notes/Volumes |Volumes]]\n- [[notes/Velocity Fields |Velocity Fields]]\n- [[notes/POP |POPs]]\n- [[notes/FLIP |FLIP]]\n- [[notes/VELLUM |VELLUM]]\n- [[notes/PYRO |PYRO]]\n- [[notes/AXIOM |AXIOM]]\n### Unusual FX / Phenomena\n- [[notes/Reaction Diffusion |Reaction Diffusion]]\n- [[notes/Strange Attractors |Strange Attractors]]\n- [[notes/Slit Scanning|Slit Scanning]]\n### Characters, Rigs etc.\n- [[notes/KineFX |KineFX]]\n### CFX and such\n- [[notes/Hair and Fur |Hair \u0026 Fur]]\n- [[notes/Feathers |Feathers]]\n- [[notes/Crowds |Crowds]]\n### Rendering\n- [[notes/Redshift#Redshift in Houdini |Redshift in Houdini]]\n- [[notes/Karma |Karma]]\n### HDAs\n- [[notes/HDA Parameters |Parameters]]\n### USD / Solaris\n- [[notes/Solaris Attribute Conversion |Solaris Attribute Conversion]]\n- [[notes/USD Basics |USD Basics]]\n- [[notes/USD Instancing |USD Instancing]]\n### Python\n- [[notes/Python In Houdini |Python In Houdini]]\n- [[notes/Scripting in Houdini |Scripting in Houdini - Talk Notes]]\n- [[notes/VDBs and Voxels |VDBs and Voxels]]\n- [[notes/Node Shapes and Colors |Setting Node Shapes and Colors]]\n### Machine Learning\n- [[notes/ML Groom Deformer|ML Groom Deformer]]\n- [[notes/ML Sketching|ML Sketching]]\n- [[notes/ML Castles |ML Castles]]\n\t- [[notes/Tower Sketcher |Training a Neural Net to Understand Sketches]]\n\t- [[notes/Vegetation and Erosion Prediction |Vegetation and Erosion Prediction on Heightfields using GANs]]\n- [[notes/Setting up Anaconda and Houdini for 3D Deep Learning on Linux |Setting up Anaconda and Houdini for 3D Deep Learning on Linux]]\n- [[notes/Abusing Heightfields as Fast Image Canvas |Abusing Heightfields as Fast Image Canvas]]\n- [[notes/Storing Grooms in Volumes |Storing Grooms in Volumes]]\n### Data \u0026 Math Tricks\n- [[notes/Bounding Box Orientation on Arbitrary Point Clouds with PCA |Bounding Box Orientation on Arbitrary Point Clouds with PCA]]\n- [[notes/PCA Dejitter|PCA Geometry Dejitter]]\n### Misc\n- [[notes/Today I Learned |\"Today I Learned\" - Compilation]]\n- [[notes/Op Operators |Op Operators]]\n- [[notes/Hscript |Hscript]]\n- [[notes/OpenCL in Houdini |OpenCL]]\n- [[notes/The Misery Of OpenCL|The Misery Of OpenCL]]\n- [[notes/64-bit|Working with 64bit Data]]\n- [[notes/Alembic |Alembic]]\n### Issues \u0026 Fixes\n- [[notes/Houdini Crash Parsec |Houdini Crash Issue when using Virtual Displays with Parsec]]\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Houdini-Crash-Parsec":{"title":"Houdini Crash Issue when using Virtual Displays with Parsec","content":"\nTo prevent the crash add the following line to your `houdini.env`.\n\n```bash\nHOUDINI_USE_HFS_OCL=0\n```\n\n---\n\nsources / further reading:\n- [Reddit Post with same Issue](https://www.reddit.com/r/Houdini/comments/nuy6st/houdini_and_parsec_remote_pc_houdini_crashes_with/https://www.reddit.com/r/Houdini/comments/nuy6st/houdini_and_parsec_remote_pc_houdini_crashes_with/)\n- [Houdini Environment Variables](https://www.sidefx.com/docs/houdini/ref/env.html)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Houdini-to-TouchDesigner":{"title":"Houdini to TouchDesigner","content":"\n[[notes/TouchDesigner |TouchDesigner]] was born as a fork of [[notes/Houdini |Houdini]] 4.1, which shows in some methodologies and naming conventions. From a [[notes/Nuke |Nuke]] compositing standpoint there are many similarities when it comes to 2D image manipulation as well.\n\n\u003e [!info] **Disclaimer:**\n\u003e \n\u003e The following are mostly the notes I took when starting to learn TD. Don't read this if you want to get started.\n\u003e I instead recommend to check out Bileam Tschepe's [Beginner Course](https://www.youtube.com/playlist?list=PLFrhecWXVn5862cxJgysq9PYSjLdfNiHz).\n\u003e This might only be useful if you are looking for some Houdini analogies!\n\n## Interface\n\n### Hotkeys\n\n- `Tab` to create nodes\n- `P` toggles the parameter window on top of the network graph\n- `C` toggles color options for nodes\n- `O` toggles a small map of all the nodes\n- `D` toggles displays the currently selected node in the background\n- `U` exits node/group and goes one layer up\n- `I` enters/opens selected node/group (one layer down)\n- `MiddleMouse Click` on a node output lets you connect it to a new node directly similar to clicking the output of a Houdini node before searching and adding something\n- `Right Click` on connections lets you insert or add operators. The latter only connects the input and not the output of the new node\n- `A` activate selected viewer\n- `H` center viewport\n- `W` wireframe mode in SOP viewport\n- `N` toggle big names in network graph\n- `MiddleMouse Click` on nodes/operators gives you all the information like in Houdini\n- `Mouse Click` on the outer edge of an operator resizes the viewport to fit the aspect ratio\n- `P` in SOPs viewer active mode opens display options\n\n### Designer vs Perform Mode\n\nWhen working you are in \"Designer\" mode. This is where you see your network and the TD UI. Perform mode is a setting that hides all the UI and only Displays the output of your network in a new window.\nYou can activate \"Perform\" mode by pressing `F1` and exit it by hitting `ESC`. Closing the window with the windows/macOS buttons closes TD fully.\n\nFor fullscreen output you have to use a `Window Comp` which has a setting `Open` that has to be set to 1 and a drop down menu that allows you to select `borderless`.\n\n## Nodes\n\n### Contexts \u003e Types / Families\n\n**COMPs** (Object Components, Panel Components) - and other miscellaneous components. Components contain other operators:\n- Panels to build UIs with buttons and what not\n- Containers\n- Dynamics solvers\n- Import Export Nodes (FBX, USD, .. )\n\n**TOPs** (Texture Operators) - all 2D image operation:\n- 2D image manipulation like in COPs or normal Nuke\n\n**CHOPs** (Channel Operators) - motion, audio, animation, control signals:\n- very much like Houdini CHOPs\n- manipulating values and driving parameters with them\n- anything that's got to do with audio\n- input for different sensor types or controllers (e.g. midi)\n\n**SOPs** (Surface Operators) - 3D points, polygons and other 3D \"primitives\":\n- equal to SOPs in Houdini\n- working with geometry\n\n**MATs** (Material Operators) - materials and shaders:\n- like MAT and SHOP nets in Houdini\n\n**DATs** (Data Operators) - ASCII text as plain text, scripts, XML, or organized in tables of cells:\n- doesn't really exist in Houdini as data is usually stored in attributes\n\n**Custom** (Custom Operators):\n- you can create custom Operators which can belong to any type with C++\n\nYou can't connect operators of different types together but there are many ways to convert between types or reference outputs into parameters just like in Houdini.\n\n### Node UI\n\n- Star: Viewer Active\n- Arrow: Bypass\n- Lock: Unlock/Lock\n- Target Circle: Display\n- Dot: Activate BG Display\n\n### Comparison\n\n##### Hou to Touch\n\n- Subnet \u003e Base\n- Poly Frame \u003e Attribute Create (only generates normals and tangents)\n- Merge = Merge \n- Transform = Transform\n- Sort = Sort\n- circle, grid, etc. are mostly the same\n\n##### Nuke to TOPs\n\n- Grade, Color Correction, Invert \u003e Levels\n- Saturation, Hueshift etc. \u003e HSV Adjust\n- Merge \u003e Composite\n- Reformat \u003e Resolution\n- Shuffle \u003e Channel Mix (similar to old shuffle style)\n- Crop = Crop\n- Blur = Blur\n- Transform = Transform\n\n### Examples\n\nUnder the `Help` tab you can find `Operator Snippets` where you can load examples for most of the nodes\n\n## Workflow\n\n###  Similarities to Houdini\n\n- always use `null` nodes to mark the end/output of something (especially useful when the output is referenced somewhere)\n- 'Copy Parameter' / 'Paste Reference' Workflow is the same\n- You can `Lock`, `Bypass`, and `Display` nodes\n\n### Random Facts\n\n- in TD the axis of any image (TOPs) are mapped to `uv coordinates` ranging from 0-1 (think STMap in Nuke)\n\n### HDAs to MyComponents\n\n\n\n---\n\nsources / further reading:\n- [TouchDesigner Glossary - Derivative](https://docs.derivative.ca/TouchDesigner_Glossary)\n- [Operator - Touchdesigner DOCs - Derivative](https://docs.derivative.ca/Operator)\n- [TouchDesigner Beginner Course - bileam tschepe (elekktronaut)](https://www.youtube.com/playlist?list=PLFrhecWXVn5862cxJgysq9PYSjLdfNiHz)\n- [Why To Use NULLs – TouchDesigner Tips, Tricks and FAQs 2 - bileam tschepe (elekktronaut)](https://www.youtube.com/watch?v=u6hb-31gd1Q)\n\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/Hscript":{"title":"Hscript","content":"\n## Expressions\n\n### Shifting Framerange in Sequence\n\n// file parameter\n\n```C\n`padzero(4,$F+44)`\n```\n\n## Textport Shenanigans\n\n### Saving HIPs as text files\n\nKirill Kovalevskiy shared this technique in his [blog](https://kiko3d.wordpress.com/2015/03/19/converting-houdini-not-commercial-files/). Check it out for a longer detailed explanation!\n\n##### Writing Files\n// textport\n\n```bash\nopscript -G -r / \u003e $TEMP/temp.cmd\n```\n\n##### Reading Back\n// textport\n\n```bash\ncmdread $TEMP/temp.cmd\n```\n\n### Batch Rename Paths: opchange\n// textport\n\n```bash\nopchange $HIP $JOB\n```\n\n// textport\n\n```bash\nopchange \"/path/to/file\" \"/new/path/to/file\"\n```\n\n---\n\nsources / further reading:\n- [opscript DOCs](https://www.sidefx.com/docs/houdini/commands/opscript.html)\n- [Converting Houdini Not Commercial Files - Kiko3d](https://kiko3d.wordpress.com/2015/03/19/converting-houdini-not-commercial-files/)\n- [cmdread DOCs](https://www.sidefx.com/docs/houdini/commands/cmdread.html)\n\n","lastmodified":"2024-11-11T20:40:13.746621014Z","tags":null},"/notes/KineFX":{"title":"KineFX","content":"\n## Rig Attribute Wrangles\n\n### Basics\n\nWe can use any kind of curve and transform it like a parent hierarchy chain using the `Rig Attribute Wrangle`!\n\n![[notes/images/roll_up_kinefx.gif]]\n\n// rigattribwrangle\n\n```C\nprerotate(4@localtransform, chf(\"amount\") * @curveu, {1,0,0});\n```\n\nThe wrangle automatically creates a `transform` and `localtransform` matrix on each point to apply all transformations.\n\nUsually you would do this beforehand to better control the orientations. A good way is using the `orientalongcurve` SOP which has a 3x3 transform export option in combination with a `rigdoctor` node.\n\n### Secondary Motion with CHOPs\n\nThe issue with the default secondary motion SOP is that you can only have a single driver point for now (H19.5). If you want to control many different joints with different drivers without setting up a secondary motion SOP for each you are out of luck. \n\nI had to build a jiggle effect for an unfolding tree with a lot of branches where I would have needed this functionality.\n\nAs a workaround I animated an `angle` attribute and jiggled it using CHOPs before deforming my rig with the `prerotate` \u0026 `prescale` function. \n\n![[notes/images/CHOP_rig_jiggle.png]]\n\n// rotate_rig wrangle\n \n```C\nprerotate(4@localtransform, @angle, axis);\nprescale(4@localtransform, scale_vector);\n```\n\nI used the `AttribChop` SOP from Nick Taylor's [aeLib](https://github.com/Aeoll/Aelib) to do the jiggle setup more conveniently. Highly recommended HDA collection!\n\nIt gives quite convincing results without having to sim anything.\n\n![[notes/images/Tree_Chop_SecondaryMotion.gif]]\n\n## Deformation in Rest Space\n\nSometimes you want to see/work with a simulation/deformation result in rest space. To do this you need to create a local coordinate system for every point and then deform that with geometry. \n\n![[notes/images/restspacedeformation.gif]]\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003eThis setup was shown to me by Michiel Hagedoorn @ SideFX, while I was working on the [[notes/ML Groom Deformer|ML Groom Deformer]] project and it comes in handy every once in a while!\n\n![[notes/images/extract_deformation_restspace.png|400]]\n\n// point wrangle \"apply_local_coords\"\n\n```C\nv@axis_x = set(1, 0, 0);\nv@axis_y = set(0, 1, 0);\nv@axis_z = set(0, 0, 1);\n\nsetattribtypeinfo(geoself(), 'point', 'axis_x', 'vector');\nsetattribtypeinfo(geoself(), 'point', 'axis_y', 'vector');\nsetattribtypeinfo(geoself(), 'point', 'axis_z', 'vector');\n```\n\nYou can then introduce some sort of deformation that goes beyond the linear blend skinning one. In this case I'm using the wrinkle deformer, but this could be anything really (Vellum Sim etc.).\n\n// point wrangle \"extract_local_disp\"\n\n```C\nmatrix3 local_from_global = invert( set( v@axis_x, v@axis_y, v@axis_z ) );\nvector global_delta = point(1,\"P\",@ptnum) - v@P;\nvector local_delta = global_delta * local_from_global;\n\nv@P = local_delta;\n```\n\n// point wrangle \"apply_disp\"\n\n```C\nv@P += point(1, \"P\", @ptnum);\n```\n\nThe resulting geometry has all the deformation data on it with the input animation being removed. You can easily apply any effects and processing now and use a default bonedeform to move it back to the correct pose after.\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/deformation_in_restspace.hip)\n## Misc\n\n### Display Rig Controls on HDA Viewer State\n\nTo expose the control rig on an HDA you have to define the \"Default State\" parameter in the \"Node\" tab of the Operator Type Properties. \n\nFor kineFX controls this should be `kinefx__rigpose`.\n\nYou also have to promote the parameters you want to control with the HDA.\n\n---\n\nsources / further reading:\n- [Flower Garden | Carl Krause | Paris HIVE 2023 - SideFX](https://www.youtube.com/watch?v=9bsb-WBGPTc)\n- [KineFX Rigging | Fur Dude | Part 9 | Add More Control Joints - SideFX](https://www.youtube.com/watch?v=3Ssa9xXnx9E)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Legal":{"title":"Legal / Impressum","content":"\nAngaben gem. § 5 TMG:\n\nRingler, Jakob\n\nKontaktaufnahme:\n\nE-Mail: [fxnotes@proton.me](mailto:fxnotes@proton.me)\n\n**Haftungsausschluss – Disclaimer:**\n\nHaftung für Inhalte\n\nAlle Inhalte unseres Internetauftritts wurden mit größter Sorgfalt und nach bestem Gewissen erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt.\n\nEine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntniserlangung einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von den o.g. Rechtsverletzungen werden wir diese Inhalte unverzüglich entfernen.\n\nHaftungsbeschränkung für externe Links\n\nUnsere Webseite enthält Links auf externe Webseiten Dritter. Auf die Inhalte dieser direkt oder indirekt verlinkten Webseiten haben wir keinen Einfluss. Daher können wir für die „externen Links“ auch keine Gewähr auf Richtigkeit der Inhalte übernehmen. Für die Inhalte der externen Links sind die jeweilige Anbieter oder Betreiber (Urheber) der Seiten verantwortlich.\n\nDie externen Links wurden zum Zeitpunkt der Linksetzung auf eventuelle Rechtsverstöße überprüft und waren im Zeitpunkt der Linksetzung frei von rechtswidrigen Inhalten. Eine ständige inhaltliche Überprüfung der externen Links ist ohne konkrete Anhaltspunkte einer Rechtsverletzung nicht möglich. Bei direkten oder indirekten Verlinkungen auf die Webseiten Dritter, die außerhalb unseres Verantwortungsbereichs liegen, würde eine Haftungsverpflichtung ausschließlich in dem Fall nur bestehen, wenn wir von den Inhalten Kenntnis erlangen und es uns technisch möglich und zumutbar wäre, die Nutzung im Falle rechtswidriger Inhalte zu verhindern.\n\nDiese Haftungsausschlusserklärung gilt auch innerhalb des eigenen Internetauftrittes „_Name Ihrer Domain_“ gesetzten Links und Verweise von Fragestellern, Blogeinträgern, Gästen des Diskussionsforums. Für illegale, fehlerhafte oder unvollständige Inhalte und insbesondere für Schäden, die aus der Nutzung oder Nichtnutzung solcherart dargestellten Informationen entstehen, haftet allein der Diensteanbieter der Seite, auf welche verwiesen wurde, nicht derjenige, der über Links auf die jeweilige Veröffentlichung lediglich verweist.\n\nWerden uns Rechtsverletzungen bekannt, werden die externen Links durch uns unverzüglich entfernt.\n\nUrheberrecht\n\nDie auf unserer Webseite veröffentlichten Inhalte und Werke unterliegen dem deutschen Urheberrecht ([http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf](http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf)) . Die Vervielfältigung, Bearbeitung, Verbreitung und jede Art der Verwertung des geistigen Eigentums in ideeller und materieller Sicht des Urhebers außerhalb der Grenzen des Urheberrechtes bedürfen der vorherigen schriftlichen Zustimmung des jeweiligen Urhebers i.S.d. Urhebergesetzes ([http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf](http://www.gesetze-im-internet.de/bundesrecht/urhg/gesamt.pdf) ). Downloads und Kopien dieser Seite sind nur für den privaten und nicht kommerziellen Gebrauch erlaubt. Sind die Inhalte auf unserer Webseite nicht von uns erstellt wurden, sind die Urheberrechte Dritter zu beachten. Die Inhalte Dritter werden als solche kenntlich gemacht. Sollten Sie trotzdem auf eine Urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden Hinweis. Bei Bekanntwerden von Rechtsverletzungen werden wir derartige Inhalte unverzüglich entfernen.\n\nDieses [Impressum](https://jurarat.de/muster-impressum) wurde freundlicherweise von [jurarat.de](https://jurarat.de/) zur Verfügung gestellt.\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Lua":{"title":"Lua","content":"\n### Syntax\n\n```lua\nprint(\"hello world\")\n\n-- comment\n\n--[[\nmultiline \ncomment\n]]--\n\nlocal var = 1 -- local var (can only be accessed inside script)\n\nlocal name = \"Jack\"\n\nprint(\"My name is: \" .. name .. \".\") -- insert variables in print strings\n\nname = nil -- clears memory\n\ndescripting = [[multi\n\tline\nstring\n]] -- multi lines strings\n\nVAR = 2 -- global variable starts with capital letter\n\n_G.var = 2 --makes sure its global even if its lower case\n\nif true then -- if statement\n\tprint(\"something\")\nend\n\n--[[\n\t==\n\t\u003c=\n\t\u003e=\n\t~=\n\t!=\n\t\u003c\n\t\u003e\n]]--\n\nif (a \u003e b ) and (a \u003e c ) then -- combine booleans with and / or\n\tprint(\"something\")\nend\n\nif (a \u003e b ) or (a \u003e c ) then\n\tprint(\"something else\")\nelse\n\tprint(\"something more else\")\nend\n\nlocal age = 33\nlocal old = age \u003e 30 and true or false -- returns true if age over 30\n\n-- for loops\n\nfor i = 1, 10, 1 do -- for (current iteration), (iterations), (stepsize)\n\tprint(i)\nend\n\nfor i = 1, 10 do -- works too. default step size is 1\n\tprint(i)\nend\n\nfor i = 10, 1, -1 do -- reverse loop\n\tprint(i)\nend\n\nlocal arr = {2, 3, 45, 65676, 34, 21, 2, 30}\n\nfor i = 1, #arr do -- loop over array\n\tprint(arr[i])\nend\n\nlocal n = 10\n\nwhile n \u003e 0 do\n\tprint(\"foo\")\n\tn = n -1\nend\n\n------- functions\n\nlocal function fn() -- local\n\tlocal x = 1\n\treturn foo\nend\n\n-- x cant be accessed outside of the function\n\nfunction globalfn(input) -- global\n\treturn input * input\nend\n\nlocal add10 = function(number) -- define function as variable\n\tlocal outcome = 10 + number\n\treturn outcome\nend\n\nprint(add10(5))\n-- output: 15\n\nlocal function Pet(name)\n\tname = name or \"Default\" -- set default values if nothing is entered\n\treturn name\nend\n```\n\n### Operators\n\n```lua\n--[[\n\t__add = +\n\t__sub = -\n\t__mul = *\n\t__div = /\n\t__mod = %\n\t__pow = ^\n\t__concat = ..\n\t__len = #\n\t__eq = ==\n]]--\n```\n\n### User Input\n\n```lua\nprint(\"Type something:\")\nlocal input = io.read() \nprint(\"You wrote: \" .. input)\n\nio.write(\"Type something:\") -- can be answered in same line\nprint(\"You wrote: \" .. input)\n\n```\n\n### Arrays / \"Tables\"\n\n```lua\nlocal arr = {1, 3, 51, 9, 17}\n\ntable.sort(arr)\n\nfor i = 1, #arr do \n\tprint(arr[i])\nend\n\ntable.insert(arr, 2, \"foo\")\n\ntable.remove(arr, 4)\n\nprint(table.concat(arr, \", \"))\n-- output: 1, foo, 3, 17, 51\n\nlocal arr2 = {\n\t{1, 2, 3},\n\t{4, 5, 6},\n\t{7, 8, 9}\n} -- nested arrays\n\nprint(arr[1][1])\n\n  \n\nfor i = 1, #arr2 do\n\tfor j = 1, #arr2[i] do\n\t\tprint(arr2[i][j])\n\tend\nend\n\n```\n\n### Recursion\n\ncall function in itself\n\n```lua\nlocal function counter(num, enum)\n\tlocal count = num + 1\n\t\n\tif(count \u003c enum) then\n\t\tprint(count)\n\t\treturn counter(count, enum)\n\tend\nend\n\nprint(counter(10, 15))\n\n-- output: 11, 12, 13, 14\n```\n\n### Key - Value Pairs\n\n```lua\nlocal function sum(...) -- (...) allows an infinite amount of arguments\n\tlocal sums = 0\n\t\n\tfor key, value in pairs({...}) do -- swirly braces transforms the arguments into a table\n\t\tprint(key, value)\n\tend\nend\n\nsum(10, 5, 6 ,4, 12)\n\n-- output:\n-- 1  10\n-- 2  5\n-- 3  6\n-- 4  4\n-- 5  12\n```\n\n### Coroutines\n\nuseful to wait for other part of programm. e.g. unpacking files \n\n```lua\nlocal routine_1 = coroutine.create(\n\tfunction ()\n\t\tfor i = 1, 10, 1 do\n\t\t\tprint(\"(Routine 1): \" .. i)\n\t\t\t\n\t\t\tif i == 5 then\n\t\t\t\tcoroutine.yield()\n\t\t\tend\n\t\tend\nend\n)\n\nlocal routine_func = function ()\n\tfor i = 11, 20 do\n\t\tprint(\"(Routine 2): \".. i)\n\tend\nend\n\nlocal routine_2 = coroutine.create(routine_func)\n\ncoroutine.resume(routine_1)\ncoroutine.resume(routine_2)\ncoroutine.resume(routine_1)\nprint(coroutine.status(routine_1))\n```\n\n### File I/O\n\n```lua\nio.output(\"file.txt\")\nio.write(\"Hello World\")\nio.close()\n\nio.read(5) -- reads 5 characters\n\n\nlocal file = io.open(\"file.txt\", \"w\") -- \"w\" write, \"r\" read, \"a\" append\nlocal reads = file:read(\"*all\")\nfile:write(\"foo bar\")\nfile:close()\n```\n\n### OS Module\n\n```lua\nprint(os.time())\n\n-- deleting stuff\nos.remove(\"path/file.ext\")\n\n-- exec commands\nos.execute(\"whoami\")\n\n-- time code\nlocal start = os.clock()\n\nfor i = 1, 10000 do\n\tprint(i)\nend\n\nprint(os.clock() - start)\n\n-- exit programm\nos.exit()\n```\n\n### Custom Modules\n\n```lua\nmymodule = {}\n\nfunction mymodule.add(x, y)\n\treturn x + y\nend\n\nfunction mymodule.power(x, y)\n\treturn x ^ y\nend\n\nreturn mymodule\n```\n\n```lua\nlocal mod = require(\"mymodule\")\nprint(mod.add(5, 10))\nprint(mod.power(5, 10))\n```\n\n### Metamethods\n\n```lua\nlocal function addTableValues(x, y)\n\treturn x.num + y.num\nend\n\nlocal metatable = {\n\t__add = addTableValues,\n\t__sub = function (x, y)\n\t\treturn x.num - y.num\n\tend\n}\n\nlocal tbl1 = { num = 50 }\nlocal tbl2 = { num = 10 }\n\nsetmetatable(tbl1, metatable)\n\nlocal ans = tbl1 - tbl2\n\nprint(ans)\n```\n\n### More\n[[notes/Lua Scripting in Resolve |Lua Scripting in Resolve]]\n\n---\n\nsources / further reading:\n- [Lua.org](https://www.lua.org/)\n- [Full Lua Programming Crash Course - Beginner to Advanced](https://www.youtube.com/watch?v=1srFmjt1Ib0)\n- [LuaRocks](https://www.youtube.com/watch?v=1srFmjt1Ib0)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Lua-Scripting-in-Resolve":{"title":"Lua Scripting in Resolve","content":"\nOverview and snippets for [[notes/Lua |Lua]] coding in DaVinci Resolve.\n\n\u003e A good API overview and multiple examples can be found in Deric's [Unofficial DaVinci Resolve Scripting Documentation](https://github.com/deric/DaVinciResolve-API-Docs)\n\n## Basics\n\n### Getting Base Objects\n\n```lua\nresolve = Resolve()\nprojectManager = resolve:GetProjectManager()\nproject = projectManager:GetCurrentProject()\nmediaPool = project:GetMediaPool()\nrootFolder = mediaPool:GetRootFolder()\nclips = rootFolder:GetClips()\ntimelineCount = project:GetTimelineCount()\n```\n\n### Switching Pages\n\n```lua\nresolve:OpenPage(\"color\") -- (\"media\", \"cut\", \"edit\", \"fusion\", \"color\", \"fairlight\", \"deliver\").\n```\n\n\n\n---\n\nsources / further reading:\n- [Unofficial DaVinci Resolve Scripting Documentation](https://github.com/deric/DaVinciResolve-API-Docs)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/ML-Castles":{"title":"ML Castles","content":"![[notes/images/ML_Castles_Banner.png]]\n\n## Machine Learning Supported Procedural Castles\nis a project I did for my bachelor thesis. The goal was to find out how machine learning could support procedural content genertation.\nI created a procedural castle \"generator\", which is enhanced by two types of ML systems.\n\n![[notes/images/CastleDemo.gif]]\n\nThe project files and code can be found on GitHub: [H_ML_Castles](https://github.com/jakobringler/H_ML_Castles)\n\n### ML 1: Training a Neural Net to Understand my Sketches\n\nA Machine learning network was used to provide a new kind of user interface to control parameters of an HDA, which procedurally generates towers.\n\n![[notes/images/TowerSketcher.gif]]\n\nmore details can be found here: [[notes/Tower Sketcher |Tower Sketcher]]\n\n### ML 2: Predicting Heightfield Data using cGANs\n\nThis idea isn't anything new and was already shown in this [SideFX demo](https://www.sidefx.com/tutorials/machine-learning-data-preparation/). Since heightfields are essentially images (2D arrays) you can use conditional GANs to translate between one image to another.\n\nYou can for example translate height data from a base terrain to an eroded version of that terrain, like shown in the SideFX demo.\n\n![[notes/images/ErosionPrediction.gif]]\n\nIn the same way you can then use the erosion data (height, water, sediment etc.) to predict where certain types of vegetation should appear. The model spits out different colored masks, which are used to scatter points for different plant or tree models.\n\n![[notes/images/VegetationDemo.gif]]\n\nmore details can be found here: [[notes/Vegetation and Erosion Prediction |Vegetation Erosion Prediction]]\n\n### Results\n\n![[notes/images/Castle1.png]]\n![[notes/images/Castle2.png]]\n![[notes/images/TopViewUserInput.png]]","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Machine-Learning":{"title":"Machine Learning","content":"## Houdini\n### Projects \u0026 Examples\n- [[notes/ML Groom Deformer|ML Groom Deformer]]\n- [[notes/ML Sketching|ML Sketching - Teaching Houdini to Understand your Drawings]]\n- [[notes/ML Castles |ML Castles]]\n\t- [[notes/Tower Sketcher |Tower Sketcher]]\n\t- [[notes/Vegetation and Erosion Prediction |Vegetation \u0026 Erosion Prediction]]\n- [[notes/PCA Dejitter|PCA Dejitter]]\n- [[notes/Digit Recognition in Houdini |Digit Recognition in Houdini]]\n### Setup Guides (old)\n-  [[notes/Setting up Anaconda and Houdini for 3D Deep Learning on Linux |Setting up Anaconda and Houdini for 3D Deep Learning on Linux]]\n- [[notes/Abusing Heightfields as Fast Image Canvas |Abusing Heightfields as Fast Image Canvas]]\n## ML Basics for Technical Artists / TDs\n##### Concepts\n- [[notes/Neural Networks |Neural Networks]]\n- [[notes/Tensors |Tensors]]\n- [[notes/Measuring Error|Measuring Error]]\n- [[notes/Loss|Loss]]\n- [[notes/Activation Functions|Activation Functions]]\n- [[notes/Back Propagation |Back Propagation]]\n- [[notes/Gradient Descent |Gradient Descent]]\n- [[notes/Regularization|Regularization]]\n##### Model Archtiectures\n- [[notes/Feed Forward Networks (FFNs) |Feed Forward Networks (FFNs)]]\n- [[notes/Convolutional Neural Networks (CNNs) |Convolutional Neural Networks (CNNs)]]\n- [[notes/Generative Adversarial Networks (GANs) |Generative Adversarial Networks (GANs)]]\n- [[notes/Recurrent Neural Networks (RNNs) |Recurrent Neural Networks (RNNs)]]\n- [[notes/Autoencoders |Autoencoders]]\n- [[notes/Neural Radiance Fields |Neural Radiance Fields (NeRFs)]]\n##### Dimensionality Reduction\n- [[notes/Principal Component Analysis |Principal Component Analysis (PCA)]]\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Matrix-Operations":{"title":"Matrix Operations","content":"\n## Basics\n\n### Dimensions\n\nHoudini most commonly uses 3x3 or 4x4 matrices to store transformation data\n\n3x3 -\u003e rotation and scale\n4x4 -\u003e rotation, scale and translation\n\n### Identity Matrix\n\nThe identity matrix is sort of the base matrix and means no transformation is applied. The diagonal ones store the scale value.\n\n$$\n\\bigg[\\begin{array}{rcl}\n\t1\u00260\u00260\u00260 \\\\\n\t0\u00261\u00260\u00260 \\\\\n\t0\u00260\u00261\u00260 \\\\\n\t0\u00260\u00260\u00261 \\\\\n\\end{array}\\bigg]\n$$\n\nIt can be created in VEX by calling `ident()`.\n\n```C#\nmatrix xform = ident();\n```\n\n### Translation\n\n```C#\nmatrix xform = ident();\nvector translation = chv(\"translation\");\n\ntranslate(xform, translation);\n\n@P *= xform;\n```\n\nThis is what happens under the hood:\n\n```C#\nvector t = chv('translate');\n\nmatrix T = set(set(1, 0, 0, 0), set(0, 1, 0, 0), set(0, 0, 1, 0), set(t.x, t.y, t.z, 1));\n\n@P *= T;\n```\n\n### Rotations\n\nQuaternion Rotations\n\n```C#\nmatrix xform = ident();\nfloat angle = radians(chf(\"angle\"));\nvector axis = chv(\"axis\");\n\nrotate(xform, angle, axis);\n\n@P *= xform;\n```\n\nEuler Rotations\n\n```C#\nvector rot = radians(chv('rotate'));\n\nmatrix3 Rx = set(1,0,0, 0,cos(rot.x),-sin(rot.x), 0,sin(rot.x),cos(rot.x));\n\nmatrix3 Ry = set(cos(rot.y),0,sin(rot.y), 0,1,0, -sin(rot.y),0,cos(rot.y));\n\nmatrix3 Rz = set(cos(rot.z),-sin(rot.z),0, sin(rot.z),cos(rot.z),0 ,0,0,1);\n\n@P = @P*Rx*Ry*Rz;\n```\n\nfor more details have a look at [[notes/Quaternion-Euler-Rotations |Quaternions and Euler Rotations]]\n\n### Scale\n\n```C#\nmatrix xform = ident();\nvector scale = chv(\"scale\");\n\nscale(xform, scale);\n\n@P *= xform;\n```\n\n### Order of Operations for Transformations\n\nThe default of the transform node and in most 3D packages ist `SRT`, which means first **Scaling** the object in place and then **Rotating** it before **Translating** it's position.\n\n### Permutations\n\n### Shear\n\n---\n\n## Usecases\n\n### Extracting a Transformation Matrix with VEX\n\nSometimes it's desirable to lock an animated mesh to the origin to perform further operations. To move it from it's position in world space to the origin we need it's transformation matrix.\n\n[Paweł Rutkowski](https://vimeo.com/284712920) has a great video on the topic. The following is basically a writeup of the contents of his video for my own memory and to easily get back to it.\n\nTo create a transformation matrix we first have to create a local coordinate system that moves with the object. To do so we have to identify 2 Points that don't deform and calculate a vector between the two. First isolate the the 2 points by deleting everything else.\n\n![[notes/images/Pasted image 20220602234539.png]]\n\n```C#\n// this goes in point wrangle 1\n\nvector P1 = point(0, \"P\", 0);\nvector P2 = point(0, \"P\", 1);\nvector up = {0,1,0};\n\nvector X = normalize(P2-P1);\nvector Z = normalize(cross(X, up));\nvector Y = normalize(cross(X, Z));\n\nvector P = P1 + (P2 - P1) / 2;\n```\n$$\n\\begin{array}{rcl}\n\t\\color{red} x-Axis \\\\\n\t\\color{green} y-Axis \\\\\n\t\\color{blue} z-Axis \\\\\n\t\\color{orange} Position \\\\\n\\end{array}\n\\equiv\n\\bigg[\\begin{array}{rcl}\n\t\\color{red} 1\u0026\\color{red}0\u0026\\color{red}0\u00260 \\\\\n\t\\color{green}0\u0026\\color{green}1\u0026\\color{green}0\u00260 \\\\\n\t\\color{blue}0\u0026\\color{blue}0\u0026\\color{blue}1\u00260 \\\\\n\t\\color{orange}0\u0026\\color{orange}0\u0026\\color{orange}0\u00261 \\\\\n\\end{array}\\bigg]\n$$ \n\nWe don't really need the fourth column but 3x4 matrices dont \"exist\" in VEX. \n\n```C#\n// this continues the first point wrangle\n\nmatrix transform = set(X, Y, Z, P); // create matrix\n```\n\nHowever this will give us the following matrix with the ones in the fourth column\n\n\n$$\n\\bigg[\\begin{array}{rcl}\n\t\\color{red} X.x\u0026\\color{red}X.y\u0026\\color{red}X.z\u00261 \\\\\n\t\\color{green} Y.x\u0026\\color{green}Y.y\u0026\\color{green}Y.z\u00261 \\\\\n\t\\color{blue} Z.x\u0026\\color{blue}Z.y\u0026\\color{blue}Z.z\u00261 \\\\\n\t\\color{orange} P.x\u0026\\color{orange}P.y\u0026\\color{orange}P.z\u00261 \\\\\n\\end{array}\\bigg]\n$$\n\nTo fix this we can use the setcomp() function.\n\n```C#\n// this continues the first point wrangle\n\nsetcomp(transform, 0, 0, 3); // set row 1 col 4 to 0\nsetcomp(transform, 0, 1, 3); // set row 2 col 4 to 0\nsetcomp(transform, 0, 2, 3); // set row 3 col 4 to 0\n\n4@transform = transform; // create matrix attribute\n```\n\nWhich will give us the correct transformation matrix:\n\n$$\n\\bigg[\\begin{array}{rcl}\n\t\\color{red} X.x\u0026\\color{red}X.y\u0026\\color{red}X.z\u00260 \\\\\n\t\\color{green} Y.x\u0026\\color{green}Y.y\u0026\\color{green}Y.z\u00260 \\\\\n\t\\color{blue} Z.x\u0026\\color{blue}Z.y\u0026\\color{blue}Z.z\u00260 \\\\\n\t\\color{orange} P.x\u0026\\color{orange}P.y\u0026\\color{orange}P.z\u00261 \\\\\n\\end{array}\\bigg]\n$$\nto move the object to the center the inverted matrix has to be multiplied with the position.\n\n```C#\n// this goes in point wrangle 2\n\nmatrix transform = point(1, \"transform\", 0);\n\nv@P *= invert(transform);\nv@N *= matrix3(invert(transform));\nv@v *= matrix3(invert(transform));\n```\n\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/ExtractTransformationMatrix.hiplc)\n\n--- \n\n## Warping Space\n\n### Twisting\n\n![[notes/images/MatrixTwist.gif]]\n\n```C#\nmatrix xform = ident();\nvector bbox = getbbox_size(0);\nvector center = getbbox_center(0);\nvector axis = chv(\"axis\");\n\nfloat min = center.z - bbox.z / 2;\nfloat max = center.z + bbox.z / 2;\n\nfloat rotations = chf(\"rotations\");\n\nfloat angle = fit(@P.z, min, max, -PI*rotations, PI*rotations);\n\nrotate(xform, angle, axis);\n\n@P *= xform;\n```\n\n### Black Hole Vortex\n\nJohn Kunz demonstrated this technique in his [Pure VEX Workshop Week 6](https://www.youtube.com/watch?v=DA0ZuIJ-W7s). \n\n![[notes/images/Pasted image 20220604182753.png]]\n\n```C#\nmatrix xform = ident();\n\nvector angle = (1.0 / exp(length(@P)))*chv(\"rotationscale\");\nfloat scale = pow(1 - 1 / exp(length(@P)), 5);\nvector translatedir = chv(\"translatedir\")*chf(\"translatescale\");\nvector translate = set(translatedir.x, translatedir.y, translatedir.z) / exp(length(@P));\n\nrotate(xform, angle.x, set(1,0,0) );\nrotate(xform, angle.y, set(0,1,0) );\nrotate(xform, angle.z, set(0,0,1) );\n\nscale(xform, scale);\n\ntranslate(xform, translate);\n\n@P *= xform;\n```\n\n---\n\nsources / further reading\n- [Linear Transformations - 3Blue1Brown](https://www.3blue1brown.com/lessons/linear-transformations)\n- [Houdini Tutorial | Extracting transformation matrix with VEX - Paweł Rutkowski](https://vimeo.com/284712920)\n- [Pure VEX Workshop Week 6: Warping with Matrices - John Kunz](https://www.youtube.com/watch?v=DA0ZuIJ-W7s)\n- [Houdini Translate Rotate Scale Bend with Matrices \u0026 Quaternions in VEX - Nodes of Nature](https://www.youtube.com/watch?v=e9qLWS2La28)\n- [Matrix Transformation- Mohamad Salame](https://www.artstation.com/blogs/mohamad_salame1/v6eP/matrix-transformation)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Measuring-Error":{"title":"Measuring Error","content":"There are several common ways to measure error mathematically, especially in the context of machine learning and statistics. A lot of these come in handy for all sorts of procedural setups that need to compare a bunch of numbers. Here are a few of the most widely used methods:\n\n1. **Mean Absolute Error (MAE)**: This measures the average absolute differences between predicted and actual values. It’s simple to understand and gives equal weight to all errors.\n\n$MAE=1n∑i=1n∣yi−y^i∣$\n\n2. **Mean Squared Error (MSE)**: This measures the average of the squared differences between predicted and actual values. Squaring the errors means larger errors have a bigger impact, which can be useful in some cases.\n\n$MSE=1n∑i=1n(yi−y^i)2$\n\n3. **Root Mean Squared Error ([[notes/RMSE|RMSE]])**: This is the square root of the MSE, bringing it back to the same scale as the original values. It’s sensitive to outliers like MSE but provides a more interpretable measure of error.\n\n$\\text{RMSE} = \\sqrt{\\text{MSE}}$\n\n4. **Mean Absolute Percentage Error (MAPE)**: This expresses the error as a percentage, which can be useful for understanding the error relative to the size of the values being predicted.\n\n$\\text{MAPE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right| \\times 100$\n\n5. **Huber Loss**: This combines the benefits of MAE and MSE. It behaves like MSE when errors are small and like MAE when they are large, making it robust to outliers.\n\n$L_{\\delta}(y, \\hat{y}) = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 \u0026 \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\\delta (|y - \\hat{y}| - \\frac{1}{2}\\delta) \u0026 \\text{otherwise}\\end{cases}$\n\n6. **Cross-Entropy Loss**: Commonly used in classification problems, this measures the difference between the predicted probability distribution and the true distribution. It’s particularly useful for multi-class classification tasks.\n\n---\n\nsources / further reading:\n- [All about loss functions like MSE, MAE, RMSE etc - Abhishek Jain](https://medium.com/@abhishekjainindore24/all-about-loss-functions-like-mse-mae-rmse-etc-36596e3802f5)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/NVIDIAs-Warp-for-Houdini":{"title":"NVIDIAs Warp for Houdini","content":"\n\"NVIDIA Warp is a Python framework that gives coders an easy way to write GPU-accelerated, kernel-based programs in NVIDIA Omniverse™ and OmniGraph.\" - https://developer.nvidia.com/warp-python\n\nChristopher Crouzet built an HDA that exposes the Warp framework to Houdini.\n\nhttps://github.com/christophercrouzet/nvidia-warp-houdini\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Node-Shapes-and-Colors":{"title":"Node Shapes and Colors","content":"\n### Getting All Node Shape Names\n\n```Python\n# Get All Node Shape Names\n\neditor = hou.ui.paneTabOfType(hou.paneTabType.NetworkEditor)\n\nprint(editor.nodeShapes())\n\n# All available Node Shape Names\n\n# ('rect', 'bone', 'bulge', 'bulge_down', 'burst', 'camera', \n# 'chevron_down', 'chevron_up', 'cigar', 'circle', 'clipped_left', \n# 'clipped_right', 'cloud', 'diamond', 'ensign', 'gurgle', 'light', \n# 'null', 'oval', 'peanut', 'pointy', 'slash', 'squared', 'star', \n# 'tabbed_left', 'tabbed_right', 'tilted', 'trapezoid_down', \n# 'trapezoid_up', 'wave')\n```\n\n### Setting a Default Color and Shape for an HDA\n\nCreate a `OnCreated` script in the Scripts tab of the type properties of the HDA and paste the following code:\n\n```Python\nkwargs[\"node\"].setColor( hou.Color(1,0,0) )\nkwargs[\"node\"].SetUserData( 'nodeshape', 'wave')\n```\n\n---\n\nsources / further reading:\n- [List of available Houdini node shapes - Morphingdesign](https://gist.github.com/morphingdesign/0d1af23664a8ce356fca0c7668f4dd85)\n- [Houdini: Organizing and managing - Simon Houdini](https://www.youtube.com/watch?v=vwB-gY7j22s)","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Nuke":{"title":"Nuke","content":"\n- [[notes/cat Files |cat Files]]\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Obsidian-and-Quartz":{"title":"Obsidian and Quartz","content":"## Meta Things About This Page\n- [[notes/FX Notes Formatting|FX Notes Formatting]]\n- [[notes/Gifs for Obsidian |How to create Gifs for Obsidian (Linux)]]","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Op-Operators":{"title":"Op Operators","content":"\n## Summary\n\n\"[Op Operators I will Never Memorize](https://www.artstation.com/blogs/mohamad_salame1/DlQG/op-operators-i-will-never-memorize)\" is a fantastic blog post by Mohamad Salame that summarizes all the obscurities and pitfalls you will face when having to deal with the `op` syntax.\n\n## Syntax\n\n- Selecting the current node: `\".\"`\n- Relative path: `../nodename`\n- Absolute path: `/obj/nodename`\n\n## Most Useful Ones\n\n### HDA Files: opdef\n\nGives access to files stored in an HDA\n\n```hscript\n`opdef:..?filename.ext`\n```\n\nor a .hda library\n\n```hscript\n`oplib:..?filename/ext`\n```\n\n### Connected Inputs: opninputs\n\n\u003e [!quote] **DOCs**\n\u003e \n\u003e Returns the number of the highest connected input. This is _not_ the number of connected inputs. If a node has four inputs and the fourth input is connected, `opninputs` will return `4`. If the first and third inputs are connected, `opninputs` will return `3`.\n\n```hscript\n`opninputs(\".\")`\n```\n\n### Node Name: opname\n\nReturns the name of the specified node\n\n```hscript\n`opname(\".\")`\n```\n\n### Texture From COPs: op\n\nCan be used to pipe the output of a COPs network live into a texture input\n\n```hscript\nop:`opfullpath(\"../cop2net1/OUT_COPS\")`[$F]\n```\n\n### Use Input Name for Exports: opinput\n\nFlexible ROP outputs\n\n```hscript\n$HIP/geo/`opinput(\".\", 0)`.bgeo.sc\n```\n\n![[notes/images/Pasted image 20230227162722.png]]\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Other":{"title":"Other","content":"\n### CG \u0026 VFX Tools\n- [[notes/Nuke |Nuke]]\n- [[notes/Redshift |Redshift]]\n- [[notes/DaVinci Resolve |DaVinci Resolve]]\n\n### Multimedia\n- [[notes/TouchDesigner |TouchDesigner]]\n\n### Tech \u0026 Programming\n- [[notes/Bash and Git |Bash and Git]]\n\n### Contributing\n- [[notes/Obsidian and Quartz |Obsidian and Quartz]]\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/POP":{"title":"POPs","content":"### Geometry Motion Blur with Respect to Relative Camera Movement\n\nWhen rendering fast moving particles you want to see the beautiful curved motion blur (think sparks). To get this you usually need to have a huge amount of subframe samples. This can easily increase memory footprint by 10x or more, because you have to load all those positions (and other attributes stored on the points) into RAM. There is however a way to create geometry trails that also respect the camera movement. This gives you full control over every aspect, while efficiently rendering.\nWhen I was first shown this setup it blew my mind, how simple it is.\n\n![[notes/images/geo_NDC_mb.gif]]\n\n- left: shows the viewport from a free camera\n- right: shows the viewport from the perspective of cam1 circling around a couple of points moving up and down\n\nThe geometry is just a couple of points scattered on a grid and moved with an animated noise. It doesn't circulate at all!\n\n![[notes/images/basePointMovement.gif]]\n\nAll the vertical smearing is a result of the fast moving camera.\n\n![[notes/images/cameraCircle.gif]]\n\nThe setup consists of 2 main parts:\n\n1. getting the points in \"screen space\" (also known as NDC) (purple)\n2. calculating and collecting the geometry at different points in time with a timeshift in a feedback loop (red)\n\n![[notes/images/geomb_feedbackloop.png]]\n\nIn the end everything gets connected to result in smooth resampled curves.\n\n##### NDC?\n\nThe heavy math lifting is done by the `to NDC` \u0026 `from NDC` VOP nodes.\n\n**NDC** stands for \"normal device coordinates\". This node maps the point positions from 3d space to a cuboid shape where x and y coordinates represent the x and y axis of the camera image and z the depth.\n\n![[notes/images/ndcspace_demo.png]]\n\nThis allows you to work in a coordinate system that takes into account how points move through the 2D image the camera sees.\n\nIn this example we won't store the ndc space in `P` but in a separate attribute `ndc`.\n\n![[notes/images/tondc.png]]\n\n##### The Feedback Loop\n\nThe Ctrl Null allows you to set the two most important parameters for motion blur. \n1. sample count\n2. shutter offset\n\n![[notes/images/geomb_feedbackloop_expr.png]]\n\nAll the loop does is shifting time incrementally `1/sample count` steps forward and merging the results. The shutter offset specifies where you start. \n- -1 means the sampling will start at the time of the last frame and end on the current one. \n- 0 would be half backwards half forwards \n- 1 only forwards from the current position to the position 1 frame in the future.\n\nThe Result looks something like this:\n\n![[notes/images/onlytrail.png]]\n\nBut wait.. No camera motion blur. This is the same result a trail node would give you. The camera motion blur is not visible yet, because the ndc attribute hasn't been applied.\n\n![[notes/images/fromndc.png]]\n\nThis gives you a correctly \"screen space\" trailed point cloud you can easily connect using an add node if you have ids.\n\n![[notes/images/ndcapplied.png]]\n\nafter the `add` SOP:\n\n![[notes/images/add_trails.png]]\n\nNow you can do all your typical SOPs tricks on it:\n- add noise\n- resample\n- color it along curve u\n- width \n- alpha blending \n- etc.\n\nThis even allows for some cool stylized effect setups:\n\n![[notes/images/stylizedMBeffect.gif]]\n\nYou can download the hip here: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/geo_ndc_mb.hip)\n\n### Initializing Attributes inside DOPs\n\nSometimes you want to initialize / build some attribute based on the geometry of the frame before. To do so you can plug it in the second input of the pop solver and then use it in the third. \n\n![[notes/images/Pasted image 20231018160005.png]]\n\nThis is very useful because the only thing that happens in the pop solver before processing the second input (yellow) is the reaping of unnecessary particles to avoid overhead. After that the usual pop solver micro solvers are run and in the end all the user instructions for the next frame are applied (green).\n\n![[notes/images/Pasted image 20231018160140.png]]\n\n---\n\nsources / further reading:\n- \n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Principal-Component-Analysis":{"title":"Principal Component Analysis","content":"“Principal component analysis is a versatile statistical method for reducing a cases-by-variables data table to its essential features, called principal components. Principal components are a few linear combinations of the original variables that maximally explain the variance of all the variables.”\n\n![[notes/images/200w.gif]]\n\nDon't Worry!\n\nIt's just a method of summarizing some data typically used for statistics and machine learning applications.\n\nI stole the following wine analogy from this excellent [stack exchange explanation](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579). I recommend reading this if you really want to understand it! The author goes into a lot more mathematical detail and explains it for different knowledge levels. I tried to explain just enough to make it useful for CG and not super confusing.\n\nImagine you have some wine bottles standing in front of you on the table. We can describe each wine by its colour, how dry or sweet it is, how strong it is, how old it is, and so on. We can compose a whole list of different characteristics for each wine.\n\n![[notes/images/winebottlesillustrationsorted.png]]\n\nBut many of them will measure related properties and so will be redundant. If so, we should be able to summarize each wine with fewer components! This is what PCA does.\nInstead of say 10 you may end up with 3 components that describe your data accurately enough\nNever 100% but close enough.\n\n![[notes/images/winedatatable_slide.png]]\n\nPCA is not selecting some characteristics and discarding the others. Instead, it constructs new characteristics called components that turn out to summarize our list of wines well. Those are linear combinations of the original data. One could look like this:\n\n```C\nC1 = color *  0.5 + age * 2 - sweetness\n```\n\nIn fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as possible. Those components are ordered by importance, which comes in handy later.\n\n![[notes/images/winedatatable2_slide.png]]\n\nNote: all the data is generally in numerical form. So things like sweetness, texture etc would be encoded as a float range.\n\nIn 2d this could look something like this \n\n![[notes/images/winechart.0001.jpg|400]]\n\nYou have 2 axis, color and age and you have a scatter plot of different wines. This is of course not how wine works but lets just pretend that it gets more colorful over time.\n\nIf those two properties are related like that you can compress them because both store the same underlying information \u003e time passed\n\nThe first PCA component for that data looks like this:\n\n![[notes/images/winechart.0002.jpg|400]]\n\nIt’s the direction through your data where the most variance occurs. The longest distance essentially.\n\nThe second component would look like this:\n\n![[notes/images/winechart.0002.2.jpg|400]]\n\nIt is perpendicular to the first one and the second most important variance of the dataset.\n\nWhat you can do then is project your original data samples onto the new components and calculate weights (how much of which component do you need) to reconstruct the original data point. \n\nBy doing that you create a new coordinate space where C1(red) is your new X axis and C2(green) is you new Y axis.\n\n![[notes/images/winechart.0004.jpg|400]]\n\nIn 2d this isn’t very useful and doesn’t compress anything, but in highdimensional space you can compress your data a lot, while still maintaining most of the valuable information.\n\nFor general CG purposes it's not too important to understand the math completely, but how to use it and what its good for.\n\nSo ..\n## How is this useful for CG?\n\n### General CG\n\n- [[notes/Bounding Box Orientation on Arbitrary Point Clouds with PCA|Bounding Box Orientation on Arbitrary Point Clouds with PCA]]\n- [[notes/PCA Dejitter|PCA Geometry Dejitter]]\n### ML in Houdini\n\n- [[notes/ML Groom Deformer|ML Groom Deformer]]\n- [[notes/ML Sketching|ML Sketching]]\n\n---\n\nsources / further reading:\n- [Data Analysis 6: Principal Component Analysis (PCA) - Computerphile](https://www.youtube.com/watch?v=TJdH6rPA-TI)\n- [Really Good PCA explanation - stats.stackexchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Projects-and-RnD":{"title":"Projects / R\u0026D","content":"- [[notes/ML Groom Deformer|ML Groom Deformer]]\n- [[notes/ML Sketching|ML Sketching - Teaching Houdini to Understand your Drawings]]\n- [[notes/ML Castles |ML  Castles]]\n\t- [[notes/Tower Sketcher |Tower Sketcher]]\n\t- [[notes/Vegetation and Erosion Prediction |Vegetation Erosion Prediction]]\n- [[notes/Digit Recognition in Houdini |Digit Recognition in Houdini]]","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Python-In-Houdini":{"title":"Python In Houdini","content":"\n## Autocomplete for Imported Libraries\nTo get autocomplete for external libraries import the packages in the Houdini Python Shell before using any of them in e.g. a Python SOP or an HDA Python Module.\n\n---\n\nsources / further reading:\n- [Python Autocomplete Trick - Richard C Thomas](https://twitter.com/doescg/status/1560999925940371456)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Python-Scripting-in-Resolve":{"title":"Python Scripting in Resolve","content":"\n## Setup\n\n### Environment Variables\nWhen setting up the API access on windows with environment variables make sure to NOT use nested variables as python will read those as strings. \n\nInstead of using the variables as shown in the provided README.txt from Black Magic Design make sure to replace all nested variables with hardcoded paths.\n\nWindows:\n\n```bash\nRESOLVE_SCRIPT_API=\"C:\\\\ProgramData\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Support\\\\Developer\\\\Scripting\\\\\"\nRESOLVE_SCRIPT_LIB=\"C:\\\\Program Files\\\\Blackmagic Design\\\\DaVinci Resolve\\\\fusionscript.dll\"\nPYTHONPATH=\"%PYTHONPATH%;C:\\\\ProgramData\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Support\\\\Developer\\\\Scripting\\\\Modules\\\\\"\n```\n\nExplained here: [Python scripting error - Blackmagic Forum](https://forum.blackmagicdesign.com/viewtopic.php?f=21\u0026t=137340)\n\n### Imp Module\n\n\n\n---\n\nsources / further reading:\n- [Unofficial DaVinci Resolve Scripting Documentation](https://deric.github.io/DaVinciResolve-API-Docs/)\n- [Python scripting error - Blackmagic Forum](https://forum.blackmagicdesign.com/viewtopic.php?f=21\u0026t=137340)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Quaternion-Euler-Rotations":{"title":"Quaternion \u0026 Euler Rotations","content":"### Quaternions\n\nexpressed as 4 numbers `vector4 = [x, y, z, w]`\n\nIt's usually used to define rotational transformation in 3D Space. To do so it needs 2 types of information: \n- rotational angle   $\\theta$\n- rotational axis   $A$\n\nThe 4 vector values of a quaternion are calculated in the following way:\n\n$q = (sin(\\frac{\\theta}{2})*Ax, sin(\\frac{\\theta}{2})*Ay, sin(\\frac{\\theta}{2})*Az, cos(\\frac{\\theta}{2}))$\n\nIn VEX we can use the quaternion function which accepts an angle in radians and an axis vector to propagate the vector4 accordingly.\n\n### Rotating Vectors\n\n```C#\n//this goes in a point wrangle\n\nfloat angle = chf(\"angle\");\nvector axis = normalize(chv(\"axis\"));\n\nvector4 rot = quaternion(radians(angle), axis);\n@P = qrotate(rot, @P);\n```\n\n### Euler Rotation\n\nWhile Quaternians define the rotational transformation with an angle around a specified axis, Euler rotation is defined by 3 Parameters (compare `Transform Node` x, y, z).\n\nTo convert Euler rotations to quaternions we need to specify the rotation order:\n\n```C#\nvector angles = chv(\"angles\");\nangles = radians(angles);\n\nint order = XFORM_XYZ; // integer arguments are defined in $HH/vex/include/math.h\n\nvector4 rot = eulertoquaternion(angles, order);\n\np@quat = rot;\n```\n\nRotation Order Integer Arguments as defined in `$HH/vex/include/math.h`:\n\n```C++\n#define XFORM_XYZ 0 // Rotate order X, Y, Z\n#define XFORM_XZY 1 // Rotate order X, Z, Y\n#define XFORM_YXZ 2 // Rotate order Y, X, Z\n#define XFORM_YZX 3 // Rotate order Y, Z, X\n#define XFORM_ZXY 4 // Rotate order Z, X, Y\n#define XFORM_ZYX 5 // Rotate order Z, Y, X\n```\n\nSame thing works backwards:\n\n```C#\nvector4 quat = p@quat;\n\nvector euler = quaterniontoeuler(quat, XFORM_XYZ);\n\nv@euler = degrees(euler);\n```\n\n\n### Blending Quaternions with `slerp()`\n\nMatrices can do most of what quaternions can do and more (translation \u0026 scale). However, one thing that quaternions enable you to do is using the `slerp` function to blend smoothly between two rotational transformations.\n\n![[notes/images/QuaternionSlerpCut.gif]]\n\nIn this example the two orientations get initialized buy rotating a line in two different ways and extracting each quaternion.\n\n```C#\n// this goes in point wrangle \"quat_1 \u0026 2\"\n\nfloat angle = chf(\"angle\");\nvector axis = normalize(chv(\"axis\"));\n\nvector4 rot = quaternion(radians(angle), axis);\n@P = qrotate(rot, @P);\n\np@quat = rot;\n```\n\nThen we can blend rotationally between the two orientations with the `slerp` function and apply the blended result.\n\n```C#\n// this goes in point wrangle  \"slerp\"\n\nvector4 quat1 = point(1, \"quat\", 1);\nvector4 quat2 = point(2, \"quat\", 1);\n\nvector4 rot = slerp(quat1, quat2, chf(\"blend\"));\n\n@P = qrotate(rot, @P);\n@N = qrotate(rot, @N);\n```\n\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/QuaternionSlerp.hiplc)\n\n### Using `dihedral` Function to Orient Vectors\n\nThe dihedral function creates a quaternion that describes the rotational transformation between two given vectors. This can be used to switch between two orientations of a mesh.\n\n![[notes/images/Pasted image 20220604005136.png]]\n```C#\nvector v1 = normalize(point(1, \"P\", 1));\nvector v2 = normalize(point(2, \"P\", 1));\n\nvector4 quat = dihedral(v1, v2);\n\n@P = qrotate(quat, @P);\n```\n\n### sources /  further reading\n- [[VEX for Algorithmic Design] E14 _ Quaternion Basics - \nJunichiro Horikawa](https://www.youtube.com/watch?v=MYRtwY-DQV8)\n- [Visualizing quaternions - Grant Sanderson (3blue1brown)](https://eater.net/quaternions)\n\n### Related\n- [[notes/Matrix Operations |Matrix Operations]]","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/RMSE":{"title":"RMSE","content":"Root Mean Squared Error (RMSE) is a widely used metric in statistics and machine learning to evaluate the accuracy of a model's predictions. It measures the average magnitude of the errors between predicted values and actual values, giving more weight to larger errors due to the squaring of differences.\n\n### How to Calculate RMSE\n\n1. **Calculate the Errors**: For each predicted value $(\\hat{y}_i​)$and actual value $({y}_i​)$, compute the difference (error):\n    \n    $ei=\\hat{y}_i - y_i​$\n    \n1. **Square the Errors**: Square each error to remove negative signs and give more weight to larger errors:\n    \n    $e_i^2$\n    \n1. **Calculate the Mean of Squared Errors**: Find the average of these squared errors:\n    \n    $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} e_i^2$\n    \n1. **Take the Square Root**: Finally, take the square root of the mean squared error:\n    \n    $\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} e_i^2}$\n\n### Now in VEX\n\n// point wrangle\n\n```C\nfloat input;\nfloat output;\n\n@e = output - input;\n@se = pow(@e, 2);\n\n// @e is the local error\n```\n\nPromote the attribute to detail using average mode and rename it to `@mse`\n\n// detail wrangle\n\n```C\n@rmse = sqrt(@mse);\n```\n\n### What RMSE Tells Us\n\n- **Units**: The RMSE is in the same units as whatever we’re predicting, making it easier to understand.\n- **Sensitivity**: RMSE really pays attention to outliers; big errors can swing the RMSE value quite a bit.\n- **Comparison**: A lower RMSE means our model is doing a better job. It’s often compared to other metrics, like Mean Absolute Error (MAE), to get a fuller picture of performance.\n\nIn short, RMSE is a handy way to summarize how accurate our predictions are, especially in regression tasks!\n\nHere's a list of other ways to measure error: [[notes/Measuring Error|Measuring Error]]\n\n---\n\nsources / further reading:\n- [What is Root Mean Square Error?](https://c3.ai/glossary/data-science/root-mean-square-error-rmse/)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Reaction-Diffusion":{"title":"Reaction Diffusion","content":"Reaction-diffusion is a concept borrowed from biology, and in computer graphics, it helps create interesting textures and patterns, like the ones you see on animal skins or in nature.\n\n![[notes/images/reactiondiffusion_banner.jpg]]\n\nIn Uni I worked on two \"shortfilm\" projects using ferrofluids that show this effect really well in reality:\n- [CONVERGENCE](https://vimeo.com/393791260)\n- [INFECTION](https://vimeo.com/452001942)\n### How It Works:\n\n1. **Two Chemicals**: Imagine there are two substances, often called A and B. They react with each other and spread out in space (like a spill).\n\n2. **Reaction**: When A and B mix, they change each other. For example, A might help create more of B, while B might slow down A. This interaction is what creates interesting patterns.\n\n3. **Diffusion**: Both substances also move around over time. They \"diffuse\" out from where they're concentrated to where they're less concentrated, like how a drop of food coloring spreads in water.\n\n4. **Feedback Loop**: The combination of the reaction and the diffusion creates a feedback loop. Sometimes, this leads to stable patterns (like stripes or spots) or dynamic ones that change over time. Those stable patterns are what we are usually after when trying to create this is CG.\n### In Graphics:\n\nYou can use reaction-diffusion to generate textures in 2D or 3D graphics. By tweaking the rules of how A and B interact and spread, they can create unique and complex designs that look organic.\n\nI recommend taking a look at Karl Sims' [browser demo](https://www.karlsims.com/rdtool.html) and all the odforce threads I linked below.\n\n---\n\nsources / further reading:\n- [Reaction-Diffusion Tutorial - Karl Sims](http://www.karlsims.com/rd.html)\n- [Fun with Reaction-Diffusion in Houdini | Dan Wills | SIGGRAPH Asia 2019](https://www.youtube.com/watch?v=K_7TkoIkFhk)\n- [Differential curve growth - Odforce](https://forums.odforce.net/topic/25534-differential-curve-growth/)\n- [Houdini Reaction Diffusion - Odforce](https://forums.odforce.net/topic/25906-houdini-reaction-diffusion/)\n- [Diffusion - Odforce](https://forums.odforce.net/topic/24406-diffusion/)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Regularization":{"title":"Regularization","content":"\n\n\n---\n\nsources / further reading:\n- \n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Resources":{"title":"Resources","content":"# Downloads \u0026 Scripts\n\n- [VEX Snippets](https://github.com/jakobringler/H_VEX_snippets)\n\n- [ML Castles](https://github.com/jakobringler/H_ML_Castles/)\n- [Pytorch Digit Recognition in Houdini](https://github.com/jakobringler/H_pytorch_digitrecognition)\n\n---\n# Links\n\n## CG \u0026 VFX\n\n### Houdini\n##### Tutorials\n- [Entagma - Moritz Schwind \u0026 Manuel Casasola Merkle](https://entagma.com/) procedural \u0026 motion design\n- [VEX for Algorithmic Design - Junichiro Horikawa](https://www.youtube.com/playlist?list=PLzRzqTjuGIDhiXsP0hN3qBxAZ6lkVfGDI) VEX, procedural design\n\n##### Blogs \u0026 Wikis\n- [cgwiki - Matt Estela](https://www.tokeru.com/cgwiki/?title=Houdini) all things houdini\n- [procegen - Konstantin Magnus](https://procegen.konstantinmagnus.de/) procedural workflows, automation\n- [wiki - John Kunz](https://wiki.johnkunz.com/index.php?title=Main_Page) VEX, resources\n- [Toadstorm Nerdblog - Henry Foster](https://www.toadstorm.com/blog/) MOPs, procedural workflows \u0026 python\n- [Houdini Gubbins](https://houdinigubbins.wordpress.com/) technical, VEX, math, geometry, algorithms\n- [FX Thinking - Alessandro Pepe](https://pepefx.blogspot.com/) FX, technical\n- [HouPy Wiki - Luke Van](https://www.houpywiki.com/) python in houdini\n- [Website / Wiki - Deborah R. Fowler](https://www.deborahrfowler.com/index.html) very technical (this has to be the worst best website)\n- [Diffuse FX - Grayden Solman](https://diffusefx.com/) FX, technical\n- [Blog - Nick Taylor (Aeoll)](https://www.nicholas-taylor.com/blog) FX, technical, motion design\n- [Blog - Jake Rice](https://jakerice.design/blog/) VEX, math, technical\n- [Blog - Sergen Eren](https://sergeneren.com/) FX, technical, clouds, photon simulation\n- [Blog - Rok Andic](https://www.rokandic.com/blog) rigging, animation\n- [Blog - Chris Turner](https://www.chrisbturner.com/blog) technical\n- [Blog - Matt Ebb](http://mattebb.com/blog/) technical\n- [Blog - Jason Harmon (What I Found)](https://whatifound.net/) FLIP, CHOPs\n- [lex.ikoon](https://lex.ikoon.cz/) VEX, Python, technical\n- [rendereverything - Gatis Kurzemnieks](https://www.rendereverything.com/blog/) VEX, technical\n- [1 Minute VEX - Aaron Smith](https://aaronsmith.tv/1-Minute-VEX) VEX\n- [Houdini Tutorials Tailored for Mathematicians - Discretization - TU Berlin](http://wordpress.discretization.de/houdini/) math, technical\n- [Blog - Discretization - Visualization Course at TU Berlin](http://wordpress.discretization.de/ddg2018/) math, technical\n- [Daily Hip - Eetu](https://dailyhip.wordpress.com/) technical, hip files\n- [Crowds Plus - Thomas K Christensen](https://crowdsplus.notion.site/)\n- [The Brain Extension - Iiro Kivistö](https://www.thebrainextension.com/) grooming, CFX\n- [Creature Garage - JesusFC](https://creaturegarage.com/) grooming\n- [Atom's Link Page - OdForce](https://forums.odforce.net/topic/25173-atoms-link-page/?tab=comments#comment-146668) - useful link list\n\n##### Best of odforce\n- [Differential Curve Growth](https://forums.odforce.net/topic/25534-differential-curve-growth)\n- [FLIP Sheeter Effect](https://forums.odforce.net/topic/18111-flip-smorganicsheeter-effect/)\n- [Smoke Solver | Tips and Tricks](https://forums.odforce.net/topic/31435-smoke-solver-tips-and-tricks/)\n- [Voronoi - Dynamic - Location Based Fracture](https://forums.odforce.net/topic/9119-voronoi-dynamic-location-based-fracture-wip/)\n##### Tech\n- [[notes/NVIDIAs Warp for Houdini |NVIDIAs Warp for Houdini]]\n- [Julia Wrangle in Houdini](https://github.com/pedohorse/yuria)\n- [HoudiniToTaichi](https://github.com/taichi-dev/taichi_houdini)\n\n##### Comparisons\n**FLIP**\n- [Houdini Flip Sim Flat Tank Particle Separation Comparison - Anthony Juno Han](https://www.youtube.com/watch?v=Zvjlg0m0ZlY)\n- [Houdini Flip Simulation Particle Fluid Surface Comparison - Anthony Juno Han](https://www.youtube.com/watch?v=4a3N2qeOF2Y)\n**Whitewater**\n- [Houdini | Whitewater tests - Attila Siops](https://www.youtube.com/watch?v=nW1pxpAw-4Y)\n- [Houdini | Beach Wave Whitewater tests (hip) - Attila Siops](https://www.youtube.com/watch?v=a38RFtpeLqc)\n- [Houdini | Rock Splash tests (comparison) - Attila Siops](https://www.youtube.com/watch?v=8sb_z2fl2hY)\n**Pyro**\n- [Houdini Pyro FX - Explosion comparison. - Cg Art School : CAS](https://www.youtube.com/watch?v=eMQYLipwFiM)\n- [Houdini Sparse Pyro FX - Campfire comparison. - Cg Art School : CAS](https://www.youtube.com/watch?v=grsev1LXOF0)\n- [Houdini Pyro FX - Billowy smoke comparison. - Cg Art School : CAS](https://www.youtube.com/watch?v=A3o2ZD_S5xE)\n\n### Misc \n- [Physically Based](https://physicallybased.info/) material value database\n- [CG Cinematography - Chris Brejon](https://chrisbrejon.com/cg-cinematography/) cinematography, lighting, color theory \u0026 management\n- [Book of USD - Remedy Entertainment](https://remedy-entertainment.github.io/USDBook/index.html) usd\n\n---\n## Math\n- [Blog - kynd.info](https://kyndinfo.notion.site/kyndinfo/) theory, design, simulation basics / quasi physics\n- [Essence of Linear Algebra - 3Blue1Brown](https://www.3blue1brown.com/topics/linear-algebra) theory\n- [The Beauty of Bézier Curves - Freya Holmér](https://www.youtube.com/watch?v=aVwxzDHniEw) theory\n- [Math Primer](https://aman.ai/primers/math/) theory, overview\n\n---\n## Deep Learning\n\n### Houdini\n- [Houdini Machine Learning - Edmond Boulet-Gilly](https://www.youtube.com/watch?v=WNEEokEq-Fg\u0026list=PLSie_1zkANDbn7wCD9kifPAp5wJmok02V) ML, PyTorch, technical\n\n### Theory\n- [Casual GAN Papers](https://www.casualganpapers.com/) theory, papers\n- [Aladdin Persson](https://www.youtube.com/c/AladdinPersson) + [Github Collection](https://github.com/aladdinpersson/Machine-Learning-Collection) theory, paper review, pytorch \u0026 tensorflow tutorials\n- [Shervine Amidi - Deep Learning Cheat Sheet](https://stanford.edu/~shervine/teaching/) theory\n\n### Courses\n- [ML YouTube Courses](https://github.com/dair-ai/ML-YouTube-Courses) \n\n---\n## Multimedia\n\n### TouchDesigner\n- [The Interactive \u0026 Immersive HQ](https://www.youtube.com/@TheInteractiveImmersiveHQ) tutorials\n- [ciphrd](https://ciphrd.com/) awesome reference and algorithm explanations\n- [elekktronaut](https://www.elekktronaut.com/tutorials) tutorials, patreon\n- [allTD](http://alltd.org/) tutorial collection (huge)\n\n---\n## Computer Science \n\n### Courses\n- [Open Source Society University - Path to a free self-taught education in Computer Science!](https://github.com/ossu/computer-science)\n- [Tutorialspoint - Algorithms and Datastructures](https://www.tutorialspoint.com/data_structures_algorithms/)\n\n### Libraries\n- [The Computational Geometry Algorithms Library](https://www.cgal.org/index.html)\n\n---\n# Tools\n\n## Workflow\n\n### Pipeline\n- [Prism Pipeline](https://prism-pipeline.com/) open source\n\n### Automation\n- [Power Automate for Win10/11](https://powerautomate.microsoft.com/en-us/) like iOS shortcuts, visual automation programming\n\n### QoL Tools\n- [Advanced Renamer](https://www.advancedrenamer.com/) file renamer\n- [Pure Ref](https://www.pureref.com/) reference board\n\n---\n# Writing\n\n### Blogs\n- [Blog Writing for Developers](https://rmoff.net/2023/07/19/blog-writing-for-developers/)","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Retiming-Techniques-for-Pyro":{"title":"Retiming Techniques for Pyro","content":"\n### Timescale in Simulation\n\nThe Timescale parameter let's you control the speed at which a solver simulates. However you have to make sure to scale up or down any forces that affect velocity accordingly. Using timescale to create slow motion might not always be the best solution, as it directly increases simulation time. (more frames more time etc.) Also matching a result you already have set up in slow motion is pretty tough.\n\n### Post Sim Retime \u0026 Timeshift\n\nWhen retiming volumes you can use the vel field from the simulation to advect the volume and calculate in between frames. Unfortunately this causes flickering as you jump from fully simulated sharp frames to  interpolated ones which tend to be a little more blurry. \n\n(e.g 10 10.5 11 11.5 etc.)\n\nTo fix this you can use a timeshift after the retime node and offset every frame by 0.25. Make sure to disable `Integer Frames`.\n\n```C\n$FF+0.25\n```\n\nThis gives you only interpolated frames and reduces flickerung drastically.\n\n(e.g. 10.125 10.625 11.125 11.625 etc.)\n\n\n---\n\nsources / further reading:\n- [Volume Retime Strategies #Houdini #VDB #Pyro Stream 2020 11 18 - John Kunz](https://www.youtube.com/watch?v=m48ynuhEFKY)\n- [Axiom Fundamentals - 17 - Time Scale ( Slow-Motion ) - Theory Accelerated](https://www.youtube.com/watch?v=U6NsyCPfCI0)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Scripting-in-Houdini":{"title":"Scripting in Houdini - Talk Notes","content":"\nThese are my personal notes about Paul Ambrosiussen's Talk [Python Scripting - Houdini](https://www.youtube.com/watch?v=CxoVzsxiruY\u0026t=2605s).\n\n### Writing Code\nuse an external editor for anything longer than a couple of lines\n\n##### Node Parameter Editor\n- worst way\n- limited space\n- copy pasting to external editors for improved python features is prone to errors\n\n##### Expression Editor\n- okayish\n- more space\n- limited editor features\n- same copy pasting problem\n- useful in PDG \u003e\u003e displays most common pdg python functions in a column\n\n##### External Editor\n- install SideFX Labs to be able to link an external editor (Sublime, VS Code etc.) to write code\n- VS Code has a VEX language extension to enable auto complete\n\n### Raising Errors, Warnings \u0026 Messages\nErrors and Warnings are displayed in the node's info section while Messages trigger a popup window \n\n```Python\nerror = \"Error!\"\n\nif not something:\n\traise hou.NodeError(error)\n```\n\n```Python\nwarning = \"Warning?\"\n\nif not something:\n\traise hou.NodeWarning(warning)\n```\n\n```Python\nmessage = \"Message.\"\n\nif not something:\n\traise hou.ui.displayMessage(message)\n```\n\n### Run, Execute \u0026 Communicate with Thrid Party Applications\n\n##### Subprocess\nlibrary that ships with Python and lets you run other programs\n\n```Python\nimport subprocess\n```\n\n##### CMD Strings\nTo run another program you have to build a command (cmd) string. Essentially a single commandline snippet that runs some executable with all the necessary parameters.\n\n```Python\nexecutablepath = \"path/to/program.exe\"\nvalue = 5 # some value from Houdini\n\ncmd = [executablepath]\n\nif something:\n\tcmd.append(\"-p\") # append a parameter\n\tcmd.append(value) #parameter value\n```\n\nThis results in the following cmd string: `path/to/program.exe -p 5`\n\n##### Running the Subprocess\nto run the command you can use the `call()` function\n\n```Python\n# disable IM dialog box (windows specific)\nsi = None\nif os.name == 'nt':\n\tsi = subprocess.STARTUPINFO()\n\tif hide_dialog:\n\t\tsi.dwFlags != subprocess.STARTF_USESHOWWINDOW\n\nsubprocess.call(cmd, startupinfo=si)\n```\n\nIn case you want to get some geometry back this function overwrites any input geo and returns the new one instead.\n\n```Python\ngeo.loadFromFile(path)\n```\n\n### Run Code Based on UI Events\n[Relevant Doc Pages](https://www.sidefx.com/docs/houdini/hom/locations.html#asset_modules)\n\nWhen building HDAs it's possible to use python callbacks to trigger scripts with custom buttons or on events such as `OnCreated()` or `OnInputChanged()`. For a full list of events have a look at the [DOCs](https://www.sidefx.com/docs/houdini/hom/locations.html#asset_events).\n\n### Shelf Tools\nEasiest way is to select a node tree and drag and drop the nodes on an empty shelf spot. this will automatically create all the necessary script to recreate the setup on the push of the shelf button. Unfortunately this creates quite messy and bloated code.\n\nTo write it yourself you can start by creating a 'New Tool' on a shelf and importing the hou module. To easily find all the necessary functions, and python objects you can drag and drop nodes inside the scripting panel which will give you the correct syntax to access the data.\n\n### Pre and Post Save\nyou can trigger scripts before and after a hip file is saved adding code to `HOUDINIPATH/scripts/beforescenesave.py \u0026 afterscenesave.py` -\u003e [DOCS](https://www.sidefx.com/docs/houdini/hom/locations.html#run-scripts-before-and-or-after-saving-the-scene-hip-file)\n\n### Processing Node Network / Script Analysis\nCounting how often each node gets used (useful to figure out support and update importance in bigger studios):\n\n```Python\nimport hou\nfrom collections import Counter\n\nparentnode = hou.node(\"/obj/geo1\")\nallnodes = parentnode.children()\n\nnodenames = [x.type().name() for x in allnodes]\n\nprint(Counter(nodenames))\n```\n\n### Dropdown Menus\nEvery Houdini Menu is described by a .xml file that describes how it's constructed and what items it consists of. Take a look at SideFX Labs `MainMenuCommon.xml`.\n\nPython Scripts can be added in an xml in the following way:\n\n```xml\n\u003csubmenu id=\"name_menu\"\u003e\n\t\u003cscriptItem id=\"itemid\"\u003e\n\t\t\u003clabel\u003eLABEL\u003c/label\u003e\n\t\t\u003cinsertAtIndex\u003e10\u003c/insertAtIndex\u003e\n\t\t\u003cscriptCode\u003e\n\t\t\t\u003c![CDATA[\n\t\t\tPYTHONSCRIPT\n\t\t\t]]\u003e\n\t\t\u003c/scriptCode\u003e\n\t\u003c/scriptItem\u003e\n\u003c/submenu\u003e\n```\n\n### Startup Scripts\ncan be used to sanity check or load default scenes etc. on startup\n\n`pythonrc.py` runs on startup (as many as found in $HOUDINIPATH)\n\n`123.py` runs when Houdini is started without a scene (only first found)\n\n`456.py` runs after a scenefile is loaded (only first found)\n\n\n---\n\nsources / further reading:\n- [Python Scripting - Houdini - Paul Ambrosiussen](https://www.youtube.com/watch?v=CxoVzsxiruY\u0026t=2605s)\n\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Setting-up-Anaconda-and-Houdini-for-3D-Deep-Learning-on-Linux":{"title":"Setting up Anaconda and Houdini for 3D Deep Learning on Linux","content":"\n### Installation\n\nTo get started install [Anaconda](https://anaconda.org/) and run the following commands in your terminal:\n\n( **hou** is the name of the new enviroment and python **3.7.4** is required because this is the version used in Houdini )\n\n```bash\nconda create -n hou python=3.7.4\n```\n\n```bash\nconda activate hou\n```\n\nThen install libraries like for example [PyTorch 3D](https://github.com/facebookresearch/pytorch3d). ( For simplicities sake we will go forward with vanilla [PyTorch](https://pytorch.org/). )\n\n```bash\nconda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch\n```\n\nAs [jpparkeramnh](https://www.sidefx.com/profile/jpparkeramnh/) pointed out in [this](https://www.sidefx.com/forum/topic/58397/) SideFX forum post you have to export the **LD_PRELOAD** variable.\n\n```bash\nexport LD_PRELOAD=$CONDA_PREFIX/lib/libpython3.7m.so\n```\n\n### .bashrc Modifications\n\nI also recommend sourcing Houdini in the terminal.\n\nTo do so first open the .bashrc in the terminal\n\n```bash\nnano .bashrc\n```\n\nand add the following lines to your .bashrc\n\n```bash\ncd /opt/hfs19.0/\nsource ./houdini_setup\ncd ~\nconda activate hou\nexport LD_PRELOAD=$CONDA_PREFIX/lib/libpython3.7m.so\nalias expenv='LD_PRELOAD=$CONDA_PREFIX/lib/libpython3.7m.so'\n```\n\n-   The first 3 lines run the houdini setup bash script\n-   Line 4 activates a default environment ( for me “hou” )\n-   Line 5 exports the LD_PRELOAD varible of the currently activated environment ( hou )\n-   Line 6 creates an alias to export the currently activated environment from the terminal\n\nNow you should be able to just activate your respecitve conda environment, export the correct LD_PRELOAD variable by typing “**expenv**” and then run houdini from there by typing “**houdini**“.\n\nTo check if everything is running as expected open the Houdini Python shell and type:\n\n```python\nimport torch \n```\n\nIf it doesn’t give you any errors you should be good to go.\n\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Slit-Scanning":{"title":"Slit Scanning","content":"## History\n\nSlit scanning is a cool photographic technique that captures motion over time by using a narrow slit to take images of a scene. Instead of snapping a traditional photo, it exposes only a thin strip of the image at a time. This way, it records movement in a way that creates a unique and often surreal effect.\n\nThe technique has roots in early photography, but it gained a lot of attention in the 20th century, especially with artists and filmmakers who wanted to explore time and space in their work. It’s often associated with experimental films and art installations, where the visuals can twist and stretch in unexpected ways. Think of it as capturing a moment while also showing how it unfolds over time—kind of like a time-lapse but with a creative twist.\n\nIt’s fascinating how something so simple can produce such mind-bending results!\n\n## Slit Scanning on Videos\n\nImages are cool, but videos are much cooler! Houdini being Houdini it's easy enough to set something like this up for animations. And you don't even have to use a slit to do the \"scanning\". you can use arbitrary values on each pixel to timeshift to the past and future which let's you create some trippy videos like this:\n\n![[notes/images/violin-side-01.gif]]\n\n\u003e [!quote] Source:\n\u003e \n\u003e The amazing RBD sim and render is from [Gurmukh Sandhu](https://ca.linkedin.com/in/gurmukh-sandhu-972ba3257), who was so kind to let me use it for this experiment\n\nYou can see the impact and destruction in some parts of the image before the ball hit the violin. 3D video? somewhat!\n\nThe setup is pretty straight forward:\n- create a bunch of points that will hold your color values (should match your resolution)\n- give those points uvs\n- create a timeoffset attribute on each point (could be a slit or noise or anything in between)\n- use the timeoffset attribute to move the points to different udims to read from different frames at the same time (your image sequence has to start on frame 1001 for this to work)\n- color the points using `attrib from map`\n- rasterize it all into a 2d volume and bring it into COPs for post processing or export\n\n![[notes/images/slitscan_clickthrough.gif]]\n\n// point wrangle \"norm_xy\"\n\n```C\nfloat norm_x = chramp(\"remap_x\", @P.x);\nfloat norm_y = chramp(\"remap_y\", @P.y);\n\nf@timeoffset = max(norm_x, norm_y);\n```\n\n// point wrangle \"udim\"\n\n```C\nint firstframe = chi(\"first_frame\");\nint lastframe = chi(\"last_frame\");\nint framerange = chi(\"frame_range\");\nint currframe = (int)@Frame;\n\nfirstframe += currframe - 1;\nlastframe += currframe - 1;\n\nfirstframe = select(firstframe\u003cframerange, firstframe, framerange);\nlastframe = select(lastframe\u003cframerange, lastframe, framerange);\n\nfloat readframe = (int)fit01(@timeoffset, firstframe, lastframe);\n\n@uv.x += (readframe-1) % 10;\n@uv.y += floor((readframe-1)/10);\n```\n\n// point wrangle \"raterize\"\n\n```C\nint pt = nearpoint(1, v@P);\nv@C = point(1, \"Cd\", pt);\n```\n\n##### Download: [File](https://github.com/jakobringler/blog/tree/hugo/content/notes/sharedfiles/video_slitscanning.hip)\n\n---\n\nsources / further reading:\n- [Slit-scan Photography - Wikipedia](https://en.wikipedia.org/wiki/Slit-scan_photography)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Solaris-Attribute-Conversion":{"title":"Solaris Attribute Conversion","content":"### USD Translations for Common Houdini Attributes\n\n```C\nv@P      \u003e\u003e\u003e points \nv@Cd     \u003e\u003e\u003e primvars:displayColor \nf@Alpha  \u003e\u003e\u003e primvars:displayOpacity \ni@id     \u003e\u003e\u003e ids\nv@uv     \u003e\u003e\u003e primvars:st // think of STMaps in Nuke\nv@N      \u003e\u003e\u003e normals \nv@v      \u003e\u003e\u003e velocities \nv@w      \u003e\u003e\u003e angularVelocities\nv@accel  \u003e\u003e\u003e accelerations \nf@width,\nf@widths,\nf@pscale \u003e\u003e\u003e widths\n```\n\n---\n\nsources / further reading:\n- [Converting attributes - DOCs](https://www.sidefx.com/docs/houdini/solaris/sop_import.html#attrs)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Storing-Grooms-in-Volumes":{"title":"Storing Grooms in Volumes","content":"\nA common problem with _most_ machine learning applications is that you need your data to be of the same size. That means the input or output [[notes/Tensors|Tensors]] have to have the same shape across all of the examples. \n\nThat's why images or volumes or other grid like data structures lend themselves well to machine learning purposes.\n\nI was trying to find a way to express hair/fur grooms in a way that doesn't change the shape of the data, if you change the hair count or segment length of the groom. I came across this [paper](https://mlchai.com/files/lyu2020real.pdf) on neural interpolation for real-time hair simulation, which had some great ideas.\n\nThe main one is to store the groom in a volume. \n\nYou can use the UVs of your mesh geometry to specify a new coordinate system consisting of the u and the v axis and use the `curveu` attribute on each hair as the third axis to move the groom to a new space, which by the nature of UVs becomes cuboid and can be easily rasterized into a volume. \n\n![[notes/images/groom2uvw.gif]]\n\nThe rasterization part acts like a filter and you lose some information along the way, but I found a volume of 512 x 512 x 20 to already be sufficient in quality for most usecases.\n\n![[notes/images/groom2volume.png]]\n\n// point wrangle move_to_tensor_space\n\n```C\nfloat voxelsize = volumevoxeldiameter(1, 0)/2;\nvector dims = primintrinsic(1, \"activevoxeldimensions\", 0);\nfloat voxelcount = dims.z;\nv@restP = v@P;\nv@P = set(@uv.x, @uv.y, @curveu*voxelsize*voxelcount);\n```\n\n// point wrangle rebuild\n\n```C\nfloat voxelsize = volumevoxeldiameter(2, 0)/2;\nvector dims = primintrinsic(2, \"activevoxeldimensions\", 0);\nfloat voxelcount = dims.z;\nvector readPos = set(@uv.x, @uv.y, @curveu*voxelsize*voxelcount);\nv@P = volumesamplev(1, 0, readPos);\n```\n\nNow you can learn stuff on arbitrary grooms! long, short, thick or thin :) \n\n---\n\nsources / further reading:\n- [Real-Time Hair Simulation with Neural Interpolation - Qing Lyu et al.](https://mlchai.com/files/lyu2020real.pdf)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Strange-Attractors":{"title":"Strange Attractors","content":"Strange attractors are patterns that emerge in chaotic systems. Even though the individual movements can seem random and unpredictable, strange attractors show a kind of underlying order. They represent points in the system's phase space where trajectories tend to cluster over time, creating intricate and often beautiful shapes. \n\n![[notes/images/strangeattractors.png]]\n\n//pointwrangle \"starting_conditions\" (used the values in each comment)\n\n```C\n@a = chf(\"sigma\"); // 10\n@b = chf(\"rho\");   // 28\n@c = chf(\"beta\");  // 8/3\n@dt = ch(\"dt\");    // 0.002\n```\n\nThe solver is just a point wrangle that moves the starting points along based on the formula and starting conditions.\n\n// the solver\n\n![[notes/images/strangeattractors_solver.png]]\n\n//pointwrangle \"move_points\"\n\n```C\nfloat x = @P.x;\nfloat y = @P.y;\nfloat z = @P.z;\n\nfloat dx = (@a * ( y - x )) * @dt;\nfloat dy = (x * ( @b - z ) - y ) * @dt;\nfloat dz = (x * y - @c * z ) * @dt;\n\n@P.x += dx;\n@P.y += dy;\n@P.z += dz;\n```\n\n---\n\nsources / further reading:\n- [Darstellung von Attraktoren mit Cinema - 3d Meier](http://www.3d-meier.de/)\n- [Houdini Tutorial 0034 Strange Attractor (Slow version) - Junichiro Horikawa](https://www.youtube.com/watch?v=saA6-edb-OE)\n- [VEX in Houdini: Strange Attractors - Entagma](https://www.youtube.com/watch?v=vl1tLebo-xY)\n- [Coding Challenge #12: The Lorenz Attractor in Processing - The Coding Train](https://www.youtube.com/watch?v=f0lkz2gSsIk)\n- [Chaos: The Science of the Butterfly Effect - Veritasium](https://www.youtube.com/watch?v=fDek6cYijxI)","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/TD-CHOPs":{"title":" TD CHOPs","content":"\n## Basics\n\nIn the `Common` tab of every CHOP parameter panel you can enable `Time Slice` view. This displays the current values as a horizontal bar compared to a graph over time like the noise or trail CHOPs.\n\n![[notes/images/Pasted image 20230714143209.png]]\n\nTo reference CHOPs values in other operators you have to make the viewer on the CHOP you want to reference active and then drag and drop the viewer on the parameter you want to drive. Then select `CHOP Reference`.\n\nIn active viewer mode you can right click the viewer to reset the min/max bounds for better display\n\n## CHOPs\n\n### Constant\n- create and name any floating point channels\n\n### Noise\n- generate different noise signals\n- generate multiple noise channels in one CHOP by typing different channel names separated by commas in the channel parameter tab\n\n### LFO\n- generates different oscillator type signals (sine, triangle, ramp. etc.)\n\n### Filter\n- different filter and smoothing operations to work on the signal\n\n### Trail\n- trails the signal over time\n\n![[notes/images/Pasted image 20230714134914.png]]\n\n### Math\n- very powerful\n- used to change range, multiply, offset etc.\n\n### Logic\n- useful to build 0,1 triggers: e.g. trigger if value is greater than x \n\n### Limit\n- clamp or quantize (step) values\n\n### Select\n- choose any number of existing CHOPs (rest gets discarded)\n- wildcards work as expected\n\t- `*` selects everything\n\t- `something*` selects every channel that starts with something... \n\n### Angle\n- converts between different units (e.g. quaternion to degrees)\n\n### Speed\n- useful to add up values over time\n\n### Trigger\n- outputs 0 or 1 if threshold is reached\n\n### Counter\n- counts integer triggers \n\n### Keyboard In\n- let's you specify keys and outputs 1 once pressed\n\n### Mouse In \n- output position, and button activations\n- self explanatory\n\n### Midi In\n- Values often range from 0-127 (128 steps)\n\n\n## Common Setups\n\n### Normalize a Signal to 0-1\n\nThe 0-1 pulse signal from the LFO gets squeezed with the lag CHOP.\n\n![[notes/images/Pasted image 20230719151345.png]]\n\nTo normalize it again there is a neat trick using the envelope CHOP.\n\n\u003eThe Envelope CHOP outputs the maximum amplitude in the vicinity of each sample of the input.\n\nPretty much like `Bounds` over time in Houdini\n\nSet it to a high number of seconds (e.g. 10). Just divide the signal through the output of it. \n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/TD-Instancing":{"title":" TD Instancing","content":"\n**Nothing new here!** These are just my tutorial notes --\u003e Links below!\n\nInstancing works similar to [[notes/Houdini |Houdini]]. You need some geometry you want to instance and some points to copy the geo to. Unlike in Houdini those points don't have to specified by 3D geometry but can also come from other types. In the end you only need 3 numbers to specify a location in space. Those can be generated by CHOPs or TOPs too!\n\nTo start we need the basic render geo render setup I already showed [[notes/TD SOPs#Basic Render Setup |here]].\n\n### SOP based\n\nAll you need to convert a default render setup to an instancing setup is another mesh that you can use to copy the base geometry to.\n\nIn this example I copied a sphere to every point on a grid:\n\n![[notes/images/Pasted image 20230719173937.png]]\n\nJust specify where the position is supposed to be picked up from in the `Instance` tab.\n\n`P(0)`, `P(1)`, `P(2)` is equivalent to the x, y \u0026 z position\n\n### CHOP based\n\nWIP\n\n### TOP based\n\nThis is pretty much a UV based setup where you start by defining the position with a texture of two ramps layered across each other.\n\nWIP\n\n### Instance Textures\n\nThe base setup is just rectangles copied to a grid\n\n![[notes/images/Pasted image 20230719171759.png]]\n\nTo instance the textures you need at least 3 things:\n- 1 or more TOP outputs named with the same prefix: `name_1`, `name_2`\n- and index array (can be TOPs or anything else, just make sure it's the same size as the base grid)\n- `Constant` MAT on the geo to display the texture correctly\n\nIn this example I also used a color noise TOP to generate different colors to apply to each instance (not necessary)\n\nIn the `Instance2` tab the configuration can be specified\n\n![[notes/images/Pasted image 20230719173244.png]]\n\n---\n\nsources / further reading:\n- [16 – Instancing – TouchDesigner Beginner Course - bileam tschepe (elekktronaut)](https://www.youtube.com/watch?v=rYet0SwTYa0)\n- [Tile Patterns – TouchDesigner Tutorial 31 - bileam tschepe (elekktronaut)](https://www.youtube.com/watch?v=gXUWcYZ8hqQ)","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/TD-SOPs":{"title":"TD SOPs","content":"\n## Basics\n\nSOPs are computed on the CPU and therefore don't scale too well. Animation in SOP land is dangerous since every frame has to be recomputed live.\n\nIn active viewer mode you can right click to get different display options:\n\n![[notes/images/Pasted image 20230714142920.png]]\n\nIt generally makes sense to turn the viewer of when working with SOPs. TD has to calculate every viewer for every timestep otherwise. To do so press the viewer flag in the top left of the operator.\n\n![[notes/images/Pasted image 20230714145122.png]]\n\n## SOPs\n\nSOPs have the most overlap with [[notes/Houdini |Houdini]]\n\n### Basic Shapes\n- Circle\n- Rectangle\n\t- Single Polygon\n- Grid\n- Box\n- Sphere\n- Tube\n- Torus\n\n### Merge\n- combine geo\n\n### Transform\n- move stuff\n\n### Attribute Create\n- generates Normals or Tangents for existing geometry\n- usually necessary before rendering GEO\n\n### Sort \n- change point order based on rule\n\n### Particles\n- emits points from GEO\n- to get random look you have to sort the input geo point order randomly\n- to view particles in the viewport/render you have to apply a `pointsprite` material\n- gets rather slow very quickly (CPU single threaded)\n\n## Setups\n\n### Basic Render Setup\n\nTo do so you need a `Geo`, a `Camera` and a `Light` COMP as well as a `Render` TOP\n\n![[notes/images/Pasted image 20230714144235.png]]\n\nYou can build simple control setups by providing a `null` for the camera to look at.\n\n---\n\nsources / further reading:\n- [10 – SOPs – TouchDesigner Beginner Course - bileam tschepe (elekktronaut)](https://www.youtube.com/watch?v=JfBNyy47YU8)\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/TD-TOPs":{"title":"TD TOPs","content":"\n## Basics\n\n## TOPs\n\n### Limit\n- very versatile\n- limit or loop values\n- quantize values or positions\n\n### Lookup\n- great to color black and white gradients, noise etc.\n- maps a grayscale value to a color picked from a ramp\n\n## Setups\n\n### UV Map\n\n![[notes/images/Pasted image 20230719163105.png]]\n\n1. Make a ramp (0 to 1)\n2. rotate 90 degrees\n3. combine to red and green channel\n\n### Custom Circular Ramp\n\nThis is useful because it ramps up completely until it reaches the image borders. The circular ramp that can be generated using the ramp TOP doesn't go above 1.\n\n![[notes/images/Pasted image 20230719162637.png]]\n\n1. Make a ramp (-0.5 to 0.5)\n2. rotate 90 degrees\n3. combine to red and green channel using reorder node\n4. use math node to combine channels and compute length\n\n![[notes/images/Pasted image 20230719163035.png]]\n\n### Color Gradients\n\n- use a lookup node\n- as the lookup just reads from the x axis of an image you can go with a Nx1 resolution where N controls the stepping of the colors (could be cool for an 8 bit look)\n\n![[notes/images/Pasted image 20230719163708.png]]\n\n### Instant Color Harmony\n\nIf you don't want to pick the colors for your ramps by hand there are some ways to built semi procedural color theory setups:\n\n![[notes/images/Pasted image 20230719165027.png]]\n\nIn this example I start with a constant (1x1 pixel) and just rotate the hue twice (120 degrees) before putting everything in a layout TOP (make sure to toggle on `Scale Resolution`). This way I only have to change the one constant color to get the correct trichromatic output. Using this technique you can replicate most commonly used color harmony rules.\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/The-Misery-Of-OpenCL":{"title":"The Misery Of OpenCL","content":"Like the legendary [Joy of Vex](https://www.tokeru.com/cgwiki/JoyOfVex.html), but without joy and less legendary.\n\n![[notes/images/opencl_pain.png|300]]\n\nWhy bother? The addition of Copernicus to Houdini makes OpenCL useful beyond pure optimization problems.\n\n\u003e [!warning] Warning!\n\u003e\n\u003e This is an ongoing project! It's still very barebones. Continue at your own risk!\n\nFor a general overview and a \"what and why\" head over to [[notes/OpenCL in Houdini|OpenCL]].\n\nThe whole guide leans very heavily on prior [[notes/VEX 101|VEX]] knowledge and draws a lot of comparisons. In general it is a good idea to prototype what you are trying to achieve in VEX before implementing it in OpenCL. \n\n- [[notes/Misery Of OpenCL 01|Misery Of OpenCL 01]] - Code Structure \u0026 Avoiding Crashes\n- [[notes/Misery Of OpenCL 02|Misery Of OpenCL 02]] - Basic Syntax and Datatypes\n- [[notes/Misery Of OpenCL 03|Misery Of OpenCL 03]] - Basic Assignments and Arithmetic\n- [[notes/Misery Of OpenCL 04|Misery Of OpenCL 04]] - Writeback Kernels\n- [[notes/Misery Of OpenCL 05|Misery Of OpenCL 05]] - Functions\n- [[notes/Misery Of OpenCL 06|Misery Of OpenCL 06]] - \n\n\n---\n\nsources / further reading:\n- [OpenCL for VEX users - DOCs](https://www.sidefx.com/docs/houdini/vex/ocl.html)\n- [OpenCL Geometry Node - DOCs](https://www.sidefx.com/docs/houdini/nodes/sop/opencl.html)\n- [OpenCL Copernicus Node - DOcs](https://www.sidefx.com/docs/houdini/nodes/cop/opencl.html)\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Today-I-Learned":{"title":"Today I Learned","content":"\nThe following is a compilation of all sorts of handy houdini tips \u0026 tricks. Most of them were shared in the #today-i-learned channel on the [Think Procedural Discord Server](https://thinkprocedural.com/).\nDo yourself a favor and check it out an scroll to the top of the thread to find some hidden secrets!\n\n## Quality of Life\n\n### Network \n\n- Pressing `8` in the network view toggles wire auto connection on and off (very handy if you move nodes around and don't want to mess up your setups)\n- Drag a wire in the middle of an unconnected node to instantly wire up in- **and** output\n- Drag \u0026 drop a camera node in the viewport to look through that camera\n- You can save geometry of any node through the right click menu\n- Use shortform search to find nodes:\n\t- `aw` -\u003e Attribute Wrangle\n\t- `om` -\u003e Object Merge\n\t- `gd` -\u003e Group Delete\n\t- `pv` -\u003e Point Velocity\n\t- `hf` -\u003e Height Field\n- if you capitalize node names they will be sorted on top in the tree view (useful when object merging stuff)\n### Viewport\n\n- `Ctrl` or `Shift` + `Arrow Keys` let's you rotate / move views\n- `M` enables first person movement with `WASD`, `Shift` to sprint, `Ctrl` to slowdown and `Q` / `E` to lift / descend\n- `Ctrl + Alt + S` enters 'Zen' mode and removes most of the UI clutter\n\n### File I/O\n\n- you can import anything straight into geo nodes by using `File`\u003e`Import`\u003e`Geometry`. This is very useful to import alembics without having to create a geo node and an alembic node inside it first.\n\n---\n\nsources / further reading:\n- [Think Procedural Discord Server](https://thinkprocedural.com/)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/TouchDesigner":{"title":"TouchDesigner","content":"\n### Basics\n- [[notes/Houdini to TouchDesigner |Houdini to TouchDesigner]]\n- [[notes/TD TOPs |TOPs]]\n- [[notes/TD CHOPs |CHOPs]]\n- [[notes/TD SOPs |SOPs]]\n- [[notes/Feedback Loops |Feedback Loops]]\n- [[notes/TD Instancing |Instancing]]\n- [[notes/Useful Expressions |Useful Expressions]]\n- [[notes/Exporting Image Sequences |Exporting Image Sequences]]\n\n### GLSL\n- [[notes/TD GLSL Basics |GLSL Basics]]\n\n### Sensors\n- [[notes/iPhone ARkit and Depth|iPhone ARkit and Depth]]","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Tower-Sketcher":{"title":"Tower Sketcher","content":"\n## Training a Neural Net to Understand Sketches\nThis is part of the [[notes/ML Castles |ML Castles]] project and was created for my bachelor's thesis.\n\n### Concept\nThe basic idea was to create an HDA and train a neural net to translate rough pixel sketches into parameter values that produce the drawn shape when entered in the HDA.\n\n![[notes/images/towerHDAconcept.png]]\n\n### HDA - Procedural Tower Generator\nThe base of the system is this very simple HDA that generates a single tower. The user can set different parameters to control the shape. The Asset outputs a 3D model and a 2D sketch derived from the model.\n\n![[notes/images/towersample.png]]\n\n![[notes/images/towerBaseBreakdown.png]]\n// base breakdown\n\n![[notes/images/towerroofbreakdown.png]]\n// roof breakdown\n\n![[notes/images/towerComparsion_.png]]\n// roof types\n\n### Setup\nTo create the necessary training data I used PDG. It's pretty straight forward to create almost infinite random variations of an HDA output and write them, or any part of their data, to disk. I generated 10.000 image-parameter pairs to train the network. The training process was also done inside of PDG in a python script node. The network was a combination of a [[notes/Convolutional Neural Networks (CNNs) |CNN]] and a [[notes/Feed Forward Networks (FFNs) |FNN]].\n \n![[notes/images/setuptowerhda(2).jpg |400]]\n\nThe Inference process (Predicting new results) was done in SOPs by running an Image ([[notes/Abusing Heightfields as Fast Image Canvas |stored on a heightfield/2D Volume]]) through a python SOP and feeding the predicted parameters back in the HDA.\n\n### Data Synthesis and PDG\nAs mentioned above the data generation was done in PDG. The setup is pretty straight forward:\n- create `n` wedges for the parameters you want to randomize\n- randomized values get stored in the pdg attribute stream\n- add slight offsets to sketch lines (noise) to get a more general/robust model\n- render out 2d sketch of resulting tower\n- fetch image path and attributes in python script and begin training\n\nHere is a sample of the dataset:\n\n![[notes/images/TowerDataset.png]]\n\n### Results\n![[notes/images/TowerSketcher.gif]]\n// gif plays at 3x speed\n\n![[notes/images/towerSketchdemo.png]]\n\nSince all the resulting values get clamped to a plausible range every sketch outputs a somewhat reasonable tower, which results in those rather incoherent pairs:\n\n ![[notes/images/towerSketchdemo2.png]]\n ","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/USD":{"title":"","content":"","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/Useful-Expressions":{"title":"Useful Expression","content":"\n### Getting the Parent Resolution\n\nespecially useful when resolution is subject to change mid project\n\n// get width and height\n\n```\nme.parent().width\nme.parent().height\n```\n\n### Using Time\n\n// set Frame or Second Time since TD started\n\n```\nabsTime.seconds\nabsTime.frame\n```\n\n### Referencing Table DAT Cells\n\n// get values\n\n```\nop('operatorname')[row,column].val\n```\n\n### Referencing Table Info\n\n// get number of rows or columns\n\n```\nop('operatorname').numRows\n\nop('operatorname').numCols\n```\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/UsefulMathNumbers":{"title":"Useful Math Functions \u0026 Numbers","content":"\n### Functions\n- [[notes/WaveExpressions |Wave  Expressions]]\n\n---\n### Golden Ratio\n\nRatio between two numbers that equals approximately: 1.618\n\n$\\frac{a+b}{a}=\\frac{a}{b}$\n\n### Golden Angle\n\n$\\phi= 137.5 \\degree$ - very useful when creating organic flowery stuff. Have a look at [this](https://entagma.com/td-essentials-create-a-swept-phyllotaxis-operator-in-houdini/) awesome video from Entagma for more information.\n\n$\\frac{360\\degree -137,5\\degree}{137,5\\degree}\\approx 1.618$\n\n### Fibonacci Sequence\n\n$F_1=F_2=1$       $F_n=F_{n-1} + F_{n-2}$\n\n0  1  1  2  3  5  8  13  21  34  55  89  144  233  377  610  987 ...\n\n---\n\n### Easing Functions\n\nCheck out this amazing page with visualizations on all sorts of easing functions:\n\nhttps://easings.net/\n\n---\n\n### VEX Constants\n\n[John Kunz](https://wiki.johnkunz.com/index.php?title=Mathematical_Functions_in_VEX) pointed out these constants that can be used in any VEX wrangle:\n\n```C++\n// defined in: $HFS/houdini/vex/include/math.h  \n\nM_E         2.7182818    // Euler's number\nLN10        2.3025850    // logarithm of 10\nLN2         0.6931471    // logarithm of 2\nLOG10E      0.4342944\nLOG2E       1.4426950\nPI          3.1415926    // 180° in radians\nM_TWO_PI    6.2831852    // 360° in radians\nPI_2        1.5707963    // 90° in radians\nPI_4        0.7853981    // 45° in radians\nSQRT1_2     0.7071067\nSQRT2       1.4142135\nTOLERANCE   0.0001\n\n// SOURCE: John Kunz's Wiki \n```\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/VELLUM":{"title":"VELLUM","content":"\n## Cloth\n\n### Stiff \u0026 Crumply (Paper, Metal, etc.)\n\nKey Parameters:\n- Constraint Iterations (higher = stiffer)\n- Plasticity (prevents unfolding)\n- High Bend Stiffness\n- Disable Compression Stiffness\n- Enable Stiffness Dropoff (Decreasing)\n\nAdditional Tricks:\n- Automatically group sharp corners and use the resulting 'crease' group in a subdivision node to maintain sharpness\n\n### Intersections\nWhen dealing with intersections on layered cloth the 'post collision passes' parameter can help. A good rule of thumb is setting them slightly above the number of expected layers (collisions with itself) of cloth.\n\n![[notes/images/layersofcloth.png]]\n\nIn this example where two cloth folds form I would count 4 possible collisions and probably set 'post collision passes' to 5.\n## Hair\n\n### Collisions\nTo fix jittering and collision issues on hair roots the `disableexternal` attribute can be useful.\n\n```C\nif(@curveu == 0)\n{\n\ti@disableexternal = 1;\n}\n```\n\nsimilarly `i@disableself = 1` can be used to avoid collisions with itself.\n\n### Stiff Hair (LowRes / HiRes Workflow)\n\nTo get anything rigid in Vellum you need loads of constraint iterations usually. For Hair you can sometimes get away with using a low resolution (sparsely resampled) version of the guides and simulate those. \n\nAll you have to do is put some uvs on each hair and lookup the new position with the `primuv` function, which takes care of the interpolation of the hiRes guides.\n\n// simulation guides (left) are resampled to about 1/5 of the actual point count\n\n![[notes/images/lowResHiRes_hair.png]]\n\n// both uvtexture nodes\n\n![[notes/images/uvtexture_hair.png]]\n\n// point wrangle\n\n```C\nv@P = primuv(1 ,\"P\", @primnum, v@uv);\n```\n\n## Fluids\n### Droplets, Sheets \u0026 Tendrils\nsame rules as described in [[notes/FLIP#Droplets Sheets Tendrils |FLIP]] apply.\n\n\n---\n\nsources / further reading:\n- [Houdini vellum edge threads details - Tutorial - Linus Rosenqvist](https://www.youtube.com/watch?v=3IidlkG-VmM)\n- [Crumple Effects in Houdini Vellum using Bend Stiffness and Plasticity - regularmenthol](https://www.youtube.com/watch?v=64ujNBGQ7P8)\n- [Houdini Quicktip: Vellum Creasing - Dave Stewart](https://vimeo.com/601670425)\n- [disableexternal - sideFX Forum](https://www.sidefx.com/forum/topic/60379/?page=1)\n\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/VEX-101":{"title":"VEX 101","content":"### The Better Cheat Sheets / Snippet Libraries / 101s\n\n- [The Joy of VEX - Matt Estella](https://www.tokeru.com/cgwiki/JoyOfVex.html)\n- [VexCheatSheet - Matt Estella](https://www.tokeru.com/cgwiki/VexCheatSheet.html)\n- [VEX Attribute Glossary - John Kunz](https://wiki.johnkunz.com/index.php?title=VEX_Attribute_Glossary)\n- [VEX for artists - Kiryha](https://github.com/kiryha/Houdini/wiki/vex-for-artists)\n- [VEX snippets - Kiryha](https://github.com/kiryha/Houdini/wiki/vex-snippets)\n- [vex_tutorial - jtomori](https://github.com/jtomori/vex_tutorial)\n\n### Datatypes\n// point wrangle\n\n```C\n// Integers\nint IntVariable = 1;\ni@IntAttribute = 1;\n\n// Floats\nfloat FloatVariable = 0.123;\nf@FloatAttribute = 0.123; // attributes will be float by default if you don't specify it otherwise\n\n// Vectors\nvector VectorVariable = {0,1,0}; // 3 floats \u003e postion, color, v\nvector VectorVariable = set(0.1, 0.2, 0.3); // use the set() function to define a vector with floats \nvector2 Vector2Variable = {0,1} // 2 float \u003e e.g. uvs\nvector4 Vector4Vairable =  quaternion(angle, axis); // 4 floats \u003e usually used to store quaternions\n\nv@Vector3Attribute = {0,1,0}; \nu@Vector2Attribute = {0,1};\np@Vector4Attribute = quaternion(angle, axis);\n\n// Matricies\nmatrix2 Matrix2Variable = {1,2,3,4}; // 2x2\nmatrix3 Matrix3Variable = {1,2,3,4,5,6,7,8,9}; // 3x3\nmatrix4 Matrix3Variable ={1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16}; // 4x4\n\n2@Matrix2Attribute = {1,2,3,4};\n3@Matrix3Attribute = {1,2,3,4,5,6,7,8,9};\n4@Matrix4Attribute = {1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16};\n\n// Arrays\nstring StringArray[] = {'A','B','C'};\nstring StringArray[] = array(variable_A, variable_B, variable_C); // use the array() function to define an array with variables \ns[]@StringArrayAttribute = {'A','B','C'};\n\n// Strings\nstring StringVariable = 'Interesting Text';\ns@StringAttribute = 'Interesting Text';\n\n// Dictionaries\ndict Dictionary;\nDictionary['key'] = 'value'; // Store 3 in the index key\nd@DictAttribute = {}; // dict, can only instantiate as empty type\nd@DictAttribute['key'] = 'value'; // can set values once instantiated\n```\n\n### Datatypes Overview\n\n// point wrangle\n\n```C\nint      =\u003e i@name\nfloat    =\u003e f@name\nvector2  =\u003e u@name\nvector3  =\u003e v@name\nvector4  =\u003e p@name\nmatrix2  =\u003e 2@name\nmatrix3  =\u003e 3@name\nmatrix   =\u003e 4@name\nstring   =\u003e s@name\ndict     =\u003e d@name\n```\n\n### Comments\n// point wrangle\n\n```C\n// normal comment\n\n/*\nmulti-line \ncomment\n*/\n```\n\n### Conversion\n// point wrangle\n\n```C\n// int to string\nint i = 123456;\nstring s = itoa(i);\n\n// string to int\nstring s = '123456';\nint i = atoi(i);\n```\n\n### String manipulation\n// point wrangle\n\n```C\n// Slicing \u003e\u003e\u003e string[start:end]\nstring s = 'abcde';\ns[:];     // abcde\ns[:-1];   // abcd\ns[1:];    // bcde\ns[1:-1];  // bcd\ns[-1];    // e\ns[0];     // a\n\n// Concatenation\nstring text = 'TEXT';\nstring number = '1234';\nstring s = sprintf('%s%s%s', text, ' = ', number);\nprintf('%s', s);\n// TEXT = 1234\n\n// Reversing\nstring s = '1234'\nreverse(s);\n// 4321\n```\n\n### Arrays\n// point wrangle\n\n```C\n// Defining\nint nums[] = { 0, 1, 2, 3, 4, 5 };\ni[]@nums = { 0, 1, 2, 3, 4, 5 };\n\nint a = 5; \ni[]@nums = array(0, 1, 2, 3, 4, a); // just like set() for vectors\n\n// Retrieving \nint num = nums[3] // 2\ni@num = i[]@nums[3] // 2\n\n// Accessing, Slicing (VEX uses Python style slicing notation)\nint start[] = nums[0:2]; // { 0, 1 }\nint end[] = nums[-2:]; // { 4, 5 }\nint rev[] = nums[::-1]; // { 5, 4, 3, 2, 1, 0 }\nint odd[] = nums[1::2]; // { 1, 3, 5 }\n\n// Sorting, Reversing\nint nums[] = {5, 1, 4, 3, 8};\nint sorted[] = sort(nums)); // {1, 4, 4, 5, 8}\nint reversed[] = reverse(sort(nums))); // {8, 3, 4, 1, 5}\n\n// Adding Elements\nint numbers[] = {1, 2, 3, 4, 5, 6};\nnumbers[1] = 7; // {1, 7, 3, 4, 5, 6}\nnumbers[6] = 7; // {1, 2, 3, 4, 5, 6, 7}\nappend(numbers, 7); // {1, 2, 3, 4, 5, 6, 7}\n\n// Finding Index of Value\nint numbers[] = {5, 4, 3, 2, 1};\nint index_of_4 = find(numbers, 4);\nprintf('%d', index_of_4 ); // 1\n\n// Split string with a space to array of strings\nstring numbres = '1 2 3 4 5 6';\nstring array[] = split(numbres, ' ');\nprintf('%s', array); // {1, 2, 3, 4, 5, 6}\n\n// Split integer into array of integers via strings\nint int_number = 312654;\nstring string_number = itoa(int_number);\nint int_numbers[];\n\nfor(int n=0; n\u003clen(string_number); n++){  \n        int_numbers[n] = atoi(string_number[n]);\n        }\n\nint nums[] = int_numbers; // {3, 1, 2, 6, 5, 4}\nint sorted[] = sort(int_numbers); // {1, 2, 3, 4, 5, 6}\nint reversesorted[] = reverse(sort(int_numbers))); // {6, 5, 4, 3, 2, 1}\n\n// Copying between arrays and vectors/matrices\n\nfloat x[];\n// Cf and P are vectors\nx = set(P);  // Assigns the components of P to the corresponding members of the array x\nCf = set(x); // Assigns the first 3 members of x as the components of the vector Cf\nfloat x[] = {1, 2} // Not long enough to fill a vector\nCf = set(x);  // Cf == {1, 2, 2}\n```\n\n### Channel Syntax\n// point wrangle\n\n```C\nch('float'); // Float\nchf('float2'); // Float\nchi('integer'); // Integer\nchv('vector'); // Vector 3\nchp('quaternion'); // Vector 4 / Quaternion\nch3('matrix3'); // 3x3 Matrix\nch4('matrix4'); // 4x4 Matrix\nchs('string'); // String\nchramp('ramp', x); // Spline Ramp\nvector(chramp('rgbramp', x)); // RGB Ramp\n```\n\n### Debug with printf\n// point wrangle\n\n```C\n// print strings\nprintf('Hello Houdini');\n\n// print datatypes\nprintf('string: %s', 'Text'); // string: Text\nprintf('interger: %d', 123); // integer: 123\nprintf('float: %f', 0.123456); // float: 0.123456\nprintf('float (rounded): %.2f', 0.123456); // float (rounded): 0.12\n\n// get all primitives\nint primitives[] = expandprimgroup(0, \"*\");\n\nforeach (int currentPrim; primitives){   \n        // print primnum\n        printf('Prim %s \\n', currentPrim);\n        }\n```\n\n### Global Attributes\n// wrangle\n\n```C\n// Available in all SOP wrangles\ni@Frame     // current frame number =\u003e $F\nf@Time      // current time in seconds =\u003e $T\nf@TimeInc   // timestep currently being used for simulation or playback\n\n// only in DOPs\ni@SimFrame  // integer simulation timestep number =\u003e $SF\nf@SimTime   // simulation time in seconds =\u003e $ST\n\n// Available in Attribute Wrangle (point, vertex, primitive and detail)\nv@P         // position of the current element\n\ni@ptnum     // point number attached to the currently processed element\ni@numpt     // total number of points in the geometry\ni@vtxnum    // linear number of the currently processed vertex\ni@numvtx    // number of vertices in the primitive of the currently processed element\ni@primnum   // primitive number attached to the currently processed element\ni@numprim   // total number of primitives in the geometry\n\n// useful: elemnum and numelem morph into whatever wrangle you are in\ni@elemnum   // index number of the currently processed element\ni@numelem   // total number of elements being processed\n\n// Available in Volume Wrangle\nv@P // position of the current voxel\nf@density // density value at the current voxel location\ni@resx, i@resy, i@resz // resolution\nv@center // center of the volume bbox\nv@dPdx, v@dPdy, v@dPdz // change in position that occurs in the x, y, and z voxel indices\ni@ix, i@iy, i@iz // voxel indices (only dense volumes / non-VDBs ) range from 0 to resolution-1\n\n// Available in Heightfield (Volume) Wrangle\nf@height // height at xy coordinate\nf@mask // mask at xy coordinate\n```\n\nmore specific / context related attributes can be found [[notes/Common Attributes |here]].\n\n---\n\nsources / further reading:\n- [The Joy of VEX - Matt Estella](https://www.tokeru.com/cgwiki/JoyOfVex)\n- [VexCheatSheet - Matt Estella](https://www.tokeru.com/cgwiki/VexCheatSheet)\n- [VEX Attribute Glossary - John Kunz](https://wiki.johnkunz.com/index.php?title=VEX_Attribute_Glossary)\n- [VEX for artists - Kiryha](https://github.com/kiryha/Houdini/wiki/vex-for-artists)\n- [VEX snippets - Kiryha](https://github.com/kiryha/Houdini/wiki/vex-snippets)\n- [vex_tutorial - jtomori](https://github.com/jtomori/vex_tutorial)\n- [VEX - Houdini DOCs](https://www.sidefx.com/docs/houdini/vex/index.html)\n\n","lastmodified":"2024-11-11T20:40:13.750621036Z","tags":null},"/notes/VEX-Snippets":{"title":"VEX Snippets","content":"\n## Wrangle Cheat Sheet\n\n\u003eI try my best to credit and link to any sources. That being said, some of those are pretty old and I have no idea where they came from.\n\u003e\n\u003eI recommend installing [Vex Snippet Library](https://github.com/dchow1992/Vex_Snippet_Library) to manage your own snippet collection. You can find all snippets formatted as .json files ready to be plugged into the library [here](https://github.com/jakobringler/H_VEX_snippets).\n\n\u003e [!tip] **Hot Tip:**\n\u003e \n\u003e You can find the attribute wrangle node by just typing `aw` ;)\n\n## Utilities\n\n### Attribute based Probability Grouping\n//point wrangle\n\n```C\n// adjust in min max to attribute min max\nfloat norm = fit(@pscale, chf(\"in_Min\"), chf(\"in_Max\"), 0, 1);\n// control distribution with ramp\nfloat ramp = chramp(\"distribution\", norm);\n// set min max percentage to 0/1 in the beginning\n// adjust for random strays\nfloat max = chf(\"max_Percentage\");\nfloat min = chf(\"min_Percentage\");\nfloat rand = rand(@ptnum);\nfloat percentage = clamp(fit01(ramp, min, max), 0, 1);\n\ni@group_selected = rand\u003cpercentage;\n```\n\n### Angle between 2 Vectors\n// point wrangle \n\n```C\nfloat angle = degrees(acos(dot(normalize(vector1), normalize(vector2))));\n```\n\n### Attribute Min Max\n// point wrangle\n\n```C\nfloat value;\nfloat values[];\nstring attrname = \"name\";\nfloat max_value;\nfloat min_value;\n\nfor (int i=0; i\u003c@numpt; i++)\n{\n\tvalue = point(geoself(), attrname, i);\n\tappend(values,value);\n}\n\nmin_value = min(values);\nmax_value = max(values);\n\nf[]@range;\n\n@range[0] = min_value;\n@range[1] = max_value;\n```\n\n### Attribute to String\n// point wrangle\n\n```C\ns@name = \"piece_\" + itoa(i@class);\n```\n\n### Attribute Transfer\n// point wrangle\n\n```C\nint posprim;\nvector param_uv;\nfloat maxdist = 10;\nfloat dist = xyzdist(1,@P,posprim,param_uv,maxdist);\nvector pos = primuv(1,\"rest\",posprim,param_uv);\nv@rest = pos;\n```\n\n### Average Point Cloud Positions\n// point wrangle\n\n```C\nvector value;\nvector values[];\n\nfor (int i=0; i\u003c@numpt; i++)\n{\n\tvalue = point(geoself(), \"P\", i);\n\tappend(values,value);\n}\n  \nvector avgP = avg(values);\n\nif(@ptnum\u003e0)\n{\n\tremovepoint(geoself(), @ptnum);\n} \n\n@P = avgP;\n```\n\n### Bias and Gain\n// point wrangle\n\n```C\nfunction float bias(float val; float bias) \n{\n    return (val / ((((1.0/bias) - 2.0)*(1.0 - val))+1.0));\n}\n\nfunction float gain(float val; float gain) \n{\n    if(val \u003c 0.5)\n    {\n        return bias(val * 2.0,gain)/2.0;\n    }\n    else\n    {\n        return bias(val * 2.0 - 1.0,1.0 - gain)/2.0 + 0.5;\n    }\n}\n\nfloat val = pow(val, exp); \n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Michael Frederickson's Tweet](https://twitter.com/mfrederickson/status/1523148417349816320)\n\u003e \n\u003e [Alan Wolfe's Blog Post](https://blog.demofox.org/2012/09/24/bias-and-gain-are-your-friend/)\n\n### Bounding Box\n// point wrangle\n\n```C\nvector bbox = getbbox_size(0);\nvector bbox_max = getbbox_max(0);\nvector bbox_min = getbbox_min(0);\nvector bbox_center = getbbox_center(0);\n  \nfloat xsize = getbbox_size(0).x;\nfloat y_max = getbbox_max(0).y;\nfloat z_min = getbbox_min(0).z;\n```\n\n### Camera Position and Direction\n// point wrangle\n\n```C\nstring cam = chs(\"cam\");\nmatrix camXform = optransform(cam); \nvector cpos;\nvector cdir;\nvector cup;\n\ncpos = cracktransform(0, 0, 0, {0,0,0}, camXform);\ncdir = vtransform(cam,\"space:world\", {0,0,-1});\ncup = vtransform(cam,\"space:world\", {0,1,0});\n\nv@P = cpos;\nv@N = cdir;\nv@up = cup;\n```\n\n### Collision Check and Deintersection with SDF VDB\n// point wrangle\n\n```C\nvector gradient = volumegradient(1, \"surface\", v@P); \nfloat surface = volumesample(1, \"surface\", v@P);\n\nif(surface \u003c chf(\"dist\"))\n{\n\tv@P += normalize(gradient) * abs(surface);\n}\n```\n\n### Color Palette Refinement using 'Despill' Technique\n// point wrangle\n\n```C\n// rotate hue\nv@Cd = rgbtohsv(v@Cd);\n@Cd.r += chf(\"rotate_Hue\");\nv@Cd = hsvtorgb(v@Cd);\n\n// define color spread (value of 2 for Tolerance keeps input colors)\n\nvector originalCd = v@Cd;\n\nif (@Cd.g \u003e (@Cd.b + @Cd.r) / 2 * (chf(\"Tol\") * 2))\n{\n    @Cd.g = (@Cd.b + @Cd.r) / 2 * (chf(\"Tol\") * 2);\n}\nelse\n{\n    @Cd.g = @Cd.g;\n}\n\n// apply tint\n\n@Cd += length(originalCd - v@Cd) * chv(\"Tint\") * chf(\"Luma\");\n\n// rotate hue back\nv@Cd = rgbtohsv(v@Cd);\n@Cd.r -= chf(\"rotate_Hue\");\nv@Cd = hsvtorgb(v@Cd);\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Chris Turner's Twitter thread about Compositing Tricks](https://twitter.com/allexceptn/status/1439253876041998346)\n\n### Crowd Transition Mirrored Clips\n// primitive wrangle\n\n```C\nstring mirrorsuffix = \"_m\";\nstring newclipname_a = s@clip_a + mirrorsuffix;\nstring newclipname_b = s@clip_b + mirrorsuffix;\n\nint clip_a = nametopoint(0, s@clip_a);\nint clip_a_m = nametopoint(0, newclipname_a);\nint clip_b = nametopoint(0, s@clip_b);\nint clip_b_m = nametopoint(0, newclipname_b);\n\nif( clip_a_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a_m, clip_b);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, newclipname_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, s@clip_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n\nif( clip_b_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a, clip_b_m);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, s@clip_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, newclipname_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n\nif( clip_a_m != -1 \u0026\u0026 clip_b_m != -1 )\n{\n    int prim = addprim(0, \"polyline\", clip_a_m, clip_b_m);\n    int pA_clip_a = setprimattrib(0, \"clip_a\", prim, newclipname_a, \"set\");\n    int pA_clip_b = setprimattrib(0, \"clip_b\", prim, newclipname_b, \"set\");\n    int pA_blend = setprimattrib(0, \"blend_durations\", prim, f[]@blend_durations, \"set\");\n    int pA_sync = setprimattrib(0, \"sync_points\", prim, f[]@sync_points, \"set\");\n    int pA_regions = setprimattrib(0, \"transition_regions\", prim, f[]@transition_regions, \"set\");\n}\n```\n\nThis is a useful snippet to setup correct transitions of mirrored clips based on the transition settings configured by hand for the base clips. To use this you have to create a name attribute on your points by copying the `clipname` attribute. You can find more [[notes/Crowds#Dealing with Mirrored Clips Automatically |here]].\n\n### Crowd Clip Properties Mirrored Clips\n// point wrangle\n\n```C\nstring mirrorsuffix = \"_m\";\nstring clipname = s@clipname;\nstring newclipname = clipname + mirrorsuffix;\n\nint clip_m = nametopoint(0, newclipname);\nint clip_exists = nametopoint(1, newclipname);\n\nif( clip_m == -1 \u0026\u0026 clip_exists != -1)\n{\n    int pt = addpoint(0, set(0,0,0));\n    int pA_clipname = setpointattrib(0, \"clipname\", pt, newclipname);\n    int pA_agent = setpointattrib(0, \"agentname\", pt, s@agentname);\n    int pA_clipnamesrc = setpointattrib(0, \"clipname_source\", pt, newclipname);\n    int pA_blend_a = setpointattrib(0, \"blend_duration_after\", pt, @blend_duration_after);\n    int pA_blend_b = setpointattrib(0, \"blend_duration_before\", pt, @blend_duration_before);\n    int pA_loop = setpointattrib(0, \"enable_looping\", pt, @enable_looping);\n    int pA_gait = setpointattrib(0, \"gait_speed\", pt, @gait_speed);\n    int pA_loopr = setpointattrib(0, \"loop_range\", pt, u@loop_range);\n    int pA_sampler = setpointattrib(0, \"sample_rate\", pt, @sample_rate);\n    int pA_start_time = setpointattrib(0, \"start_time\", pt, @start_time);\n}\n```\n\nRead more [[notes/Crowds#Dealing with Mirrored Clips Automatically 2 |here]]\n### Edgefalloff\n// point wrangle\n\n```C\nif (@edgefalloff==1)\n{\n\tint near[] = nearpoints(0,@P,chf(\"dist\"));\n\t\n\tforeach (int pnt;near)\n\t{\n\t\tvector pntP = point(0,\"P\",pnt);\n\t\tfloat dist = fit(distance(pntP,@P),0,chf(\"dist\")*2,1,-1);\n\t\tsetpointattrib(0,\"edgefalloff\",pnt,dist,\"set\");\n\t}\n}\n```\n\n### Expand Group Over Geo\n// point wrangle\n\n```C\nint pc = pcopen(0, 'P', @P, chf('radius'), chi('maxpts'));\n\nwhile (pciterate(pc) \u003e 0)\n{\n\tint currentpt;\t\n\tpcimport(pc, 'point.number', currentpt);\n\tsetpointgroup(0, 'group1', currentpt, 1);\n}\n```\n\n### Extract Tranformation Matrix\n// point wrangle\n\n```C\nvector P1 = point(0, \"P\", 0);\nvector P2 = point(0, \"P\", 1);\nvector up = {0,1,0};\n\nvector X = normalize(P2-P1);\nvector Z = normalize(cross(up, X));\nvector Y = normalize(cross(X, Z));\n\nvector P = P1 + (P2 - P1) / 2;\n\nmatrix transform = set(X, Y, Z, P);\n\nsetcomp(transform, 0, 0, 3); \nsetcomp(transform, 0, 1, 3);\nsetcomp(transform, 0, 2, 3); \n\n4@transform = transform;\n```\n\nHave a look at [[notes/Matrix Operations |this note]] for more information on how it's used.\n\n### Flow Vector around Geometry\n// point wrangle\n\n```C\n vector up = chv(\"up\"); // usually 0, 1, 0 for Y  \n vector norm = normalize(v@N);  \n vector c1 = cross(norm, up);  \n vector c2 = cross(norm, c1);  \n v@revolve = c1;\n v@flow = c2;\n```\n\n### Follow Surface with Particles using the VDB Gradient\n// point pop wrangle\n\n```C\nfloat surf = volumesample(1, \"surface\", v@P);\nvector grad = volumegradient(1, \"surface\", v@P);\n\nv@P -= normalize(grad) * surf;\n```\n\n### Frustum Culling Volume Fields in DOPs\n// gas field wrangle\n\n```C\nstring cam = chsop(\"cam\");\nvector ndcP = toNDC(cam,@P);\n//vector camP = ptransform(\"space:camera\",@P); //non perspective space if needed\n\nfloat mult = 1;\n\nif(chi('cull_x_negative')) mult *= ndcP.x \u003e -ch('cam_x_neg') ;\nif(chi('cull_x_positive')) mult *= ndcP.x \u003c 1 + ch('cam_x_pos') ;\nif(chi('cull_y_negative')) mult *= ndcP.y \u003e -ch('cam_y_neg') ;\nif(chi('cull_y_positive')) mult *= ndcP.y \u003c 1 + ch('cam_y_pos') ;\nif(chi('cull_z_negative')) mult *= ndcP.z \u003c ch('cam_z_neg') ;\nif(chi('cull_z_positive')) mult *= ndcP.z \u003e -ch('cam_z_pos') ;\n\n\n//foreach( int i; string field; split(chs('fields'))){} //maybe loop over given fields, usually density is enough\nf@density *= mult;\nf@temperature *= mult;\nf@flame *= mult;\n@Cd *= mult;\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Lewis Taylor's Talk on Volume Efficiency](https://vimeo.com/894363970)\n\nRead more [[notes/Volumes#Frustum Rasterizing vs Frustum Culling SOPs vs Frustum Culling DOPs |here]]\n\n### Group by N Connections\n// point wrangle\n\n```C\nint n = chi(\"Neighbours\");\n\nif (neighbourcount (0, @ptnum) \u003e n)\n{\n\tsetpointgroup (0, \"grouped\", @ptnum, 1);\n}\n```\n\n### Gravity on Curves (Hanging Cables)\n// point wrangle\n\n```C\nfloat stiffness = clamp(chf(\"stiffness\"), 0, 0.99);\nfloat u = @curveu * (1 - @curveu) * 4;\nu = pow( 1 - u, (1 / (1 - stiffness)));\n\n@P.y *= clamp(fit01(gradient, (1 - ch(\"gravity\")), 1), 0, 1);\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Chris Turner's Tweet](https://twitter.com/allexceptn/status/1488954032425213958)\n\n### Helix from Line\n// point wrangle\n\n```C\n  float freq = chf(\"freq\");\n  float amp = chf(\"amp\");\n  vector pos = @P;\n  \n  pos.x += sin(@P.y * freq) * amp;\n  pos.z += cos(@P.y * freq) * amp;\n  @P = pos;\n```\n\n### If Statements, Ternary Operations, Select Function\n// point wrangle\n\n```C\n// long and most common version\nif(value \u003e threshold)\n{\n\tvalue;\n}\nelse\n{\n\t0;\n}\n\n// Ternary\nvalue = (value \u003e threshold) ? value: 0;\n// this does the same thing but as a oneliner \n// (condition) ? ValueIfTrue: ValueIfFalse\n\n// Select Function (the fastest of the three when it comes to pure performance)\nvalue = select( value \u003e threshold, value, 0);\n// select(condition, ValueIfTrue, ValueIfFalse)\n\n// @swalsch pointed out on the cgwiki discord that in this case the absolutely fastest and most elegant way would be as follows:\nvalue *= value\u003ethreshold;\n// which is so elegant that I had to include it. (Only works if your False Value is 0 anyways)\n```\n\n### Inflate Geo and Avoid Intersections / Fake Collisions \n// point wrangle\n\nThis snippet is best used by running it inside a compiled for-each loop set to \"by count\". Make sure to recalculate the point normals of the geometry inside the loop before the wrangle!\n\n```C\nint handle;\nint maxPoints = chi('maxPts');\nfloat searchDist = ch('searchDist');\nvector avgN;\n\nint n[] = nearpoints(0, v@P ,searchDist,maxPoints);\navgN = v@N;\nfor( int ii = 0 ; ii \u003c len(n) ; ii++ ) {\n    int pt = n[ii];\n    \n    if( ii == 1 ){\n        avgN = point(0, \"N\", pt);        \n    }\n    if( ii \u003e 1 ){ \n        avgN += point(0, \"N\", pt);\n    }\n}\navgN /= len(n);\n\nfloat dot;\ndot = dot(@N,avgN);\ndot = fit(dot,-.1,1,0,1);\n@P += v@N * ch('stepSize') * dot;\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Jacob Clark](https://www.linkedin.com/in/jclark2900/) shared this amazing setup on a houdini discord channel. Thanks for letting me post it!\n\n### Isolate Overlapping Points\n// point wrangle\n\n```C\nint near[] = pcfind(0, \"P\", @P, 0.0001, 2);\n\nif(len(near) \u003e 1)\n{\n\tsetpointgroup(0, \"double\", @ptnum, 1);\n}\n```\n\n### Name Attribute for each Prim Group\n// point wrangle\n\n```C\nstring grps[] = detailintrinsic(0, 'primitivegroups');\nforeach(string g; grps)\n{\n\taddprimattrib(0, g, 123);\n}\n```\n\n### Normalize @age\n// point wrangle\n\n```C\n@normage = @age / @life;\n```\n\n### Orientation Template for Copy\n// point wrangle\n\n```C\n@up = {0,1,0};\n@orient = quaternion(maketransform(@N,@up));\nvector4 rot_Y = quaternion(radians(ch('Y')),{0,1,0});\n@orient = qmultiply(@orient, rot_Y);\n```\n\n### Point Density\n// point wrangle\n\n```C \nfloat maxdist = chf(\"maxdist\");  \nint maxpts = chi(\"maxpts\");  \nint points = len(nearpoints(0, @P, maxdist, maxpts));  \n  \nf@density = float(points) / maxpts;\n```\n\n### Ray to Surface SDF\n// point wrangle\n\n```C\n@P -= volumegradient(1, \"surface\", @P) * volumesample(1, \"surface\", @P) \n```\n\n### Remove Point by Condition\n// point wrangle\n\n[Mai Ao](https://twitter.com/aomai01) compared two point deletion methods, where method 1 gives a 15x speed increase over the traditional `removepoint()` function\n\n1. group points first and blast group in another step\n\n```C\nfloat half_cone_rad = radians(chf(\"half_cone\"));\n@group_to_delete = acos(dot(@P, {0,0,1})) \u003c= half_cone_rad;\n```\n\n2. removepoint()\n\n```C\nfloat half_cone_rad = radians(chf(\"half_cone\"));\n\nif(acos(dot(@P, {0,0,1})) \u003c= half_cone_rad)\n{\n\tremovepoint(0, @ptnum);\n}\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e [Mai Ao's Tweet](https://twitter.com/aomai01/status/1514226273794641925/photo/1)\n\n### Remove Point Percentage\n// point wrangle\n\n```C\nint percentage = ch('percentage'); \n\nif(@ptnum % 100 \u003c percentage)\n{\n\tremovepoint(0, @ptnum );\n}\n```\n\n### Remove Point Percentage (ID aware)\n// point wrangle\n\n```C\nif (haspointattrib(0, \"id\"))\n{    \n    if (rand(@id+chi('seed')+0.33)\u003cchf('percentage'))\n    {\n        removepoint(geoself(), @ptnum);\n    }    \n}\nelse\n    if (  rand(@ptnum+chi('seed')+0.33)\u003cchf('percentage')) \n    {\n        removepoint(geoself(), @ptnum);\n    }\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e Thanks Hannes!\n\n### Rotate Vector\n// point wrangle\n\n```C\nfloat angle = chf(\"angle\");\nvector axis = normalize(chv(\"axis\"));\n\nvector4 rot = quaternion(radians(angle), axis);\n@P = qrotate(rot, @P);\n```\n\n### Sharpen Point Cloud\n// point wrangle\n\n```C\nint handle = pcopen(0, \"P\", @P, chf(\"radius\"), chi(\"maxpoints\"));\nv@P = pcfilter(handle, \"P\");\n```\n\n### Vector Flow on Objects\n// point wrangle\n\n```C\nvector pos = v@P * chf(\"scale\");\nvector dir = curlnoise2d(pos + @Time * chf(\"time\"));\nmatrix3 mat = dihedral(set(0,0,1), v@N);\ndir *= mat;\n\nv@v = dir;\n```\n\n### Velocity Tester\n// point wrangle\n\nVery useful to visualize how meshed fluids will move in the motion blur of a rendered frame.\n\n```C\n@v *=  chf('vScale'); // Should be 1 by default\nfloat velMax = ch('max_vel'); // Should be a high number by default\nfloat velLength = length(@v);\n\n// Clamp Velocity\nif(velLength \u003e velMax)\n    {\n    @v = normalize(@v)*velMax;\n    }\n    \n// Export Speed\nif (chi(\"export_Speed\")==1)\n    {\n    @speed = length(@v);\n    }\n\n// Test Velocity\n@P += @v/$FPS*chf('velTester');\n```\n\n\u003e [!quote] **Sources:**\n\u003e \n\u003e Thanks Hannes!\n\n### Wave Expressions\n\n[[notes/WaveExpressions |Summary]] of different useful periodic functions like square or sawtooth\n\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/Vector-Shenanigans":{"title":"Vector Shenanigans","content":"\n### Vector Flow on Objects\n\n![[notes/images/vectorflowonobject.png]]\n\n// point wrangle\n\n```C\nvector pos = v@P * chf(\"scale\");\nvector dir = curlnoise2d(pos + @Time * chf(\"time\"));\nmatrix3 mat = dihedral(set(0,0,1), v@N);\ndir *= mat;\n\nv@v = dir;\n```\n\n---\n\n- related / sources:","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/Vegetation-and-Erosion-Prediction":{"title":"Vegetation Erosion Prediction","content":"\n## Predicting Heightfield Data using cGANs\nThe basic functionalities were already demonstrated by SideFX in a [talk](https://www.sidefx.com/tutorials/machine-learning-data-preparation/) they gave about PDG.\n\n### Concept\nHeightfields are essentially images (2D volumes). Their grid-like topology makes them perfect for machine learning. \n![[notes/images/heightfield to pixel.png]]\n\nSimilar to height data all sorts of other information can be stored in masks on the heightfield. The image below shows masks, which can be used to scatter geometry later.\n![[notes/images/vegetationMaskToGeo.png]]\n\nThe idea is to train a ML model to predict masks, and erosion based on the input heightmap.\n\n### Pix2Pix\n![Image](https://raw.githubusercontent.com/phillipi/pix2pix/master/imgs/examples.jpg)\n\n[Pix2Pix](https://github.com/phillipi/pix2pix) is a popular machine learning model which translates images from one state to another. Similar to language translation you can translate the content of an image to a different art style.\n\nTo train pix2pix you need lots of image pairs that show the input and the result you want to get.\n\n### Setup\nThe setup is as shown below:\n1. build a procedural environment setup (Output heightfield and masks etc.)\n2. use PDG to randomize some parameters to generate a sufficiently large dataset\n3. calculate erosion output / vegetation masks for each randomized Input\n4. fetch results in PDG and combine to image pairs\n5. train pix2pix model on image pairs\n6. feed new inputs to the model and let it predict the results using a python sop that calls the pytorch model\n\n![[notes/images/Houdini Heightfield Prediction using Pix2Pix.jpg]]\n\n### Data Synthesis and PDG\n\nAs described above Houdini calculates the correct erosion / vegetation masks for a specific input. With the height data it provides water and sediment maps among others.\n![[notes/images/noiseterraintoerossion.png]]\n\n// Input and Eroded Heightfield\n\nThe two images below show 1 data sample consisting of input and output\nYou can use the RGB channels to encode all sorts of data that could be useful. In this example it is set up as follows:\n\ninput R = Height\ninput G = Castle Foundation\ninput B = Castle Bridge Foundation\n\noutput R = Height\noutput G = Water\noutput B = Sediment\n\nI chose to encode the castle and bridge foundations, because it was especially important, that there will be little to no height changes in this area and I hoped I could steer the model to focus on that. I can't be sure whether it really worked because of the additional masks, but the results were pretty good.\n\n// Erosion Data Sample (Input \u0026 Output)\n\n![[notes/images/erosionsample.png]]\n\nFor the Vegetation I included different information:\n\ninput R = Height\ninput G = South Direction ( Where the Sun hits most )\ninput B = Water\n\noutput R = Plant Type 1\noutput G = Plant Type 2\noutput B = Plant Type 3\n\n// Vegetation Data Sample (Input \u0026 Output)\n\n![[notes/images/vegetationsample.png]]\n\n### Results\n\n###### Erosion\n\n![[notes/images/ErosionPrediction.gif]]\n\n// Result (Ground Truth left / Prediction right)\n\n![[notes/images/heightfieldGroundtruthvspredicted.png]]\n\n// Error Visualization\n![[notes/images/HeightfieldErrorVis.png]]\n\n#### Vegetation\n\n![[notes/images/VegetationDemo.gif]]\n\n// Predicted Masks (left) \u0026 Scattered Assets (right)\n\n![[notes/images/vegetationToScattering.png]]\n### Outlook\n\nIf provided with the right real world data set you could create a scatter system that matches the vegetation of certain regions in the world fairly easy.\nAlso it might be possible to encode parameter values such as erosion strength etc. in the image to enhance user control.","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/Velocity-Fields":{"title":"Velocity Fields","content":"## Noise\n\n### Curl Noise\n\nCurl noise is especially useful because it is a divergence free noise, which means that particles cannot converge to sink points where they get stuck.\n\nThe easiest way to set this up is through VOPs:\n\n![[notes/images/curlnoise.png]]\n\n## Advanced Velocity Fields\n\n### Calculate Potential Flow with VDBs\n\n![[notes/images/potential_flow_setup.png]]\n\nYou can use the `VDB Potential Flow` node to predict how air would move around an object. You can set the direction the 'wind' is coming from with the `Background Velocity` parameter on the node. To make the velocities converge behind the object you can use two cross product operations to cross the velocity field with the gradient of the sdf twice! \n\n// content of: doublecross_with_gradient - Volume VOP\n\n![[notes/images/potential_flow_vop.png]]\nCheck out Mats video on the topic: [Houdini Tutorial: Advanced Velocity Fields / Part One - Mats](https://www.youtube.com/watch?v=K0cNvpXujmk)\n\n### Strange Attractors\n\n![[notes/images/strangeattractors.png]]\n\nmight be useful for some magic FX.\n\nI put them on their own page [[notes/Strange Attractors |here]].\n\n### Magnetic Fields\n\n![[notes/images/Pasted image 20230706174615.png]]\n- in the top left 3 points are created which have a `charge` attribute ranging from -1 to 1 resembling positive and negative magnets\n- all the purple nodes setup the velocity field\n- everything green is just for visualization\n\n// volume wrangle\n\n```C\nint numCharges = npoints(1);\nvector direction = set(0,0,0);\n\nfor( int i = 0; i \u003c numCharges; i++ )\n{\n    vector chargePosition = point(1, \"P\", i);\n    float chargeValue = point(1, \"charge\", i);\n    \n    vector chargeinfluence = (( chargePosition - v@P ) * chargeValue ) / pow(length(chargePosition - v@P), chf(\"power\")); // set power to 3 as default\n    direction += chargeinfluence;\n}\n\ndirection = direction / numCharges;\n\nv@vel = direction;\n```\n\n---\n\nsources / further reading:\n- [Houdini Tutorial: Advanced Velocity Fields / Part One - Mats](https://www.youtube.com/watch?v=K0cNvpXujmk)\n- [Houdini Algorithmic Live #021 - Magnetic Field Visualization -  Junichiro Horikawa](https://www.youtube.com/watch?v=pnfFbF-60qw)","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/Vocabulary":{"title":"Vocabulary","content":"\n### Buoyancy\nAll liquids and gases in the presence of gravity exert an upward force—called buoyancy—on any object immersed in them. If the object is less dense than the liquid or gas, buoyancy will make it float or rise upwards.\n### Curl\nRotation of a vector field. Can be viewed as flow of fluid or gas through said field.\n\n![[notes/images/curl.png]]\n\nrelated to: [[notes/Vocabulary#Divergence |Divergence]]\n### Divergence\nFlow of a vector field. Can be viewed as flow of fluid or gas through said field.\nExpanding Flow: positive Divergence (like a magnet pushing away)\nCompressing Flow: negative Divergence (like a magnet pulling in)\n\n![[notes/images/positiveAndNegativeDivergence.png]]\n\nrelated to: [[notes/Vocabulary#Curl |Curl]]\n### Eulerian Simulation\nValues are stored on grids / volumes / fields\nthink pyro simulation\nrelated to: [[notes/Vocabulary#Lagrangian Simulation |Lagrangian Simulation]]\n### Gradient\non volumes this can be used like normals on geometry to find out how the surface is oriented.\n### Lagrangian Simulation\nValues are stored on particles\nthink POP simulation\nrelated to: [[notes/Vocabulary#Eulerian Simulation |Eulerian Simulation]]\n### Scalar Fields\nScalar fields store a single float per voxel.\nExamples: Density, Temperature, Masks\n### Vector Fields\nVector Fields store three floats per voxel.\nExamples: Color, Velocity, Force\n### Vorticity\nVorticity is a pseudovector field that describes the local spinning motion of a continuum near some point (the tendency of something to rotate, as would be seen by an observer located at that point and traveling along with the flow.\n### Perpendicular Lines\nLines that meet at right angle / 90 degrees\n\n\n---\n\nsources / further reading:\n- [Divergence and curl: The language of Maxwell's equations, fluid flow, and more - 3Blue1Brown](https://www.youtube.com/watch?v=rB83DpBJQsE)\n- [The idea of the curl of a vector field - Math Insight](https://mathinsight.org/curl_idea)\n- [The idea of the divergence of a vector field - Math Insight](https://mathinsight.org/divergence_idea)\n- [Gradients and Partial Derivatives - Physics Videos by Eugene Khutoryansky)](https://www.youtube.com/watch?v=GkB4vW16QHI)\n- [Vorticity - Wikipedia](https://en.wikipedia.org/wiki/Vorticity)\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/Volumes":{"title":"Volumes","content":"## Inefficient Volumes\n\nThe following are my personal notes on Lewis Taylor's fantastic [SYDHUG Talk](https://vimeo.com/894363970). Definitely worth watching! There is also a download link for all the demo scenes in the video description.\n### Impacts of Inefficient Volumes\n\n- Slower simulation time\n\t- balance between needed fidelity and simulation time\n- Disk storage\n\t- not as cheap as commonly assumed\n\t- impacts all departments\n- Render times\n\t- not only dependent on voxel resolution\n\t- also has to load the cache\n\t- especially for BG elements the overhead to get the files is huge even though the pure rendering might be fast\n- Server IO\n\t- not infinite\n\t- more people working at the same time can easily lead to bandwidth issues\n- Slow to iterate\n- Screwing yourself and your fellow Artist's out of the resource of TIME\n\n### How to reduce Volumetric Data\n\n- Voxel Resolution - how much do you need for the given Camera/render res\n- Bit depth - 32 vs 16 - when does it matter? Can you use 8?\n\t- velocity and even density can be 16 bit in many cases\n\t- color field can easily be just 8 bit in most cases\n- Culling fields - remove data in fields that aren't needed\n\t- delete voxel data in certain areas\n\t- cull velocity by density \u003e if there is no density you won't need any velocity most of the time\n\t- can take 20%+ of the cache size\n- Resampling - reducing voxel res per field - it doesn't all need to be the same\n\t- density, flame and temperature fields should be better left as they are\n\t- velocity on the other hand can be resampled to half or sometimes even quarter the res as it is only used to smear the other fields in the motion blur\n\t- can remove 65-70% of the cache size\n\t- velocity overhead is usually more than all the other fields combined (vector3)\n- Frustum based rasterization - a camera based alternative to Cartesian grids\n\t- added in Houdini 20\n\t- Camera oriented voxel grid\n\n### Frustum Volumes\n\nInstead of orienting voxels along the xyz axis the camera frustum is used to rasterize the voxels to the cameras 2D plane x and y axis as well as the z depth.\n\n##### PROS\n- This can, depending on the simulation, divide your cache size by 10 or more!\n- Especially useful for particle type simulations like whitewater where stuff trails or comes from far away close to the camera. Essentially every shot, where volume travels a fair amount of z depth will profit most from this approach.\n\n##### CONS\n- cannot be easily reused as it is locked to the camera perspective. Sometimes more caches for different cameras are still smaller than one big cartesian rasterized cache.\n- In practice this means you have to rebuild your shaders to match the look of the cartesian rasterized volume 1:1, but it's not too bad.\n- can be problematic when you expect shadows to be cast from a volume outside of your frustum. The lighting may look weird in that case\n\n### Volume Compression\n\n\"it really really adds up\"\n\nLewis compared 3 vdbs:\n\n1. uncompressed\n\t32 bit \n\tno field culling\n\tno field resampling\n\t**34** mb\n\n2. compressed but still cartesian\n\t16 bit\n\tfield culling - vel\n\tfield resample - 2x vel\n\t**3.2** mb\n\n3. compressed + frustum cull\n\t16 bit\n\tfield culling - vel\n\tfrustum cull in sim\n\tfield resample - 2x vel\n\t**1.8** mb\n\n### Other Useful Information\n##### How many Voxels do you need per Pixel?\n\nUsually you are good to go with 1/2.5 voxels per pixel. \n\n##### Frustum Rasterizing vs Frustum Culling SOPs vs Frustum Culling DOPs\n\n1. Rasterizing\n\nYou need the `VDB Rasterize Frustum` SOP and a camera frustum VDB.\n- Z  Scale on the VDB node controls the amount of voxels in camera depth\n- It makes sense to bake in the motion blur using the according parameters on the vdb rasterize node\n\n![[notes/images/furstumcull_node.PNG]]\n\n2. Culling SOPs\n\nIf you just want to cut off anything outside the cameras frustum you can use the `VDB Clip` SOP.\n\n3. Culling DOPs\n\nTo cull right inside the simulation you can use the following snippet inside a `Gas Field Wrangle` node piped into the advection output of the pyro solver.\n\n// gas field wrangle\n\n```C\n// by Lewis Taylor\n\nstring cam = chsop(\"cam\");\nvector ndcP = toNDC(cam,@P);\n//vector camP = ptransform(\"space:camera\",@P); //non perspective space if needed\n\nfloat mult = 1;\n\nif(chi('cull_x_negative')) mult *= ndcP.x \u003e -ch('cam_x_neg') ;\nif(chi('cull_x_positive')) mult *= ndcP.x \u003c 1 + ch('cam_x_pos') ;\nif(chi('cull_y_negative')) mult *= ndcP.y \u003e -ch('cam_y_neg') ;\nif(chi('cull_y_positive')) mult *= ndcP.y \u003c 1 + ch('cam_y_pos') ;\nif(chi('cull_z_negative')) mult *= ndcP.z \u003c ch('cam_z_neg') ;\nif(chi('cull_z_positive')) mult *= ndcP.z \u003e -ch('cam_z_pos') ;\n\n\n//foreach( int i; string field; split(chs('fields'))){} //maybe loop over given fields, usually density is enough\nf@density *= mult;\nf@temperature *= mult;\nf@flame *= mult;\n@Cd *= mult;\n```\n\n\n---\n\nsources / further reading:\n- [SYDHUG_volumes - Lewis Taylor](https://vimeo.com/894363970)\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/WaveExpressions":{"title":"Wave Expressions","content":"\n## Useful Periodic Functions\n\nI recently stumbled across [this](https://www.cameroncarson.com/nuke-wave-expressions) phenomenal summary of wave expressions by Cameron Carson. Check it out! He wrote everything for Nuke so I decided to build a similar little library for VEX and adjust the necessary syntax / functions.\n\n---\n\nEvery wrangle runs on a 10 unit long resampled line along the z-axis and includes this base code to setup the parameters:\n\n```C\nfloat offset = chf(\"offset\");\nfloat wavelength = chf(\"waveLength\");\nfloat max = chf(\"max\");\nfloat min = chf(\"min\");\n```\n\n### Random\n\n![[notes/images/vexwaves.1.jpg]]\n\n```C\n@P.y = random((@P.z+offset)/wavelength) * (max-min) + min;\n```\n\n### Noise\n\n![[notes/images/vexwaves.2.jpg]]\n\n```C\n@P.y = noise((@P.z+offset)/wavelength) * (max-min) + min;\n```\n\n### Sin\n\n![[notes/images/vexwaves.3.jpg]]\n\n```C\n@P.y = (sin((@P.z+offset)/wavelength)+1)/2 * (max-min) + min;\n```\n\n### Triangle\n\n![[notes/images/vexwaves.4.jpg]]\n\n```C\n@P.y = (asin(sin(2*PI*(@P.z+offset)/wavelength))/PI+0.5) * (max-min) + min;\n```\n\n### Square\n\n![[notes/images/vexwaves.5.jpg]]\n\n```C\n@P.y = rint((sin(2*PI*(@P.z+offset)/wavelength)+1)/2) * (max-min) + min;\n```\n\n### Sawtooth\n\n![[notes/images/vexwaves.6.jpg]]\n\n```C\n@P.y = ((@P.z+offset) % wavelength)/wavelength * (max-min) + min;\n```\n\n### Sawtooth Parabolic\n\n![[notes/images/vexwaves.7.jpg]]\n\n```C\n@P.y = sin((PI*(@P.z+offset)/(2*wavelength)) % (PI/2)) * (max-min) + min;\n```\n\n### Sawtooth Parabolic Reversed\n\n![[notes/images/vexwaves.8.jpg]]\n\n```C\n@P.y = cos((PI*(@P.z+offset)/(2*wavelength)) % (PI/2)) * (max-min) + min;\n```\n\n### Sawtooth Exponential\n\n![[notes/images/vexwaves.9.jpg]]\n\n```C\n@P.y = (exp(2*PI*((@P.z+offset) % wavelength)/wavelength)-1)/exp(2*PI) * (max-min) + min;\n```\n\n### Sawtooth Exponential Reversed\n\n![[notes/images/vexwaves.9.5.jpg]]\n\n```C\n@P.y = (exp(2*PI*(1-(((@P.z+offset) % wavelength)/wavelength)))-1)/exp(2*PI) * (max-min) + min;\n```\n\n### Bounce\n\n![[notes/images/vexwaves.10.jpg]]\n\n```C\n@P.y = abs(sin(PI*(@P.z + offset)/wavelength))* (max-min) + min;\n```\n\n### Blip\n\n![[notes/images/vexwaves.11.jpg]]\n\n```C\nfloat bliplength = chf(\"bliplength\");\nfloat value = ((@P.z+(offset+wavelength)) % (wavelength+bliplength)/(wavelength)) * (wavelength/bliplength) - (wavelength/bliplength);\n\nif(value \u003e 0)\n{\n    value = 1;\n}\n\n@P.y = fit01(value, min, max);\n```\n\n\n\nsources / further reading:\n- [Nuke Wave Expressions - Cameron Carson](https://www.cameroncarson.com/nuke-wave-expressions)\n- [List of Periodic Functions - Wikipedia](https://en.wikipedia.org/wiki/List_of_periodic_functions)\n\n\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/cat-Files":{"title":"cat Files","content":"\n[.cat Files](https://learn.foundry.com/nuke/developers/13.2/catfilecreationreferenceguide/introduction.html) enable the user to deploy pretrained pytorch models directly into Nuke\n\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/iPhone-ARkit-and-Depth":{"title":"iPhone ARkit and Depth","content":"\nYou can use the ZIG SIM or ZIG SIM PRO app to stream data live into TD over the network using OSC or JSON formats. (the PRO version allows for more sensor access such as camera \u0026 depth)\n\nI linked a quick setup tutorials below.\n\n---\n\nsources / further reading:\n- [iPhone ARkit \u0026 Depth Sensor in TouchDesigner Tutorial - Part 1! - The Interactive \u0026 Immersive HQ](https://www.youtube.com/watch?v=pwwuZj8KK6M)\n- [iPhone ARkit \u0026 Depth Sensor in TouchDesigner Tutorial - Part 2! - The Interactive \u0026 Immersive HQ](https://www.youtube.com/watch?v=dfKfVJfy7SI)\n","lastmodified":"2024-11-11T20:40:13.754621058Z","tags":null},"/notes/primuv-and-xyzdist":{"title":"primuv \u0026 xyzdist","content":"\nThis is mostly a write up of different blog posts, tutorials, cg wiki's [Joy Of Vex 19](https://www.tokeru.com/cgwiki/index.php?title=JoyOfVex19) and Toadstorm's [The joy of xyzdist() and primuv()](https://www.toadstorm.com/blog/?p=465) for myself, so I recommend to check those out first!\n\n### Usecases\n\n- Stick things to geometry using VEX\n- Smoothly sample / interpolate attribute values from the closest geometry (avoids flickering as closest point changes -\u003e problem with `minpos()` / `nearpoint()`\n\n### primuv() function\n\n`primuv()` provides a way to smoothly interpolate an attribute between points on the target geometry.\n\n![[notes/images/primuv_sample_ex.gif]]\nIn this example I sample the color and the position at the vector `uv` and apply it to a single point before copying a sphere to it.\n\n// pointwrangle \"sample_primuv\":\n```C\nvector uv = chv(\"vector\");\nv@Cd = primuv(1, \"Cd\", 0, uv);\nv@P = primuv(1, \"P\", 0, uv);\n```\n\n// vex definition\n```C\n\u003ctype\u003e primuv(\u003cgeometry\u003egeometry, string attribute_name, int prim_num, vector uvw)\n```\n\nIf you give it a primid and a uv coordinate it returns the value of the specified attribute at the uv-interpolated position. UVs in this case are not typical uv coordinates used for texturing, but special **parametric UVs**.\n\n### Interpolating quaternions (p@orient)\n\nTo correctly interpolate quaternions you need spherical interpolation. The `primuv` function only interpolates linearly. To counter this issue you can convert any quaternion on your reference geometry to a `matrix3` transform and then sample it, before converting it back to a quaternion on the target geometry.\n\nThis is basically the same as creating 3 vectors, which represent the quaternion, and interpolating those.\n\nYou should also use `polardecomp()` on your matrix to make sure it is orthonormal and avoid getting \"a wonky quaternion\".\n\nthe full code looks like this:\n\n// point wrangle on reference geometry\n```C\nmatrix3 m  = qconvert(p@orient);\n```\n\n// point wrangle on target geometry\n```C\nmatrix3 interp_matrix = primuv(1, \"m\", prim_num, uv);\np@orient = quaternion(polardecomp(interp_matrix)); \n```\n\n\u003e [!quote] **Sources:**\n\u003e @Reinhold pointed this out on a houdini discord. Thanks!!\n\n### Parametric UVs vs Regular UVs\n\nWhile regular UVs usually span accross multiple primitives and map a 2D space to the 3D geometry, paramteric UVs are calculated automatically per primitive. They also don't exist as a `@uv` attribute.\n\nParametric UVs are a 0 to 1 gradient mapped across the prim. Think of a default STMap in Nuke where the x and y-axis resolution is fit to a 0-1 range and put into the red and green image channels.\n\n![[notes/images/stmap_primuvs.png]]\n\nYou can visualize them using the scatter SOP. Just turn on `sourceprimuv` but change it to `Cd` in the `output` tab.\n\n![[notes/images/primuvs_scatter.png]]\n\nAll the primitives (**NOT** volumes, vdbs, metaballs) have continuous parametric UVs:\n\n![[notes/images/primitive_primuvs.png]]\n\n### xyzdist() function\n\nWhile the `minpos()` function can be used to find the closest point on a geometry, the `xyzdist()` function returns the `id` of the closest and the parametric `uv` coordinates of this primitive at which a point would be closest. This info can then be fed into the `primuv()` function.\n\n// vex definition\n```C\nfloat  xyzdist(\u003cgeometry\u003egeometry, string primgroup, vector origin, int \u0026prim, vector \u0026uv, float maxdist)\n```\n\nThe `\u0026` symbols mean that vex will write to the attributes you specify. This is necessary because the `xyzdist()` function can return more than one value at the same time and vex can't handle filling multiplte variables at once otherwise. It's similar to the `rotate` and `scale` matrix functions.\n\nBy default `xyzdist()` returns the distance to the closest \"point\" that lies on the surface of the target geometry. By using the additional `\u0026attributes` we can get the id and uv information I mentioned before.\n\nThe usual setup  to query information by combining it with the `primuv()` function looks something like this:\n\n```C\ni@prim; \nv@uv;\nf@dist;\n \n@dist = xyzdist( 1, @P, @prim, @uv );\n \n@P = primuv( 1, 'P', @prim, @uv );\n```\n\nUnlike `minpos()` you can read all sorts of attributes with this and not just `@P`.\n\n### Make Stuff Stick to Animated Geo\n\n![[notes/images/xyzdist_sticktogeo_ex.gif]]\n\nTo make this work we need to make sure to:\n1. create a `p@orient` attribute that follows the animated geo correctly\n2. freeze the animation before placing calculating `xyzdist()`\n\n![[notes/images/sticktogeo_xyzdist.png]]\n\n// pointwrangle \"xyzdist\"\n```C\ni@prim; \nv@uv;\nf@dist;\n \n@dist = xyzdist( 1, @P, @prim, @uv );\n```\n\n//pointwrangle \"primuv\"\n```C\n@P = primuv( 1, 'P', i@prim, @uv );\np@orient = primuv( 1, 'orient', i@prim, @uv );\n```\n\nThe rest ist pretty much the same as described above. Calculate `xyzdist()`, use `primuv()` to fetch `@orient` from the animated geo, copy to points etc.\n\n\u003e [!Warning] **Causes Issues!**\n\u003e \n\u003e This setup works 95% well but can cause issues with not correctly interpolated orients. I described [[notes/primuv and xyzdist#Interpolating quaternions (p@orient) |above]] how to work around it.\n\n\n\n---\n\nsources / further reading:\n- [The joy of xyzdist() and primuv() - Toadstorm](https://www.toadstorm.com/blog/?p=465)\n- [Joy Of Vex 19 - cgwiki](https://tokeru.com/cgwiki/JoyOfVex19.html) \n- [primuv - Houdini VEX DOCs - SideFX](https://www.sidefx.com/docs/houdini/vex/functions/primuv.html)\n- [xyzdist - Houdini VEX DOCs - SideFX](https://www.sidefx.com/docs/houdini/vex/functions/xyzdist.html)\n\n\n","lastmodified":"2024-11-11T20:40:17.350641131Z","tags":null},"/notes/vexStrings":{"title":"Strings in VEX","content":"This list was created and kindly shared by Mattias Malmer on a discord I'm on. Thanks for letting me host it here:\n\n```\nabspath() -- Returns the full path of a file.\nchr() -- Converts a Unicode codepoint to a UTF8 string.\nconcat() -- Concatenates all the specified strings into a single string.\ndecode() -- Decodes a variable name that was previously encoded.\ndecodeattrib() -- Decodes a geometry attribute name that was previously encoded.\ndecodeparm() -- Decodes a node parameter name that was previously encoded.\ndecodeutf8() -- Decodes a UTF8 string into a series of codepoints.\nencode() -- Encodes any string into a valid variable name.\nencodeattrib() -- Encodes any string into a valid geometry attribute name.\nencodeparm() -- Encodes any string into a valid node parameter name.\nencodeutf8() -- Encodes a UTF8 string from a series of codepoints.\nendswith() -- Indicates if the string ends with the specified string.\nfind() -- Finds an item in an array or string.\nisalpha() -- Returns 1 if all characters in the string are alphabetic.\nisdigit() -- Returns 1 if all characters in the string are numeric.\nitoa() -- Converts an integer to a string.\njoin() -- Concatenates all strings of an array, inserting a common spacer.\nlstrip() -- Strips leading whitespace from a string.\nmakevalidvarname() -- Forces a string to conform to the rules for variable names.\nmatch() -- Returns 1 if the subject matches the pattern specified, or 0 if it doesn’t.\nopdigits() -- Returns the integer value of the last sequence of digits of a string.\nord() -- Converts a UTF8 string into a codepoint.\npluralize() -- Converts an English noun to its plural.\nre_find() -- Matches a regular expression in a string.\nre_findall() -- Finds all instances of the given regular expression in the string.\nre_match() -- Returns 1 if the entire input string matches the expression.\nre_replace() -- Replaces instances of regex_find with regex_replace.\nre_split() -- Splits the given string based on regex match.\nrelativepath() -- Computes the relative path for two full paths.\nrelpath() -- Returns the relative path to a file.\nreplace() -- Replaces occurrences of a substring.\nreplace_match() -- Replaces the matched string pattern with another pattern.\nrstrip() -- Strips trailing whitespace from a string.\nsplit() -- Splits a string into tokens.\nsplitpath() -- Splits a file path into the directory and name parts.\nsprintf() -- Formats a string like printf but returns the result as a string instead of printing it.\nstartswith() -- Returns 1 if the string starts with the specified string.\nstrip() -- Strips leading and trailing whitespace from a string.\nstrlen() -- Returns the length of the string.\ntitlecase() -- Returns a string that is the titlecase version of the input string.\ntolower() -- Converts all characters in a string to lower case.\ntoupper() -- Converts all characters in a string to upper case.\n```\n\n---\n\nsources / further reading:\n- [Houdini DOCs -SideFX](https://www.sidefx.com/docs/houdini/vex/strings.html)\n\n","lastmodified":"2024-11-11T20:40:17.358641176Z","tags":null}}