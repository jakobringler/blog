<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="This is one of the projects I presented at the Houdini HIVE Horizon event hosted by SideFX in Toronto. You can watch the full presentation here."><title>ML Groom Deformer</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://fxnotes.xyz//icon.png><link href=https://fxnotes.xyz/styles.76c8384632ed1e1f62917b5027e790d8.min.css rel=stylesheet><link href=https://fxnotes.xyz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://fxnotes.xyz/js/darkmode.1feb6613eaffbf49acf3a460f176acbe.min.js></script>
<script src=https://fxnotes.xyz/js/util.39f53d45cb9520bdaf946bd063598b19.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://fxnotes.xyz/js/popover.37b1455b8f0603154072b9467132c659.min.js></script>
<script src=https://fxnotes.xyz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://fxnotes.xyz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://fxnotes.xyz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://fxnotes.xyz/",fetchData=Promise.all([fetch("https://fxnotes.xyz/indices/linkIndex.c5da6b57613204220ede1bbe63b7c50d.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://fxnotes.xyz/indices/contentIndex.e47316e38597c6d0dd601eb03358611f.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const n=new URL(BASE_URL),s=n.pathname,o=window.location.pathname,i=s==o;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts();const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=i&&!0;drawGraph("https://fxnotes.xyz",t,[{"/moc":"#4388cc"}],t?{centerForce:1.3,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.3,linkDistance:1,opacityScale:3,repelForce:1,scale:1}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.4,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2}),initPopover("https://fxnotes.xyz",!0,!0)},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/fxnotes.xyz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://fxnotes.xyz/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://fxnotes.xyz/>fx notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><h1>ML Groom Deformer</h1><p class=meta>Last updated
Feb 25, 2025
<a href=https://github.com/jakobringler/blog/tree/hugo/content/notes/ML%20Groom%20Deformer.md rel=noopener>Edit Source</a></p><ul class=tags><li><a href=https://fxnotes.xyz/tags/houdini/>Houdini</a></li><li><a href=https://fxnotes.xyz/tags/machinelearning/>Machinelearning</a></li></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#general-idea>General Idea</a><ol><li><a href=#the-problem>The Problem</a></li><li><a href=#the-solution>The Solution</a></li></ol></li><li><a href=#data-generation>Data Generation</a><ol><li><a href=#joint-limits>Joint Limits</a></li><li><a href=#pose-randomization>Pose Randomization</a></li><li><a href=#simulation-loop>Simulation Loop</a></li><li><a href=#storing-the-examples>Storing the Examples</a></li></ol></li><li><a href=#pca-subspace-compression-and-serialization>PCA Subspace Compression and Serialization</a><ol><li><a href=#creating-the-subspace>Creating the Subspace</a></li><li><a href=#calculating-the-weights-per-example>Calculating the Weights per Example</a></li><li><a href=#why-do-all-this>Why Do All This?</a></li><li><a href=#serialization>Serialization</a></li><li><a href=#data-set-export>Data Set Export</a></li></ol></li><li><a href=#training>Training</a><ol><li><a href=#ml-regression-train>ML Regression Train</a></li><li><a href=#wedging>Wedging</a></li></ol></li><li><a href=#inference>Inference</a></li><li><a href=#results>Results</a></li><li><a href=#credits>Credits</a></li></ol></nav></details></aside><p>This is one of the projects I presented at the Houdini HIVE Horizon event hosted by SideFX in Toronto. You can watch the full presentation
<a href="https://www.youtube.com/watch?v=oDTResIxPeQ" rel=noopener>here</a>. I&rsquo;ll go into more specific setup details in this text summary. You can find an almost identical version of the text (better formatting) on the
<a href=https://www.sidefx.com/tutorials/ml-groom-deformer/ rel=noopener>SideFX tutorial page</a> and download the project files from the
<a href=https://www.sidefx.com/contentlibrary/ml-groom-deformer/ rel=noopener>content library</a>.</p><p><img src=/notes/images/previewimage.jpg width=auto></p><a href=#general-idea><h2 id=general-idea><span class=hanchor arialabel=Anchor># </span>General Idea</h2></a><p>In essence, the goal is to predict how fur deforms on a character or creature using a machine learning model, instead of relying on expensive simulations and procedural setups.</p><p>The setup is based on the
<a href=https://www.sidefx.com/contentlibrary/ml-deformer-h205/ rel=noopener>ML Deformer example</a> that was included in the content library with the release of Houdini 20.5.</p><a href=#the-problem><h3 id=the-problem><span class=hanchor arialabel=Anchor># </span>The Problem</h3></a><p>There are a few common issues when deforming hair naively or linearly. Particularly around the joint areas, there are numerous intersection problems, where hair above the joint pierces through the fur layer below (see the second position in the graphic below). Ideally, we want to see a nice layering of guides and better volume preservation. Achieving this can be computationally expensive and often requires a simulation along with one or more post-processing steps to get right.</p><p><img src=/notes/images/ML-groom-input-target-comparsion.gif width=auto></p><p>Left to right: Rig, Default Guide Deform, Ideal Deformation (Quasi-Static Simulation), Rest Guides, Displacement in Rest Space</p><a href=#the-solution><h3 id=the-solution><span class=hanchor arialabel=Anchor># </span>The Solution</h3></a><p>Our goal is to predict the deformation that needs to be applied to the guides based on the rig position using an ML model. The model learns to map from the rotation data of each rig joint to the approximated displacement of the guides for each pose.</p><p>To do this, we need a substantial number of examples for the model to learn from. The general pipeline looks something like this:</p><p><img src=/notes/images/mlgroom_datageneration.png width=auto></p><ol><li>Define joint limits on our rig (fixed values or automatically in a procedural fashion)</li><li>Generate a bunch of random poses based on these joint limits (think 3000 or more / we need good coverage)</li><li>Simulate a ground truth/good guide deformation for each pose</li><li>Collect all deformations and compress the data into a PCA subspace (conceptually the hard part)</li><li>Store the full preprocessed dataset to disk</li><li>Train a ML model in TOPs(which is actually the easy part)</li></ol><a href=#data-generation><h2 id=data-generation><span class=hanchor arialabel=Anchor># </span>Data Generation</h2></a><p>To generate all the training data we need 2 things to start out with:</p><ol><li>a rigged model of a character/creature (in this case the wolf)</li><li>a set of guides</li></ol><a href=#joint-limits><h3 id=joint-limits><span class=hanchor arialabel=Anchor># </span>Joint Limits</h3></a><p>This could be a set degree value on each joint (e.g., 30) or, preferably, if we have a few motion clips for the rig, we can procedurally configure the joint limits based on the range of motion present in our clips. To do this, use the <code>configure joint limits SOP</code>.</p><a href=#pose-randomization><h3 id=pose-randomization><span class=hanchor arialabel=Anchor># </span>Pose Randomization</h3></a><p>In Houdini 20.5, a new node was introduced to streamline this process. The <code>ml pose generate SOP</code> takes existing joint limits and generates random poses in a Gaussian distribution. This means there will be more normal poses than extreme ones, which is desirable for training a model that performs well in common positions.</p><p><img src=/notes/images/ML_groom_generated_poses.gif width=auto></p><p>The output is a bunch of posed packed rigs stacked on top of each other.</p><a href=#simulation-loop><h3 id=simulation-loop><span class=hanchor arialabel=Anchor># </span>Simulation Loop</h3></a><p>In this stage we run over each of our generated poses using TOPs and generate/simulate a good guide deformation result accordingly.</p><p>This step could be almost anything. There is just a few limitations:</p><p>The result has to be deterministic / procedural / quasi-static!</p><p>That means we can&rsquo;t have any inertia (think jiggle, overshoot, simulation goodness) and we should always get the same result when we rerun the same inputs. Also the inputs should influence the outputs in a smooth way. Additionally, the relationship between inputs and outputs should be smooth; similar inputs should yield closely related results. So, let&rsquo;s avoid any noise that might lead to wildly different outcomes for nearly identical inputs.</p><p>In this case I&rsquo;m using a combination of a quasi-static tet simulation and a de-intersection pass, in which the guides are advected through a velocity field built from the hair and skin.</p><a href=#create-pose-blend-animation><h4 id=create-pose-blend-animation><span class=hanchor arialabel=Anchor># </span>Create Pose Blend Animation</h4></a><p>In this step we blend over from the rest position to the target rig position to create an animation we use to drive our simulation.</p><p><img src=/notes/images/animate_to_pos.gif width=400></p><a href=#quasi-static-tet-simulation-vellum><h4 id=quasi-static-tet-simulation-vellum><span class=hanchor arialabel=Anchor># </span>Quasi-Static Tet Simulation (Vellum)</h4></a><p>To simulate the main part of the fur a tet cage is built from the guides to simulate the volume of the fur and how it deforms. This is very useful for avoiding guide-to-guide collisions and ensures correct layering of fur by design.</p><p><img src=/notes/images/guides_to_tets.gif width=400></p><p>To generate the tet cage I use some simple VDB operations:</p><p>guides > VDB from particles > reshape dilate close erode >tet conform</p><p>The tet simulation runs in vellum on quasi-static mode. The red interior points get pinned to the animation to move the soft body with it.</p><p><img src=/notes/images/simulate-tets-edited.gif width=400></p><p>After applying the deformation with a point deform we get rid of all the jagged edges and most of the intersections. The volume of the whole groom gets preserved too.</p><p><img src=/notes/images/pointdeform_to_tet_cage.gif width=400></p><a href=#de-intersection-through-velocity-advection><h4 id=de-intersection-through-velocity-advection><span class=hanchor arialabel=Anchor># </span>De-Intersection through Velocity Advection</h4></a><p>To clean up any left over guide intersections we advect the guides through a velocity field that is built from the normals of the skin mesh and the tangents of the guides themselves. Due to the nature of velocity fields the guides basically de-intersect themselves and the flow gets clean up a little on the way there. I first saw this technique in Animalogic&rsquo;s
<a href="https://www.youtube.com/watch?v=NgOxluYHb54" rel=noopener>talk</a> about their automated CFX pipeline.</p><p><img src=/notes/images/velocity_deintersect.gif width=400></p><a href=#ground-truth-examples><h4 id=ground-truth-examples><span class=hanchor arialabel=Anchor># </span>Ground Truth Examples</h4></a><p>After all of this we end up with one correctly deformed set of guides per pose:</p><p><img src=/notes/images/generated-guidedeformation.gif width=400></p><a href=#calculate-rest-displacement><h4 id=calculate-rest-displacement><span class=hanchor arialabel=Anchor># </span>Calculate Rest Displacement</h4></a><p>We need to then move each of these poses back into rest space using the classical guide deform node. In rest space we compute the difference of each point on the deformed guides to the rest guides using the <code>shapediff SOP</code>. This gives us a clean point cloud we will store together with the rig position as one training example.</p><p><img src=/notes/images/rest_space_disp_extraction.gif width=auto></p><a href=#storing-the-examples><h3 id=storing-the-examples><span class=hanchor arialabel=Anchor># </span>Storing the Examples</h3></a><p>Once all of the poses are simulated and the displacement is generated we can start assembling what is called an <code>ML Example</code> in Houdini. Usually this is called a data sample. It consists of an input and a target and is one of the many examples we show the network during training to make it (hopefully) learn the task at hand.</p><p>To do this we can use the <code>ml example SOP</code>, which packs each of the inputs before packing the merged pairs again. This ensures data pairs stay together and don&rsquo;t get out of sync somehow downstream.</p><p><img src=/notes/images/ml_example_slide.png width=auto></p><a href=#pca-subspace-compression-and-serialization><h2 id=pca-subspace-compression-and-serialization><span class=hanchor arialabel=Anchor># </span>PCA Subspace Compression and Serialization</h2></a><p>After generating all of our pose displacements we bring in those points and merge them all together and calculate our PCA subspace.</p><p>If we want to read more on Principal Component Analysis, I tried to explain it in a non mathy way <a href=/notes/Principal-Component-Analysis rel=noopener class=internal-link data-src=/notes/Principal-Component-Analysis>here</a>.</p><a href=#creating-the-subspace><h3 id=creating-the-subspace><span class=hanchor arialabel=Anchor># </span>Creating the Subspace</h3></a><p>We let PCA do the math magic and compress our 4000 examples down to fewer components. In this case we can think of it as inventing new, but fewer blendshapes of the original displacements that (if combined correctly) can reconstruct all of our inputs accurately.</p><p><img src=/notes/images/pca_subspace_disp_slide.png width=auto></p><p>This PCA-generated point cloud is all of the new components (invented blend shapes) stacked on top of each other. They aren&rsquo;t separated in any way in Houdini (not packed, no ids or anything). We determine which points belong to each component by understanding the size of the sample. For example, if our dataset contains 100,000 points, we can identify the components by examining the list of points. The first component consists of points from 0 to 99,999 (ptnum 100k-1), while the second component includes points from 100,000 to 199,999 (ptnum 200k-1), and so forth.</p><a href=#calculating-the-weights-per-example><h3 id=calculating-the-weights-per-example><span class=hanchor arialabel=Anchor># </span>Calculating the Weights per Example</h3></a><p>In the next step we will calculate how much of each component (blend shape) we need to combine to reconstruct each original displacement.</p><p>We loop over each example and let PCA &ldquo;project&rdquo; (PCA math term for calculating the weights) our displacement points onto the subspace. This returns a list of weights that correspond to the amount of components in the subspace.</p><p>Those weights can later be applied to the subspace point cloud to reconstruct the original displacement point cloud (close enough at least).</p><p><img src=/notes/images/pca_project_slide.png width=auto></p><p>The reconstruction will never be 100%, but we can reach 95%+ levels while using only a few hundred components.
Hair guides are especially tricky to compress, because of the high point count (100.000+ points) and the missing relationship between individual hair stands.</p><p>For things like skin geometry (think muscle or cloth deformer) we can usually get away with much fewer components (64-128 possibly) and still reach high reconstruction accuracy.</p><a href=#why-do-all-this><h3 id=why-do-all-this><span class=hanchor arialabel=Anchor># </span>Why Do All This?</h3></a><p>Instead of having to learn 100.000 point positions (300k float values), we only need the model to predict the weights needed for reconstruction (64, 128, 512, 1024 floats or whatever we choose). The size of our network stays small and training and inference is much faster that way. Performance would also suffer immensely, trying to map a few joint rotation values to a giant list of point position values.</p><a href=#serialization><h3 id=serialization><span class=hanchor arialabel=Anchor># </span>Serialization</h3></a><p>On the other side of that for loop we serialize the joint transforms using the new <code>ml pose serialize SOP</code>. All this node does is take the 3x3 matrices stored on each joint and map the values to a -1 to 1 range and store each float of that matrix on a single point in series. We end up with a long list of float values which represent our joint rotation data. This is necessary because neural networks don&rsquo;t like matrices as a single input. It&rsquo;s easier to only work with single float values. The mapping to the -1 to 1 range makes sure it plays nicely with <a class="internal-link broken">activation functions</a> inside the network.</p><p><img src=/notes/images/serialize_slide.png width=auto></p><a href=#data-set-export><h3 id=data-set-export><span class=hanchor arialabel=Anchor># </span>Data Set Export</h3></a><p>After postprocessing all the examples we can write out the full data set to disk. The <code>ml example output SOP</code> stores all the samples together in a single data_set.raw file, which will be read by the training process later on.</p><a href=#training><h2 id=training><span class=hanchor arialabel=Anchor># </span>Training</h2></a><p>The training is completely done inside of TOPs. There isn&rsquo;t too much to it after having done all that preparation work, which does the heavy lifting and is the process that takes the most amount of time (to setup and to run all the simulations as well).</p><a href=#ml-regression-train><h3 id=ml-regression-train><span class=hanchor arialabel=Anchor># </span>ML Regression Train</h3></a><p><img src=/notes/images/mlregressiontrain_TOP_slide.png width=auto></p><p>The core of the whole system is the <code>ml regression train TOP</code>, which is a wrapper around PyTorch under the hood. We can set all the common training parameters on there and it comes with some nice quality of life features, such as splitting our dataset automatically in training and validation data based on a ratio we can specify.</p><p><img src=/notes/images/mlregressiontrain_parms_slide.png width=auto></p><p>We can also control all the parameters of the network (called hyperparameters for some obscure ml reason).</p><p>The most important ones are:</p><ul><li>Uniform Hidden Layers (how big is our network in width)</li><li>Uniform Hidden Width (how many &ldquo;neurons&rdquo; each layer has)</li><li>Batch Size (how many examples to look at before adjusting the weights > average)</li><li>Learning Rate (how big the steps are the model takes towards the goal / think jumping down mountain or walking carefully)</li></ul><p>To start training something we only need to make sure to specify the correct directory and name of our dataset on the files tab. By default this should be correct though. Be careful: Only specify the filename without the extension. It won&rsquo;t run if we add the <code>.raw</code>. (last tested in H20.5.370)</p><a href=#wedging><h3 id=wedging><span class=hanchor arialabel=Anchor># </span>Wedging</h3></a><p>Having all those parameters available on the node in TOPs opens the door to do some wedging! This is common practice and is usually called hyperparameter tuning. We could run multiple experiments to find the right combination of parameters that give we the best performing model. The parameters I mentioned above are a good place to start wedging. For the groom deformation example here I only wedged the amount of layers and the size of each layer.</p><a href=#inference><h2 id=inference><span class=hanchor arialabel=Anchor># </span>Inference</h2></a><p>Or doing everything in reverse..</p><p>Inference is the process of applying the model to new inputs. To do this we need to make sure the inputs come in in the exact same shape as they did when training.</p><p>So we serialize the new rig pose before applying our model using the <code>ml regression inference SOP</code>. This SOP is just a simpler version of the <code>onnx inference SOP</code>. It takes the model and some info on what values to read/write from where (points or volumes).</p><p><img src=/notes/images/reverseprep_slide.png width=auto></p><p>The model then spits out a list of weights, which we can use to reconstruct our displacement based on the subspace components we saved earlier.</p><p><img src=/notes/images/reversepca_slide.png width=auto></p><p>Feed those two into a PCA node and let it do it&rsquo;s thing. This gives us the displacement we need to deform the guides. The rest should be pretty straight forward. Apply the predicted displacement to our guides by adding each vector to the corresponding point on the curves.</p><p><img src=/notes/images/reversedisp_slide.png width=auto></p><p>That gives us a jagged looking ruffled up wolf. But if we deform it into the correct position the displacement is based on we get a smooth looking result.</p><p><img src=/notes/images/blend-to-pose.gif width=auto></p><p><img src=/notes/images/runcycle.gif width=auto></p><p>Here&rsquo;s a frame by frame preview where I blend back and forth between the original linear guide deform and the ml prediction</p><p><img src=/notes/images/guide_blending_edited4_00057600.gif width=auto></p><p>Also we can measure and visualize the error of our prediction. The number is the <a href=/notes/RMSE rel=noopener class=internal-link data-src=/notes/RMSE>RMSE</a> (Root Mean Squared Error) of all the point differences. The color visualizes the local error compared to the ground truth on the right.</p><p><img src=/notes/images/error-vis.gif width=auto></p><a href=#results><h2 id=results><span class=hanchor arialabel=Anchor># </span>Results</h2></a><p>Here&rsquo;s a few more screenshots and renders of how this could affect a full groom:</p><p><img src=/notes/images/compare_fullgroom_slide.png width=auto></p><p><img src=/notes/images/fullgroom_compare_slide.png width=auto></p><p><img src=/notes/images/runcycle_sideview_02.gif width=auto></p><a href=#credits><h2 id=credits><span class=hanchor arialabel=Anchor># </span>Credits</h2></a><p>Thanks to the amazing team at SideFX for all the help with this project!</p><p>Support on All Fronts: Fianna Wong
ML Tools Developer: Michiel Hagedoorn
Wolf Model & Groom: Lorena E&rsquo;Vers
CFX Help: Kai Stavginski & Liesbeth Levick</p><hr><p>sources / further reading:</p><ul><li><a href="https://www.youtube.com/watch?v=oDTResIxPeQ" rel=noopener>PCA Shenanigans and How to ML | Jakob Ringler | Houdini Horizon</a></li><li><a href="https://www.youtube.com/watch?v=rPdOjkEwxQM" rel=noopener>KineFX - APEX Rigging and Skinning with ML Deformer | Wave VFX | FMX HIVE 2024</a></li><li><a href=https://www.sidefx.com/contentlibrary/ml-deformer-h205/ rel=noopener>ML Groom Deformer - Houdini 20.5 - SideFX</a></li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/notes/Houdini/ data-ctx="ML Groom Deformer" data-src=/notes/Houdini class=internal-link>Houdini</a></li><li><a href=/notes/KineFX/ data-ctx="ML Groom Deformer" data-src=/notes/KineFX class=internal-link>KineFX</a></li><li><a href=/notes/Machine-Learning/ data-ctx="ML Groom Deformer" data-src=/notes/Machine-Learning class=internal-link>Machine Learning</a></li><li><a href=/notes/Principal-Component-Analysis/ data-ctx="ML Groom Deformer" data-src=/notes/Principal-Component-Analysis class=internal-link>Principal Component Analysis</a></li><li><a href=/notes/Projects-and-RnD/ data-ctx="ML Groom Deformer" data-src=/notes/Projects-and-RnD class=internal-link>Projects / R&D</a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://fxnotes.xyz/js/graph.8cefef8d9a78cb623e39ee66e5a4af0e.js></script></div></div><div id=contact_buttons><footer><p>Made by <a href=https://www.jakobringler.com>Jakob Ringler</a> using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2025</p><ul><li><a href=https://fxnotes.xyz/>Home</a></li><li><a href=https://twitter.com/jakobrin>Twitter</a></li><li><a href=https://github.com/jakobringler>Github</a></li><li><a href=https://fxnotes.xyz/notes/Legal/>Legal / Impressum</a></li></ul></footer></div></div></body></html>